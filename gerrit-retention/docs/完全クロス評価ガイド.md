# 完全クロス評価ガイド

**訓練ラベル × 評価期間のマトリクス評価**

---

## 📊 実験概要

### 訓練ラベル（5 種類）

各モデルは異なる期間の貢献を予測するよう訓練されます：

1. **0-1m**: cutoff 後 0-1 ヶ月以内に貢献
2. **0-3m**: cutoff 後 0-3 ヶ月以内に貢献
3. **0-6m**: cutoff 後 0-6 ヶ月以内に貢献
4. **0-9m**: cutoff 後 0-9 ヶ月以内に貢献
5. **0-12m**: cutoff 後 0-12 ヶ月以内に貢献

### 評価期間（4 種類）

各モデルは異なる期間で評価されます：

1. **0-3m**: cutoff 後 0-3 ヶ月の間に貢献があるか
2. **3-6m**: cutoff 後 3-6 ヶ月の間に貢献があるか
3. **6-9m**: cutoff 後 6-9 ヶ月の間に貢献があるか
4. **9-12m**: cutoff 後 9-12 ヶ月の間に貢献があるか

### マトリクス構成

```
            評価期間
          | 0-3m | 3-6m | 6-9m | 9-12m |
----------|------|------|------|-------|
訓練 0-1m |  ●   |  ●   |  ●   |   ●   |
     0-3m |  ●   |  ●   |  ●   |   ●   |
     0-6m |  ●   |  ●   |  ●   |   ●   |
     0-9m |  ●   |  ●   |  ●   |   ●   |
    0-12m |  ●   |  ●   |  ●   |   ●   |

合計: 5訓練 × 4評価 = 20評価
```

---

## 🚀 実行方法

### サーバで実行（推奨）

```bash
# サーバにログイン
ssh your-server
cd /path/to/gerrit-retention

# 実行権限付与
chmod +x scripts/training/irl/run_full_cross_evaluation.sh

# バックグラウンド実行
nohup bash scripts/training/irl/run_full_cross_evaluation.sh > /tmp/full_cross_eval.log 2>&1 &

# プロセスID確認
echo $! | tee /tmp/full_cross_eval.pid

# ログ監視
tail -f /tmp/full_cross_eval.log
```

### ローカルで実行

```bash
cd /Users/kazuki-h/rl/gerrit-retention
bash scripts/training/irl/run_full_cross_evaluation.sh
```

---

## ⏱️ 実行時間

### 見積もり

| 項目         | 時間                    |
| ------------ | ----------------------- |
| 1 モデル訓練 | 約 60 分（エポック 20） |
| 1 評価       | 約 60 分（訓練と同様）  |
| **合計**     | **約 5-6 時間**         |

**内訳:**

- 5 モデル訓練: 5 時間（並列実行なし）
- 20 評価: 各モデル訓練時に実施

**注意**: 実際には各訓練ラベルで 1 つのモデルを訓練し、4 つの評価期間で評価するため、評価自体は訓練と同時に実行されます。

---

## 📊 進捗確認

### 完了数確認

```bash
# 訓練モデル数（5個になれば訓練完了）
ls outputs/full_cross_eval/train_*/irl_model.pt 2>/dev/null | wc -l

# 評価完了数（20個になれば全て完了）
ls outputs/full_cross_eval/train_*/eval_*/metrics.json 2>/dev/null | wc -l

# メインログ
tail -50 outputs/full_cross_eval/logs/main.log
```

### 詳細確認

```bash
# 特定モデルの訓練ログ
tail -f outputs/full_cross_eval/logs/train_0-3m.log

# 特定評価のログ
tail -f outputs/full_cross_eval/logs/train_0-3m_eval_3-6m.log
```

---

## 📥 結果確認

### 自動集計

実験完了後、自動的に以下が生成されます：

```
outputs/full_cross_eval/
├── full_cross_evaluation_results.csv    # 全結果の詳細
├── matrix_AUC_ROC.csv                   # AUC-ROCマトリクス
├── matrix_F1.csv                        # F1スコアマトリクス
├── matrix_継続rate.csv                  # 継続率マトリクス
└── ... (その他のメトリクス)
```

### 手動集計

```bash
uv run python scripts/training/irl/summarize_full_cross_evaluation.py outputs/full_cross_eval
```

### 結果の見方

**AUC-ROC マトリクス例:**

```
            0-3m   3-6m   6-9m   9-12m
0-1m        0.720  0.680  0.650  0.620
0-3m        0.740  0.710  0.680  0.650
0-6m        0.750  0.730  0.710  0.680
0-9m        0.755  0.740  0.730  0.710
0-12m       0.760  0.750  0.740  0.730
```

**解釈:**

- **行**: 訓練に使用したラベル期間
- **列**: 評価する期間
- **値**: AUC-ROC スコア（高いほど良い）

**期待される傾向:**

1. **対角線付近が高い**: 訓練期間と評価期間が近いほど性能が良い
2. **長期訓練が汎化**: 0-12m で訓練したモデルは様々な期間で安定
3. **短期評価が難しい**: 0-3m の予測は最も難しい可能性

---

## 📁 出力ファイル構造

```
outputs/full_cross_eval/
├── logs/
│   ├── main.log                                  # 全体の進捗ログ
│   ├── train_0-1m.log                            # 0-1m訓練ログ
│   ├── train_0-1m_eval_0-3m.log                  # 0-1m→0-3m評価ログ
│   ├── train_0-1m_eval_3-6m.log                  # 0-1m→3-6m評価ログ
│   └── ... (その他のログ)
├── train_0-1m/
│   ├── irl_model.pt                              # 訓練済みモデル
│   ├── metrics.json                              # 訓練時の評価メトリクス
│   ├── training.log                              # 訓練の詳細ログ
│   ├── eval_0-3m/
│   │   └── metrics.json                          # 0-3m評価メトリクス
│   ├── eval_3-6m/
│   │   └── metrics.json                          # 3-6m評価メトリクス
│   ├── eval_6-9m/
│   │   └── metrics.json                          # 6-9m評価メトリクス
│   └── eval_9-12m/
│       └── metrics.json                          # 9-12m評価メトリクス
├── train_0-3m/
│   └── ... (同様)
├── train_0-6m/
│   └── ... (同様)
├── train_0-9m/
│   └── ... (同様)
├── train_0-12m/
│   └── ... (同様)
├── full_cross_evaluation_results.csv             # 全結果CSVファイル
├── matrix_AUC_ROC.csv                            # AUC-ROCマトリクス
├── matrix_F1.csv                                 # F1スコアマトリクス
└── ... (その他のマトリクス)
```

---

## 🎯 研究的意義

### 1. 汎化性能の検証

異なる期間で訓練したモデルが、他の期間でどれだけ性能を発揮できるか確認できます。

### 2. 最適な訓練期間の特定

どの訓練期間（0-1m, 0-3m, ...）が最も汎化性能が高いか特定できます。

### 3. 予測難易度の把握

どの評価期間（0-3m, 3-6m, ...）が最も予測が難しいか把握できます。

### 4. 実用的な知見

- **短期予測**: 0-3m 予測に最適なモデルはどれか？
- **中期予測**: 3-6m 予測に最適なモデルはどれか？
- **長期予測**: 9-12m 予測に最適なモデルはどれか？

---

## 📊 期待される結果

### 仮説 1: 長期訓練モデルの汎化性能が高い

```
0-12mで訓練したモデルは、全ての評価期間で安定した性能を示す
```

### 仮説 2: 短期予測が最も難しい

```
0-3m評価は全てのモデルで最も低い性能を示す
```

### 仮説 3: 訓練期間と評価期間の一致で性能向上

```
0-3m訓練 → 0-3m評価の性能が最も高い（対角線が強い）
```

---

## 🔧 トラブルシューティング

### プロセス確認

```bash
# プロセス確認
ps aux | grep run_full_cross_evaluation

# 停止
cat /tmp/full_cross_eval.pid | xargs kill

# 強制停止
pkill -f run_full_cross_evaluation
```

### 途中経過の確認

```bash
# 完了した訓練モデル
ls -lh outputs/full_cross_eval/train_*/irl_model.pt

# 完了した評価
find outputs/full_cross_eval -name "metrics.json" -type f

# 最新の評価メトリクス
find outputs/full_cross_eval/train_*/eval_* -name "metrics.json" -exec tail -5 {} \;
```

### エラー発生時

```bash
# エラーログ確認
grep -i "error\|エラー\|失敗" outputs/full_cross_eval/logs/main.log

# 個別ログ確認
tail -100 outputs/full_cross_eval/logs/train_0-3m_eval_3-6m.log
```

---

## 📚 関連ドキュメント

- [全シーケンス月次集約ラベル実験結果](./全シーケンス月次集約ラベル実験結果.md) - 基礎実験の結果
- [クロス評価実行ガイド](./クロス評価実行ガイド.md) - 簡易版クロス評価
- [SERVER_EXECUTION_GUIDE.md](../SERVER_EXECUTION_GUIDE.md) - サーバ実行の詳細

---

## ✅ 実行チェックリスト

### 実行前

- [ ] データファイルが存在する
- [ ] 十分なディスク容量がある（約 15GB）
- [ ] 十分な実行時間がある（約 5-6 時間）
- [ ] スクリプトが実行可能

### 実行中

- [ ] 定期的に進捗を確認（完了数）
- [ ] エラーログをチェック
- [ ] ディスク容量を監視

### 実行後

- [ ] 全 5 モデルが訓練完了
- [ ] 全 20 評価が完了
- [ ] マトリクス CSV ファイルが生成されている
- [ ] 結果の解釈と可視化

---

## 🎓 次のステップ

結果を元に：

1. **最適モデルの特定**: どの訓練期間が最も汎化性能が高いか
2. **可視化**: ヒートマップでマトリクスを可視化
3. **論文執筆**: クロス評価結果を論文に含める
4. **実運用**: 最適なモデルを実運用に適用
