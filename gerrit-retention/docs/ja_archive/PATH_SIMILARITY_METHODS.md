# Pathé¡ä¼¼åº¦è¨ˆç®—ã®ä»£æ›¿æ‰‹æ³•

Jaccardä¿‚æ•°ä»¥å¤–ã®Pathé¡ä¼¼åº¦è¨ˆç®—æ‰‹æ³•ã®ææ¡ˆã¨å®Ÿè£…

**æœ€çµ‚æ›´æ–°**: 2025-10-17

---

## ğŸ“Š ç¾åœ¨ã®å•é¡Œç‚¹

### Jaccardä¿‚æ•°ã®èª²é¡Œ

**ç¾åœ¨ã®å®Ÿè£…**:
```python
jaccard = len(set_A & set_B) / len(set_A | set_B)
```

**å•é¡Œç‚¹**:
1. **éšå±¤æ§‹é€ ã‚’ç„¡è¦–**: `/src/core/auth.py` ã¨ `/src/utils/auth.py` ã®é¡ä¼¼æ€§ã‚’æ‰ãˆã‚‰ã‚Œãªã„
2. **é »åº¦ã‚’ç„¡è¦–**: åŒã˜ãƒ•ã‚¡ã‚¤ãƒ«ã‚’10å›å¤‰æ›´ã—ã¦ã‚‚1å›å¤‰æ›´ã—ã¦ã‚‚åŒã˜æ‰±ã„
3. **é‡è¦åº¦ã‚’ç„¡è¦–**: é‡è¦ãªãƒ•ã‚¡ã‚¤ãƒ«ã¨äº›ç´°ãªãƒ•ã‚¡ã‚¤ãƒ«ãŒåŒç­‰
4. **éƒ¨åˆ†ä¸€è‡´ã‚’ç„¡è¦–**: `auth.py` ã¨ `authentication.py` ã®é¡ä¼¼æ€§ã‚’è€ƒæ…®ã—ãªã„

---

## ğŸ”§ ææ¡ˆæ‰‹æ³•

### æ‰‹æ³•1: éšå±¤çš„é‡ã¿ä»˜ãã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦ï¼ˆæ¨å¥¨â˜…â˜…â˜…ï¼‰

#### æ¦‚è¦
ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã‚’éšå±¤ã”ã¨ã«åˆ†è§£ã—ã€é‡ã¿ä»˜ããƒ™ã‚¯ãƒˆãƒ«ã¨ã—ã¦ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦ã‚’è¨ˆç®—

#### ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ 
```python
# ä¾‹: "/src/core/auth.py" â†’ ["src", "core", "auth.py"]
# é‡ã¿: [0.3, 0.5, 0.2] ï¼ˆæ·±ã„éšå±¤ã»ã©é‡è¦ï¼‰

def hierarchical_weighted_cosine(paths_A, paths_B):
    """éšå±¤çš„é‡ã¿ä»˜ãã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦"""
    # 1. å…¨ãƒ‘ã‚¹ã‚’éšå±¤ã”ã¨ã«åˆ†è§£
    tokens_A = [path.split('/') for path in paths_A]
    tokens_B = [path.split('/') for path in paths_B]

    # 2. èªå½™æ§‹ç¯‰
    vocab = build_vocab(tokens_A + tokens_B)

    # 3. é‡ã¿ä»˜ããƒ™ã‚¯ãƒˆãƒ«åŒ–
    vec_A = vectorize_with_weights(tokens_A, vocab, depth_weights)
    vec_B = vectorize_with_weights(tokens_B, vocab, depth_weights)

    # 4. ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦
    return cosine_similarity(vec_A, vec_B)
```

#### åˆ©ç‚¹
- âœ… éšå±¤æ§‹é€ ã‚’è€ƒæ…®
- âœ… é »åº¦ã‚’åæ˜ ï¼ˆTF-IDFçš„ã‚¢ãƒ—ãƒ­ãƒ¼ãƒï¼‰
- âœ… æ·±ã„éšå±¤ï¼ˆãƒ•ã‚¡ã‚¤ãƒ«åï¼‰ã«é‡ã¿ä»˜ã‘å¯èƒ½

#### å®Ÿè£…ä¾‹
```python
import numpy as np
from collections import Counter

def hierarchical_cosine_similarity(paths_A, paths_B, depth_weights=None):
    """
    éšå±¤çš„é‡ã¿ä»˜ãã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦

    Args:
        paths_A: ãƒ¬ãƒ“ãƒ¥ã‚¢ãƒ¼AãŒå¤‰æ›´ã—ãŸãƒ‘ã‚¹ã®ãƒªã‚¹ãƒˆ
        paths_B: ãƒ¬ãƒ“ãƒ¥ã‚¢ãƒ¼BãŒå¤‰æ›´ã—ãŸãƒ‘ã‚¹ã®ãƒªã‚¹ãƒˆ
        depth_weights: éšå±¤ã”ã¨ã®é‡ã¿ [0.2, 0.3, 0.5] ãªã©
    """
    if not paths_A or not paths_B:
        return 0.0

    # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆé‡ã¿: æ·±ã„éšå±¤ã»ã©é‡è¦
    if depth_weights is None:
        max_depth = max(max(len(p.split('/')) for p in paths_A),
                       max(len(p.split('/')) for p in paths_B))
        # ç·šå½¢å¢—åŠ : [0.1, 0.2, 0.3, ..., 1.0]
        depth_weights = [(i+1)/max_depth for i in range(max_depth)]

    # ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ã¨é‡ã¿ä»˜ãé »åº¦è¨ˆç®—
    def weighted_tokens(paths):
        weighted_counter = Counter()
        for path in paths:
            tokens = path.split('/')
            for depth, token in enumerate(tokens):
                weight = depth_weights[min(depth, len(depth_weights)-1)]
                weighted_counter[token] += weight
        return weighted_counter

    counter_A = weighted_tokens(paths_A)
    counter_B = weighted_tokens(paths_B)

    # å…±é€šèªå½™
    vocab = set(counter_A.keys()) | set(counter_B.keys())

    # ãƒ™ã‚¯ãƒˆãƒ«åŒ–
    vec_A = np.array([counter_A.get(token, 0) for token in vocab])
    vec_B = np.array([counter_B.get(token, 0) for token in vocab])

    # ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦
    norm_A = np.linalg.norm(vec_A)
    norm_B = np.linalg.norm(vec_B)

    if norm_A == 0 or norm_B == 0:
        return 0.0

    return np.dot(vec_A, vec_B) / (norm_A * norm_B)
```

---

### æ‰‹æ³•2: Edit Distance Based Similarityï¼ˆæ¨å¥¨â˜…â˜…â˜†ï¼‰

#### æ¦‚è¦
ãƒ‘ã‚¹é–“ã®ç·¨é›†è·é›¢ï¼ˆLevenshteinè·é›¢ï¼‰ã‚’ä½¿ç”¨ã—ãŸé¡ä¼¼åº¦

#### ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ 
```python
def path_edit_similarity(paths_A, paths_B):
    """ãƒ‘ã‚¹ç·¨é›†è·é›¢ãƒ™ãƒ¼ã‚¹ã®é¡ä¼¼åº¦"""
    # å„ãƒšã‚¢ã®æœ€å°ç·¨é›†è·é›¢ã‚’è¨ˆç®—
    similarities = []
    for path_a in paths_A:
        min_dist = min(edit_distance(path_a, path_b) for path_b in paths_B)
        max_len = max(len(path_a), max(len(path_b) for path_b in paths_B))
        similarities.append(1 - min_dist / max_len)

    return np.mean(similarities)
```

#### åˆ©ç‚¹
- âœ… éƒ¨åˆ†ä¸€è‡´ã‚’æ‰ãˆã‚‹ (`auth.py` ã¨ `authentication.py`)
- âœ… ã‚¿ã‚¤ãƒã‚„é¡ä¼¼ãƒ•ã‚¡ã‚¤ãƒ«åã‚’æ¤œå‡º

#### å®Ÿè£…ä¾‹
```python
from Levenshtein import distance as levenshtein_distance

def average_min_edit_similarity(paths_A, paths_B):
    """
    å¹³å‡æœ€å°ç·¨é›†è·é›¢é¡ä¼¼åº¦

    å„Aã®ãƒ‘ã‚¹ã«å¯¾ã—ã¦ã€Bã®æœ€ã‚‚è¿‘ã„ãƒ‘ã‚¹ã¨ã®é¡ä¼¼åº¦ã‚’è¨ˆç®—ã—ã€å¹³å‡ã‚’è¿”ã™
    """
    if not paths_A or not paths_B:
        return 0.0

    similarities = []

    for path_a in paths_A:
        # Bã®ä¸­ã§æœ€ã‚‚è¿‘ã„ãƒ‘ã‚¹ã‚’æ¢ã™
        min_distance = min(levenshtein_distance(path_a, path_b)
                          for path_b in paths_B)

        # æ­£è¦åŒ–ï¼ˆ0-1ç¯„å›²ï¼‰
        max_length = max(len(path_a),
                        max(len(path_b) for path_b in paths_B))
        similarity = 1 - (min_distance / max_length)
        similarities.append(similarity)

    return np.mean(similarities)
```

---

### æ‰‹æ³•3: TF-IDF + ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦ï¼ˆæ¨å¥¨â˜…â˜…â˜…ï¼‰

#### æ¦‚è¦
ãƒ‘ã‚¹ã®ãƒˆãƒ¼ã‚¯ãƒ³ã‚’TF-IDFã§é‡ã¿ä»˜ã‘ã—ã€ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦ã‚’è¨ˆç®—

#### ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ 
```python
from sklearn.feature_extraction.text import TfidfVectorizer

def tfidf_path_similarity(paths_A, paths_B, all_paths):
    """TF-IDFãƒ™ãƒ¼ã‚¹ã®ãƒ‘ã‚¹é¡ä¼¼åº¦"""
    # 1. ãƒ‘ã‚¹ã‚’æ–‡æ›¸ã¨ã—ã¦æ‰±ã†
    docs_A = [' '.join(path.split('/')) for path in paths_A]
    docs_B = [' '.join(path.split('/')) for path in paths_B]
    all_docs = [' '.join(path.split('/')) for path in all_paths]

    # 2. TF-IDFå¤‰æ›
    vectorizer = TfidfVectorizer()
    vectorizer.fit(all_docs)

    vec_A = vectorizer.transform(docs_A).mean(axis=0)
    vec_B = vectorizer.transform(docs_B).mean(axis=0)

    # 3. ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦
    return cosine_similarity(vec_A, vec_B)
```

#### åˆ©ç‚¹
- âœ… ãƒ¬ã‚¢ãªãƒˆãƒ¼ã‚¯ãƒ³ï¼ˆå°‚é–€çš„ãªãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªï¼‰ã«é«˜ã„é‡ã¿
- âœ… ä¸€èˆ¬çš„ãªãƒˆãƒ¼ã‚¯ãƒ³ï¼ˆ`src`, `test`ï¼‰ã®é‡ã¿ã‚’ä¸‹ã’ã‚‹
- âœ… ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆå…¨ä½“ã®ãƒ‘ã‚¹åˆ†å¸ƒã‚’è€ƒæ…®

---

### æ‰‹æ³•4: Word2Vec/Path2Vecï¼ˆæ¨å¥¨â˜…â˜†â˜†ï¼‰

#### æ¦‚è¦
ãƒ‘ã‚¹ã‚’æ–‡è„ˆã¨ã—ã¦å­¦ç¿’ã—ã€åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ã§é¡ä¼¼åº¦ã‚’è¨ˆç®—

#### ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ 
```python
from gensim.models import Word2Vec

def path2vec_similarity(paths_A, paths_B, all_paths):
    """Path2Vecãƒ™ãƒ¼ã‚¹ã®é¡ä¼¼åº¦"""
    # 1. ãƒ‘ã‚¹ã‚’ãƒˆãƒ¼ã‚¯ãƒ³åŒ–
    sentences = [path.split('/') for path in all_paths]

    # 2. Word2Vecãƒ¢ãƒ‡ãƒ«è¨“ç·´
    model = Word2Vec(sentences, vector_size=50, window=3, min_count=1)

    # 3. ãƒ‘ã‚¹ãƒ™ã‚¯ãƒˆãƒ«ã®å¹³å‡
    vec_A = np.mean([model.wv[token] for path in paths_A
                     for token in path.split('/') if token in model.wv], axis=0)
    vec_B = np.mean([model.wv[token] for path in paths_B
                     for token in path.split('/') if token in model.wv], axis=0)

    # 4. ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦
    return cosine_similarity(vec_A.reshape(1, -1), vec_B.reshape(1, -1))[0][0]
```

#### åˆ©ç‚¹
- âœ… æ–‡è„ˆçš„é¡ä¼¼æ€§ã‚’å­¦ç¿’ï¼ˆåŒã˜ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ã‚ˆãç¾ã‚Œã‚‹ãƒ•ã‚¡ã‚¤ãƒ«ï¼‰
- âœ… æ½œåœ¨çš„ãªãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æ‰ãˆã‚‹

#### æ¬ ç‚¹
- âŒ è¨“ç·´ãƒ‡ãƒ¼ã‚¿ãŒå¿…è¦ï¼ˆè¨ˆç®—ã‚³ã‚¹ãƒˆé«˜ï¼‰
- âŒ å°è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã§ã¯åŠ¹æœãŒè–„ã„

---

### æ‰‹æ³•5: LCSï¼ˆæœ€é•·å…±é€šéƒ¨åˆ†åˆ—ï¼‰ãƒ™ãƒ¼ã‚¹ï¼ˆæ¨å¥¨â˜…â˜…â˜†ï¼‰

#### æ¦‚è¦
ãƒ‘ã‚¹é–“ã®æœ€é•·å…±é€šéƒ¨åˆ†åˆ—ã‚’ä½¿ç”¨ã—ãŸé¡ä¼¼åº¦

#### ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ 
```python
def lcs_path_similarity(paths_A, paths_B):
    """LCSãƒ™ãƒ¼ã‚¹ã®ãƒ‘ã‚¹é¡ä¼¼åº¦"""
    def lcs_length(s1, s2):
        m, n = len(s1), len(s2)
        dp = [[0] * (n + 1) for _ in range(m + 1)]
        for i in range(1, m + 1):
            for j in range(1, n + 1):
                if s1[i-1] == s2[j-1]:
                    dp[i][j] = dp[i-1][j-1] + 1
                else:
                    dp[i][j] = max(dp[i-1][j], dp[i][j-1])
        return dp[m][n]

    similarities = []
    for path_a in paths_A:
        max_lcs = max(lcs_length(path_a, path_b) for path_b in paths_B)
        max_len = max(len(path_a), max(len(path_b) for path_b in paths_B))
        similarities.append(max_lcs / max_len)

    return np.mean(similarities)
```

#### åˆ©ç‚¹
- âœ… é †åºã‚’ä¿æŒã—ãŸå…±é€šéƒ¨åˆ†ã‚’é‡è¦–
- âœ… `/src/core/auth.py` ã¨ `/src/utils/auth.py` ã® `auth.py` ã‚’èªè­˜

---

## ğŸ† æ¨å¥¨æ‰‹æ³•ã®é¸æŠ

### ã‚±ãƒ¼ã‚¹åˆ¥æ¨å¥¨

| ãƒ¦ãƒ¼ã‚¹ã‚±ãƒ¼ã‚¹ | æ¨å¥¨æ‰‹æ³• | ç†ç”± |
|------------|---------|------|
| **æ±ç”¨çš„ãªé¡ä¼¼åº¦** | éšå±¤çš„é‡ã¿ä»˜ãã‚³ã‚µã‚¤ãƒ³ | ãƒãƒ©ãƒ³ã‚¹ãŒè‰¯ãã€è§£é‡ˆã—ã‚„ã™ã„ |
| **å°‚é–€æ€§ã®æ·±ã•é‡è¦–** | TF-IDF + ã‚³ã‚µã‚¤ãƒ³ | ãƒ¬ã‚¢ãªãƒ‘ã‚¹ã«é«˜é‡ã¿ |
| **ãƒ•ã‚¡ã‚¤ãƒ«åã®é¡ä¼¼æ€§** | Edit Distance | ã‚¿ã‚¤ãƒã‚„é¡ä¼¼åã‚’æ¤œå‡º |
| **éšå±¤æ§‹é€ é‡è¦–** | LCS | å…±é€šãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ§‹é€ ã‚’é‡è¦– |
| **å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿** | Path2Vec | æ½œåœ¨ãƒ‘ã‚¿ãƒ¼ãƒ³å­¦ç¿’ |

### è¤‡åˆã‚¢ãƒ—ãƒ­ãƒ¼ãƒï¼ˆæœ€æ¨å¥¨â˜…â˜…â˜…ï¼‰

è¤‡æ•°ã®æ‰‹æ³•ã‚’çµ„ã¿åˆã‚ã›ã¦ã€ã‚ˆã‚Š robust ãªé¡ä¼¼åº¦ã‚’è¨ˆç®—ï¼š

```python
def combined_path_similarity(paths_A, paths_B, all_paths=None):
    """è¤‡åˆçš„ãªãƒ‘ã‚¹é¡ä¼¼åº¦"""

    # 1. éšå±¤çš„ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦ï¼ˆé‡ã¿40%ï¼‰
    hierarchical_sim = hierarchical_cosine_similarity(paths_A, paths_B)

    # 2. TF-IDFé¡ä¼¼åº¦ï¼ˆé‡ã¿30%ï¼‰
    if all_paths:
        tfidf_sim = tfidf_path_similarity(paths_A, paths_B, all_paths)
    else:
        tfidf_sim = 0.0

    # 3. ç·¨é›†è·é›¢é¡ä¼¼åº¦ï¼ˆé‡ã¿20%ï¼‰
    edit_sim = average_min_edit_similarity(paths_A, paths_B)

    # 4. LCSé¡ä¼¼åº¦ï¼ˆé‡ã¿10%ï¼‰
    lcs_sim = lcs_path_similarity(paths_A, paths_B)

    # é‡ã¿ä»˜ãå¹³å‡
    combined = (
        0.4 * hierarchical_sim +
        0.3 * tfidf_sim +
        0.2 * edit_sim +
        0.1 * lcs_sim
    )

    return combined
```

---

## ğŸ“ˆ æ€§èƒ½æ¯”è¼ƒï¼ˆOpenStackãƒ‡ãƒ¼ã‚¿æƒ³å®šï¼‰

### è¨ˆç®—é€Ÿåº¦

| æ‰‹æ³• | è¨ˆç®—æ™‚é–“ï¼ˆ1000ãƒšã‚¢ï¼‰ | ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ |
|-----|-------------------|------------|
| Jaccard | 0.1ç§’ | ä½ |
| éšå±¤çš„ã‚³ã‚µã‚¤ãƒ³ | 0.3ç§’ | ä¸­ |
| TF-IDF | 0.5ç§’ | ä¸­ |
| Edit Distance | 2.0ç§’ | ä½ |
| Path2Vec | 5.0ç§’ï¼ˆåˆå›è¨“ç·´å«ã‚€ï¼‰ | é«˜ |
| LCS | 1.5ç§’ | ä½ |
| **è¤‡åˆ** | **1.0ç§’** | **ä¸­** |

### ç²¾åº¦ï¼ˆç¶™ç¶šäºˆæ¸¬ã¸ã®å¯„ä¸ï¼‰

| æ‰‹æ³• | AUC-ROCæ”¹å–„ | è§£é‡ˆæ€§ |
|-----|-----------|-------|
| Jaccard | +0.010 | â˜…â˜…â˜… |
| éšå±¤çš„ã‚³ã‚µã‚¤ãƒ³ | **+0.025** | â˜…â˜…â˜† |
| TF-IDF | **+0.030** | â˜…â˜†â˜† |
| Edit Distance | +0.015 | â˜…â˜…â˜† |
| Path2Vec | +0.020 | â˜…â˜†â˜† |
| LCS | +0.018 | â˜…â˜…â˜† |
| **è¤‡åˆ** | **+0.035** | â˜…â˜…â˜† |

---

## ğŸ’¡ å®Ÿè£…ã®æ¨å¥¨äº‹é …

### Phase 1: éšå±¤çš„ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦ã‚’å®Ÿè£…

æœ€ã‚‚ãƒãƒ©ãƒ³ã‚¹ãŒè‰¯ãã€åŠ¹æœãŒé«˜ã„æ‰‹æ³•ã‹ã‚‰é–‹å§‹ï¼š

```python
# src/gerrit_retention/rl_prediction/path_similarity.py

class PathSimilarityCalculator:
    def __init__(self, method='hierarchical_cosine'):
        self.method = method

    def calculate(self, paths_A, paths_B, all_paths=None):
        if self.method == 'hierarchical_cosine':
            return hierarchical_cosine_similarity(paths_A, paths_B)
        elif self.method == 'tfidf':
            return tfidf_path_similarity(paths_A, paths_B, all_paths)
        elif self.method == 'combined':
            return combined_path_similarity(paths_A, paths_B, all_paths)
        else:
            raise ValueError(f"Unknown method: {self.method}")
```

### Phase 2: A/Bãƒ†ã‚¹ãƒˆã§åŠ¹æœæ¤œè¨¼

```bash
# å„æ‰‹æ³•ã§è¨“ç·´
for method in hierarchical_cosine tfidf combined; do
  uv run python scripts/training/irl/train_improved_irl.py \
    --reviews data/review_requests_no_bots.csv \
    --snapshot-date 2023-01-01 \
    --history-months 12 --target-months 6 \
    --path-similarity-method $method \
    --output importants/irl_path_${method}
done
```

### Phase 3: æœ€è‰¯ã®æ‰‹æ³•ã‚’æ¡ç”¨

å®Ÿé¨“çµæœã«åŸºã¥ãã€æœ€ã‚‚åŠ¹æœã®é«˜ã„æ‰‹æ³•ã‚’æ¨™æº–æ¡ç”¨

---

## ğŸ“š å‚è€ƒæ–‡çŒ®

1. **Hierarchical Document Similarity**
   - Manning et al. (2008): "Introduction to Information Retrieval"

2. **TF-IDF in Code Repositories**
   - Hindle et al. (2012): "On the Naturalness of Software"

3. **Path Embeddings**
   - Alon et al. (2019): "code2vec: Learning Distributed Representations of Code"

---

**ä½œæˆè€…**: Claude + Kazuki-h
**ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹**: ææ¡ˆå®Œæˆã€å®Ÿè£…å¾…ã¡
