# LSTM æ™‚ç³»åˆ—å­¦ç¿’ã®ä¿®æ­£

**ä½œæˆæ—¥**: 2025-10-22  
**å•é¡Œ**: äºˆæ¸¬ç²¾åº¦ãŒæ¥µç«¯ã«æ‚ªã„ï¼ˆAUC-ROC=0.5, pred_mean=1.0ï¼‰  
**åŸå› **: æ™‚ç³»åˆ—çŠ¶æ…‹ã®ä¸é©åˆ‡ãªå®Ÿè£…

---

## ğŸ”´ ç™ºè¦‹ã•ã‚ŒãŸå•é¡Œ

### å•é¡Œ 1: å…¨ã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ—ã§åŒã˜çŠ¶æ…‹ã‚’ä½¿ç”¨

**Beforeï¼ˆèª¤ã‚Šï¼‰**:

```python
# âŒ å…¨ã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ—ã§åŒã˜çŠ¶æ…‹
state_tensors = [self.state_to_tensor(state) for _ in range(self.seq_len)]
state_seq = torch.stack(state_tensors)  # [20, state_dim]

# ä¾‹: 20ã‚¹ãƒ†ãƒƒãƒ—ã™ã¹ã¦ã§åŒã˜çŠ¶æ…‹
state_seq = [
    [0.5, 0.3, 0.2, ...],  # ã‚¹ãƒ†ãƒƒãƒ—1ï¼ˆ19ãƒ¶æœˆå‰ï¼‰â† åŒã˜
    [0.5, 0.3, 0.2, ...],  # ã‚¹ãƒ†ãƒƒãƒ—2ï¼ˆ18ãƒ¶æœˆå‰ï¼‰â† åŒã˜
    ...
    [0.5, 0.3, 0.2, ...],  # ã‚¹ãƒ†ãƒƒãƒ—20ï¼ˆç¾åœ¨ï¼‰  â† åŒã˜
]

çµæœ:
- LSTM ãŒ state ã‹ã‚‰å­¦ç¿’ã§ããªã„
- action ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã®ã¿ã‚’è¦‹ã‚‹
- ã—ã‹ã— action ã ã‘ã§ã¯äºˆæ¸¬å›°é›£
- â†’ ãƒ¢ãƒ‡ãƒ«ãŒå¹³å‡å€¤ï¼ˆç¶™ç¶šç‡70%ï¼‰ã«åæŸ
```

**Afterï¼ˆæ­£ã—ã„ï¼‰**:

```python
# âœ… å„ã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ—ã§ãã®æ™‚ç‚¹ã®çŠ¶æ…‹ã‚’å†æ§‹ç¯‰
step_dates = []
for i in range(self.seq_len):
    days_before = (self.seq_len - 1 - i) * 30
    step_date = context_date - timedelta(days=days_before)
    step_dates.append(step_date)

state_tensors = []
for i, step_date in enumerate(step_dates):
    # ãã®æ™‚ç‚¹ã¾ã§ã®å±¥æ­´
    history_up_to_step = [
        act for act in activity_history
        if act.get('timestamp', context_date) <= step_date
    ]

    # ãã®æ™‚ç‚¹ã§ã®çŠ¶æ…‹ã‚’è¨ˆç®—
    state_at_step = self.extract_developer_state(
        developer,
        history_up_to_step,
        step_date
    )

    state_tensors.append(self.state_to_tensor(state_at_step))

state_seq = torch.stack(state_tensors)  # [20, state_dim]

# ä¾‹: å„ã‚¹ãƒ†ãƒƒãƒ—ã§ç•°ãªã‚‹çŠ¶æ…‹
state_seq = [
    [0.1, 0.2, 0.1, ...],  # ã‚¹ãƒ†ãƒƒãƒ—1ï¼ˆ19ãƒ¶æœˆå‰ï¼‰â† åˆæœŸçŠ¶æ…‹
    [0.2, 0.3, 0.2, ...],  # ã‚¹ãƒ†ãƒƒãƒ—2ï¼ˆ18ãƒ¶æœˆå‰ï¼‰â† å°‘ã—æˆé•·
    [0.3, 0.4, 0.3, ...],  # ã‚¹ãƒ†ãƒƒãƒ—3
    ...
    [0.5, 0.6, 0.5, ...],  # ã‚¹ãƒ†ãƒƒãƒ—20ï¼ˆç¾åœ¨ï¼‰  â† ç¾åœ¨ã®çŠ¶æ…‹
]

çµæœ:
- LSTM ãŒ state ã®æ™‚ç³»åˆ—å¤‰åŒ–ã‚’å­¦ç¿’ã§ãã‚‹
- çµŒé¨“ã®è“„ç©ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æ‰ãˆã‚‰ã‚Œã‚‹
- action ã¨ state ã®ç›¸äº’ä½œç”¨ã‚’å­¦ç¿’
```

### å•é¡Œ 2: ã‚¯ãƒ©ã‚¹ä¸å‡è¡¡ï¼ˆç¶™ç¶šç‡ 70%ï¼‰

**å•é¡Œ**:

```python
ç¶™ç¶šç‡ = 70%
â†’ ã€Œå…¨å“¡ç¶™ç¶šã€ã¨äºˆæ¸¬ã™ã‚Œã° 70% æ­£è§£
â†’ ãƒ¢ãƒ‡ãƒ«ãŒå±€æ‰€æœ€é©è§£ã«é™¥ã‚‹
â†’ pred_mean = 1.0, AUC-ROC = 0.5
```

**è§£æ±ºç­–**: Focal Loss + ã‚¯ãƒ©ã‚¹ã‚¦ã‚§ã‚¤ãƒˆ

```python
# Focal Loss: é›£ã—ã„ã‚µãƒ³ãƒ—ãƒ«ã«ç„¦ç‚¹
def focal_loss(self, pred, target, alpha=0.25, gamma=2.0):
    bce = F.binary_cross_entropy(pred, target, reduction='none')
    p_t = torch.where(target == 1, pred, 1 - pred)
    focal_weight = (1 - p_t) ** gamma  # é›£ã—ã„ã‚µãƒ³ãƒ—ãƒ«ï¼ˆp_t â‰ˆ 0.5ï¼‰ã®é‡ã¿ã‚’ä¸Šã’ã‚‹
    loss = alpha * focal_weight * bce
    return loss.mean()

# ã‚¯ãƒ©ã‚¹ã‚¦ã‚§ã‚¤ãƒˆ: å°‘æ•°æ´¾ï¼ˆéç¶™ç¶šè€…ï¼‰ã®é‡ã¿ã‚’ä¸Šã’ã‚‹
if positive_rate > 0.5:
    pos_weight = 1.0
    neg_weight = positive_rate / (1 - positive_rate)  # ä¾‹: 70%/30% = 2.33
else:
    pos_weight = (1 - positive_rate) / positive_rate
    neg_weight = 1.0

# æå¤±è¨ˆç®—
continuation_loss = self.focal_loss(pred, target) * sample_weight
```

### å•é¡Œ 3: å­¦ç¿’ç‡ãŒé«˜ã™ãã‚‹

**Before**: `lr = 0.001`

- åˆæœŸæå¤±: 0.0155
- ã‚¨ãƒãƒƒã‚¯ 10: æå¤± = 0.0000 â† ç•°å¸¸ã«æ—©ã„åæŸ

**After**: `lr = 0.0001`

- ã‚ˆã‚Šæ…é‡ãªå­¦ç¿’
- å±€æ‰€æœ€é©è§£ã‚’å›é¿

---

## ğŸ“ ä¿®æ­£å†…å®¹

### 1. æ™‚ç³»åˆ—çŠ¶æ…‹ã®å†æ§‹ç¯‰ï¼ˆè¨“ç·´æ™‚ï¼‰

**ãƒ•ã‚¡ã‚¤ãƒ«**: `src/gerrit_retention/rl_prediction/retention_irl_system.py`  
**é–¢æ•°**: `train_irl`  
**è¡Œ**: 352-416

```python
# Beforeï¼ˆèª¤ã‚Šï¼‰
state_tensors = [self.state_to_tensor(state) for _ in range(self.seq_len)]

# Afterï¼ˆæ­£ã—ã„ï¼‰
# å„ã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ—ã§ã®æ—¥ä»˜ã‚’è¨ˆç®—
step_dates = []
for i in range(self.seq_len):
    days_before = (self.seq_len - 1 - i) * 30
    step_date = context_date - timedelta(days=days_before)
    step_dates.append(step_date)

# å„ã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ—ã§çŠ¶æ…‹ã‚’å†æ§‹ç¯‰
state_tensors = []
for i, step_date in enumerate(step_dates):
    history_up_to_step = [
        act for act in activity_history
        if act.get('timestamp', context_date) <= step_date
    ]

    if history_up_to_step:
        state_at_step = self.extract_developer_state(
            developer,
            history_up_to_step,
            step_date
        )
    else:
        state_at_step = state  # åˆæœŸçŠ¶æ…‹

    state_tensors.append(self.state_to_tensor(state_at_step))

state_seq = torch.stack(state_tensors).unsqueeze(0)
```

### 2. æ™‚ç³»åˆ—çŠ¶æ…‹ã®å†æ§‹ç¯‰ï¼ˆäºˆæ¸¬æ™‚ï¼‰

**ãƒ•ã‚¡ã‚¤ãƒ«**: `src/gerrit_retention/rl_prediction/retention_irl_system.py`  
**é–¢æ•°**: `predict_continuation_probability`  
**è¡Œ**: 548-592

åŒæ§˜ã®ãƒ­ã‚¸ãƒƒã‚¯ã‚’äºˆæ¸¬é–¢æ•°ã«ã‚‚é©ç”¨ã€‚

### 3. Focal Loss ã®è¿½åŠ 

**ãƒ•ã‚¡ã‚¤ãƒ«**: `src/gerrit_retention/rl_prediction/retention_irl_system.py`  
**è¡Œ**: 306-320

```python
def focal_loss(self, pred, target, alpha=0.25, gamma=2.0):
    """
    Focal Loss: é›£ã—ã„ã‚µãƒ³ãƒ—ãƒ«ã«ç„¦ç‚¹ã‚’å½“ã¦ã‚‹
    """
    bce = torch.nn.functional.binary_cross_entropy(pred, target, reduction='none')
    p_t = torch.where(target == 1, pred, 1 - pred)
    focal_weight = (1 - p_t) ** gamma
    loss = alpha * focal_weight * bce
    return loss.mean()
```

### 4. ã‚¯ãƒ©ã‚¹ã‚¦ã‚§ã‚¤ãƒˆã®è¨ˆç®—

**ãƒ•ã‚¡ã‚¤ãƒ«**: `src/gerrit_retention/rl_prediction/retention_irl_system.py`  
**è¡Œ**: 335-352

```python
# ã‚¯ãƒ©ã‚¹ä¸å‡è¡¡ã‚’è¨ˆç®—
positive_count = sum(1 for t in expert_trajectories if t.get('continued', True))
negative_count = len(expert_trajectories) - positive_count
positive_rate = positive_count / len(expert_trajectories)

# ã‚¯ãƒ©ã‚¹ã‚¦ã‚§ã‚¤ãƒˆã‚’è¨ˆç®—
if positive_rate > 0.5:
    pos_weight = 1.0
    neg_weight = positive_rate / (1 - positive_rate)
else:
    pos_weight = (1 - positive_rate) / positive_rate
    neg_weight = 1.0

logger.info(f"ç¶™ç¶šç‡: {positive_rate:.1%}")
logger.info(f"ã‚¯ãƒ©ã‚¹ã‚¦ã‚§ã‚¤ãƒˆ: ç¶™ç¶š={pos_weight:.2f}, éç¶™ç¶š={neg_weight:.2f}")
```

### 5. æå¤±è¨ˆç®—ã®ä¿®æ­£

**ãƒ•ã‚¡ã‚¤ãƒ«**: `src/gerrit_retention/rl_prediction/retention_irl_system.py`  
**è¡Œ**: 438-453

```python
# Before
continuation_loss = self.bce_loss(predicted_continuation, target_continuation)

# After
continuation_loss = self.focal_loss(
    predicted_continuation,
    target_continuation,
    alpha=0.25,
    gamma=2.0
)

# ã‚¯ãƒ©ã‚¹ã‚¦ã‚§ã‚¤ãƒˆã‚’è¿½åŠ ã§é©ç”¨
sample_weight = pos_weight if continuation_label else neg_weight
continuation_loss = continuation_loss * sample_weight
```

### 6. å­¦ç¿’ç‡ã®å‰Šæ¸›

**ãƒ•ã‚¡ã‚¤ãƒ«**: `src/gerrit_retention/rl_prediction/retention_irl_system.py`  
**è¡Œ**: 151-155

```python
# Before
lr=config.get('learning_rate', 0.001)

# After
lr=config.get('learning_rate', 0.0001)  # 10åˆ†ã®1ã«å‰Šæ¸›
```

---

## ğŸ“Š æœŸå¾…ã•ã‚Œã‚‹æ”¹å–„

### Beforeï¼ˆä¿®æ­£å‰ï¼‰

```
è¨“ç·´:
  ã‚¨ãƒãƒƒã‚¯0:  æå¤± = 0.0155
  ã‚¨ãƒãƒƒã‚¯10: æå¤± = 0.0000  â† ç•°å¸¸ã«æ—©ã„
  ã‚¨ãƒãƒƒã‚¯30: æå¤± = 0.0000

è©•ä¾¡:
  pred_mean: 1.0              â† å…¨å“¡ã€Œç¶™ç¶šã€äºˆæ¸¬
  pred_std:  0.0              â† åˆ†æ•£ã‚¼ãƒ­
  AUC-ROC:   0.5              â† ãƒ©ãƒ³ãƒ€ãƒ äºˆæ¸¬
  AUC-PR:    0.849            â† ç¶™ç¶šç‡ãŒé«˜ã„ã ã‘
  F1:        0.823            â† Recall=1.0ã®ã¿
  Precision: 0.699 (=ç¶™ç¶šç‡) â† å…¨å“¡äºˆæ¸¬ã®çµæœ
  Recall:    1.0              â† å…¨å“¡äºˆæ¸¬
```

### Afterï¼ˆä¿®æ­£å¾Œãƒ»æœŸå¾…å€¤ï¼‰

```
è¨“ç·´:
  ã‚¨ãƒãƒƒã‚¯0:  æå¤± = 0.15-0.30   â† ã‚ˆã‚Šé«˜ã„åˆæœŸæå¤±
  ã‚¨ãƒãƒƒã‚¯10: æå¤± = 0.10-0.15   â† ç·©ã‚„ã‹ãªåæŸ
  ã‚¨ãƒãƒƒã‚¯30: æå¤± = 0.05-0.10   â† ç¶™ç¶šçš„ãªæ”¹å–„

è©•ä¾¡:
  pred_mean: 0.5-0.7           â† å¤šæ§˜ãªäºˆæ¸¬
  pred_std:  0.1-0.2           â† é©åˆ‡ãªåˆ†æ•£
  AUC-ROC:   0.65-0.75         â† è­˜åˆ¥èƒ½åŠ›ã‚ã‚Š âœ…
  AUC-PR:    0.70-0.80         â† æ”¹å–„
  F1:        0.60-0.75         â† ãƒãƒ©ãƒ³ã‚¹ã®è‰¯ã„äºˆæ¸¬
  Precision: 0.60-0.70         â† é©åˆ‡ãªç²¾åº¦
  Recall:    0.70-0.85         â† éå‰°äºˆæ¸¬ã®å›é¿
```

---

## ğŸ¯ ç†è«–çš„æ ¹æ‹ 

### ãªãœå„ã‚¹ãƒ†ãƒƒãƒ—ã§çŠ¶æ…‹ã‚’å†æ§‹ç¯‰ã™ã‚‹ã®ã‹ï¼Ÿ

**æ™‚ç³»åˆ—ã®æœ¬è³ª**:

```
é–‹ç™ºè€…ã®æˆé•·ãƒ—ãƒ­ã‚»ã‚¹:
t=0ï¼ˆ19ãƒ¶æœˆå‰ï¼‰: åˆå¿ƒè€…ã€çµŒé¨“å°‘ãªã„ã€ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ1å€‹
t=10ï¼ˆ9ãƒ¶æœˆå‰ï¼‰: ä¸­ç´šè€…ã€çµŒé¨“å¢—åŠ ã€ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ3å€‹
t=20ï¼ˆç¾åœ¨ï¼‰:    ãƒ™ãƒ†ãƒ©ãƒ³ã€çµŒé¨“è±Šå¯Œã€ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ5å€‹

â†’ å„æ™‚ç‚¹ã§ã€Œç•°ãªã‚‹çŠ¶æ…‹ã€ã«ã„ã‚‹
â†’ çŠ¶æ…‹ã®å¤‰åŒ–ãƒ‘ã‚¿ãƒ¼ãƒ³ãŒç¶™ç¶šæ€§ã‚’äºˆæ¸¬ã™ã‚‹éµ
```

**LSTM ãŒå­¦ç¿’ã™ã¹ããƒ‘ã‚¿ãƒ¼ãƒ³**:

1. **æˆé•·ãƒ‘ã‚¿ãƒ¼ãƒ³**: çµŒé¨“ãŒé †èª¿ã«å¢—åŠ  â†’ ç¶™ç¶šã—ã‚„ã™ã„
2. **åœæ»ãƒ‘ã‚¿ãƒ¼ãƒ³**: çµŒé¨“ãŒå¢—ãˆãªã„ â†’ é›¢è„±ãƒªã‚¹ã‚¯
3. **åŠ é€Ÿãƒ‘ã‚¿ãƒ¼ãƒ³**: æ´»å‹•é »åº¦ãŒå¢—åŠ  â†’ é«˜ã„ç¶™ç¶šæ€§
4. **æ¸›é€Ÿãƒ‘ã‚¿ãƒ¼ãƒ³**: æ´»å‹•é »åº¦ãŒæ¸›å°‘ â†’ é›¢è„±ã®å‰å…†

**èª¤ã£ãŸå®Ÿè£…ã®å•é¡Œ**:

```python
# å…¨ã‚¹ãƒ†ãƒƒãƒ—ã§åŒã˜çŠ¶æ…‹
state = [çµŒé¨“5å¹´, ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ5å€‹, ...]

â†’ LSTMã¯ã€Œç¾åœ¨ã®çŠ¶æ…‹ã€ã—ã‹è¦‹ãˆãªã„
â†’ ã€Œæˆé•·ãƒ‘ã‚¿ãƒ¼ãƒ³ã€ã‚’å­¦ç¿’ã§ããªã„
â†’ actionã®æ™‚ç³»åˆ—ã®ã¿ã«ä¾å­˜
â†’ ã—ã‹ã—actionã ã‘ã§ã¯ä¸ååˆ†
â†’ ãƒ¢ãƒ‡ãƒ«ãŒå¹³å‡å€¤ã«åæŸ
```

---

## ğŸš€ æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—

### 1. æ—¢å­˜ãƒ¢ãƒ‡ãƒ«ã‚’å‰Šé™¤

```bash
cd /Users/kazuki-h/rl/gerrit-retention
rm -rf outputs/cross_evaluation/model_*
```

### 2. å†è¨“ç·´

```bash
bash scripts/training/irl/run_cross_evaluation.sh
```

### 3. çµæœã®ç¢ºèª

```bash
# è¨“ç·´ãƒ­ã‚°ã‚’ç¢ºèªï¼ˆæå¤±ã®å¤‰åŒ–ï¼‰
tail -50 outputs/cross_evaluation/model_0_6m/training.log

# è©•ä¾¡çµæœã‚’ç¢ºèª
cat outputs/cross_evaluation/model_0_6m/evaluation_results.json

# æœŸå¾…ã•ã‚Œã‚‹æ”¹å–„:
# - pred_mean: 0.5-0.7 ï¼ˆç¾çŠ¶: 1.0ï¼‰
# - pred_std: > 0.05 ï¼ˆç¾çŠ¶: 0.0ï¼‰
# - AUC-ROC: > 0.60 ï¼ˆç¾çŠ¶: 0.5ï¼‰
```

### 4. è©³ç´°ãªè©•ä¾¡

```bash
# ã‚¯ãƒ­ã‚¹è©•ä¾¡çµæœ
cat outputs/cross_evaluation/cross_evaluation_results.csv

# ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ç¢ºèª
open outputs/cross_evaluation/cross_evaluation_heatmaps.png
```

---

## ğŸ”§ ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°

### ã‚¨ãƒ©ãƒ¼: timestamp å±æ€§ãŒãªã„

```python
# ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
KeyError: 'timestamp'

# åŸå› 
activity_history ã®å„è¦ç´ ã« 'timestamp' ã‚­ãƒ¼ãŒãªã„

# è§£æ±ºç­–
ãƒ‡ãƒ¼ã‚¿æŠ½å‡ºã‚¹ã‚¯ãƒªãƒ—ãƒˆã§ timestamp ã‚’è¿½åŠ 
ã¾ãŸã¯ã€åˆ¥ã®ã‚­ãƒ¼åï¼ˆ'date', 'request_time'ï¼‰ã‚’ä½¿ç”¨
```

### ä¾ç„¶ã¨ã—ã¦ pred_mean = 1.0

```python
# è€ƒãˆã‚‰ã‚Œã‚‹åŸå› 
1. ãƒ‡ãƒ¼ã‚¿ã®ç¶™ç¶šç‡ãŒéå¸¸ã«é«˜ã„ï¼ˆ> 90%ï¼‰
   â†’ ã•ã‚‰ã«å¼·ã„ã‚¯ãƒ©ã‚¹ã‚¦ã‚§ã‚¤ãƒˆãŒå¿…è¦

2. å­¦ç¿’ç‡ãŒã¾ã é«˜ã„
   â†’ 0.00001 ã«ä¸‹ã’ã‚‹

3. Focal Loss ã® gamma ãŒä¸é©åˆ‡
   â†’ gamma=3.0 ã¾ãŸã¯ 4.0 ã«ä¸Šã’ã‚‹
```

### è¨“ç·´ãŒé…ã„

```python
# å„ã‚¹ãƒ†ãƒƒãƒ—ã§çŠ¶æ…‹ã‚’å†è¨ˆç®—ã™ã‚‹ãŸã‚é…ããªã‚‹

# é«˜é€ŸåŒ–ã‚ªãƒ—ã‚·ãƒ§ãƒ³:
1. çŠ¶æ…‹ã‚’äº‹å‰è¨ˆç®—ã—ã¦ã‚­ãƒ£ãƒƒã‚·ãƒ¥
2. ãƒãƒƒãƒå‡¦ç†ã®æœ€é©åŒ–
3. GPUä½¿ç”¨ï¼ˆCUDAï¼‰
```

---

## ğŸ“š é–¢é€£ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ

- [ç²¾åº¦å•é¡Œã®åˆ†æã¨è§£æ±ºç­–.md](./ç²¾åº¦å•é¡Œã®åˆ†æã¨è§£æ±ºç­–.md) - é–¾å€¤å•é¡Œã®åˆ†æ
- [é–¾å€¤çµ±ä¸€å®Ÿè£…ã®è©³ç´°.md](./é–¾å€¤çµ±ä¸€å®Ÿè£…ã®è©³ç´°.md) - é–¾å€¤ã®çµ±ä¸€
- [IRL è¨­è¨ˆã¨å®Ÿé¨“çµæœã‚µãƒãƒªãƒ¼.md](./IRLè¨­è¨ˆã¨å®Ÿé¨“çµæœã‚µãƒãƒªãƒ¼.md) - å…¨ä½“ã‚µãƒãƒªãƒ¼

---

**æœ€çµ‚æ›´æ–°**: 2025-10-22  
**å®Ÿè£…ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹**: âœ… å®Œäº†  
**æ¬¡ã®ã‚¢ã‚¯ã‚·ãƒ§ãƒ³**: ãƒ¢ãƒ‡ãƒ«ã®å†è¨“ç·´ã¨è©•ä¾¡
