# OSS レビュアー継続予測のための逆強化学習手法：詳細解説

## 1. 研究の背景と目的

### 1.1 研究の動機

オープンソースソフトウェア（OSS）プロジェクトでは、コードレビューが品質保証の重要な役割を果たしている。しかし、レビュアーの継続的な参加を維持することは大きな課題である。レビュアーの離脱は、レビューの遅延やプロジェクトの停滞を引き起こす可能性がある。

### 1.2 研究の目的

本研究では、**レビュアーの継続的な参加を予測**することを目的とする。具体的には：

1. **短期・中期・長期の継続予測**：0-3 ヶ月、3-6 ヶ月、6-9 ヶ月、9-12 ヶ月後の活動継続を予測
2. **時系列パターンの学習**：過去の活動履歴から継続に寄与する要因を学習
3. **逆強化学習の活用**：専門家（継続した開発者）の行動から報酬関数を学習

### 1.3 本研究のアプローチ

本研究では、以下の 3 つの特徴的な手法を採用する：

- **スライディングウィンドウラベル**：互いに排他的な時間窓（0-3m, 3-6m, 6-9m, 9-12m）を定義し、各時間スケールに特有の継続パターンを独立して学習
- **月次集約ラベル**：各月末を基準点として継続ラベルを付与し、同一月内の活動に一貫したラベルを提供することで、学習の安定性を向上
- **同一プロジェクト継続判定**：履歴期間内で活動したプロジェクトでの継続のみを評価し、プロジェクト間移動を離脱とみなすことで、コミュニティ内での継続を明確に定義

---

## 2. データ収集

### 2.1 データソース

**OpenStack Gerrit**を対象とする。Gerrit は、Git ベースのコードレビューシステムであり、すべてのレビュー活動が記録される。

### 2.2 収集データ

#### 2.2.1 データ収集方法

**Gerrit REST API**（`https://review.opendev.org/changes/`）を使用してデータを収集。クエリパラメータでプロジェクトや期間を指定し、レビュー依頼とその詳細情報を取得する。

#### 2.2.2 収集データの概要

以下の情報を収集：

1. **レビュー依頼（Change）**：変更 ID、プロジェクト名、作成日時、ステータス、コード変更量（追加/削除行数）
2. **レビュアー情報**：レビュアーのメールアドレス、名前、割り当て日時
3. **レビュー活動**：レビューコメント、投票スコア（Code-Review: -2 ～ +2、Verified: -1 ～ +1）、応答時刻
4. **活動履歴**：過去の活動統計（30 日/90 日/180 日間のレビュー数、応答率、在籍日数）
5. **相互作用履歴**：レビュアーと変更作成者間の過去の相互作用回数

これらの生データから、後述の**状態特徴量**（レビュアーの経験、活動パターン、品質スコア等）と**行動特徴量**（各活動の種類、規模、品質等）を抽出する。

### 2.3 データ量

| 項目                     | 値                              |
| ------------------------ | ------------------------------- |
| **対象期間**             | 5 年（2018-2023）               |
| **総レビュー依頼数**     | 約 137,000 件                   |
| **ユニークレビュアー数** | 623 人（複数プロジェクト）      |
| **ユニークレビュアー数** | 241 人（nova 単一）             |
| **総活動数**             | 約 60,000 イベント              |
| **対象プロジェクト数**   | 10+（nova, neutron, cinder 等） |

### 2.4 対象プロジェクト

- **複数プロジェクト実験**：OpenStack 全体（nova, neutron, cinder, glance, keystone 等）
- **単一プロジェクト実験**：openstack/nova のみ

---

## 3. データ前処理

### 3.1 データクレンジング

1. **Bot アカウントの除外**：自動化された Bot アカウント（CI/CD 等）を除外
2. **重複の除去**：同一レビュアー × 同一変更の重複レコードを削除
3. **異常値の除去**：タイムスタンプが範囲外のレコードを除外

### 3.2 時系列整列

すべてのデータを`request_time`（レビュー依頼時刻）でソートし、時系列順に整列する。

### 3.3 プロジェクトフィルタ

実験設定に応じて、特定のプロジェクトのみを抽出。

---

## 4. ラベル付け：月次集約スライディングウィンドウラベル

### 4.1 ラベル付けの原理

#### 4.1.1 ラベルが表現するもの

本研究のラベルは、**ある時点でのレビュアーの「継続可能性」を表す二値変数**である。具体的には：

- **ラベル = True（1）**：基準時点から指定期間後に活動がある → 継続している
- **ラベル = False（0）**：基準時点から指定期間後に活動がない → 継続していない

このラベルは、単なる「過去の事実」ではなく、**「その時点での状態が、将来の継続とどう関連しているか」を学習するための教師信号**として機能する。

#### 4.1.2 月次集約の必要性

レビュアーの活動は不規則であり、日単位でラベルを付与すると以下の問題が生じる：

1. **ラベルの不安定性**：隣接する日で大きく異なるラベルが付与される可能性
2. **学習の非効率性**：同じレビュアーの類似した状態に対して矛盾したラベルが生成される
3. **過学習のリスク**：細かすぎる時間粒度により、ノイズを学習してしまう

**解決策**：各月の月末を**基準時点**として、その月内の全活動に同一のラベルを付与する。これにより、**月単位での継続パターン**を学習し、より一般化された予測モデルを構築できる。

#### 4.1.3 基準時点の設定

各活動について：

1. **活動日から月を特定**：例）2022-01-05 → 2022 年 1 月
2. **その月の最終日を基準時点とする**：例）2022-01-31
3. **基準時点から将来を見る**：例）2022-01-31 から 0 ～ 3 ヶ月後（2022-01-31 ～ 2022-04-30）
4. **将来窓内に活動があるか判定**：ある → True、ない → False
5. **その月の全活動に同じラベルを付与**

この仕組みにより、**「2022 年 1 月時点で活動していたレビュアーは、0 ～ 3 ヶ月後も継続するか？」という問いに対する答え**をラベルとして表現する。

#### 4.1.4 スライディングウィンドウの意味

本研究では、各期間を**互いに排他的な独立した窓**として定義する：

- **0-3m**：0 ～ 3 ヶ月後の活動（短期）
- **3-6m**：3 ～ 6 ヶ月後の活動（中期）← **0-3m は含まない**
- **6-9m**：6 ～ 9 ヶ月後の活動（長期）← **0-6m は含まない**
- **9-12m**：9 ～ 12 ヶ月後の活動（超長期）← **0-9m は含まない**

この設計により、**各時間スケールに特有の継続パターン**を独立して学習できる。各時間窓で重要となる特徴は異なると考えられる：

- **短期継続（0-3m）**：最近の活動頻度や活動トレンドが重要な予測因子
- **中期継続（3-6m）**：プロジェクトへのコミットメントや協力度が重要
- **長期継続（6-9m）**：コミュニティとの強い結びつきや経験の深さが重要

各時間スケール専用のモデルを訓練することで、時間軸に沿った継続性の変化を捉えることができる。

#### 4.1.5 学習における役割

このラベルは、**逆強化学習における報酬関数の学習**に使用される：

1. **正例（True）**：継続したレビュアーの活動パターンから、「継続を促進する要因」を学習
2. **負例（False）**：離脱したレビュアーの活動パターンから、「離脱のシグナル」を学習

LSTM は、過去の活動シーケンスから時系列パターンを抽出し、各時点での「継続確率」を予測する。このとき、月次集約ラベルが**各月の代表的な継続状態**を示すことで、安定した学習が可能になる。

### 4.2 スライディングウィンドウの定義

各期間の詳細な定義：

| ラベル    | 範囲                          | 基準時点からの期間                  | 説明       |
| --------- | ----------------------------- | ----------------------------------- | ---------- |
| **0-3m**  | 0 ～ 3 ヶ月後                 | [基準日, 基準日 + 3 ヶ月)           | 短期継続   |
| **3-6m**  | 3 ～ 6 ヶ月後（0-3m は除く）  | [基準日 + 3 ヶ月, 基準日 + 6 ヶ月)  | 中期継続   |
| **6-9m**  | 6 ～ 9 ヶ月後（0-6m は除く）  | [基準日 + 6 ヶ月, 基準日 + 9 ヶ月)  | 長期継続   |
| **9-12m** | 9 ～ 12 ヶ月後（0-9m は除く） | [基準日 + 9 ヶ月, 基準日 + 12 ヶ月) | 超長期継続 |

**重要な特徴**：各窓は互いに**排他的（重複なし）**であり、異なる時間スケールでの継続パターンを独立して学習できる。

### 4.3 月次集約の仕組み

1. **月の特定**：各活動の日付から月を特定（例：2022-01-15 → 2022 年 1 月）
2. **月末の計算**：その月の最終日を計算（例：2022-01-31）
3. **将来窓の計算**：月末から指定された期間の窓を計算（例：月末 + 0 ～ 3 ヶ月）
4. **継続判定**：その窓内に活動があるかを判定（履歴期間内のプロジェクトのみ）
5. **ラベル付与**：同じ月の全活動に同じラベルを付与

### 4.4 具体例

#### 4.4.1 時系列データの例

**レビュアー A さんの活動履歴（nova プロジェクト）：**

```
学習期間内：
  2022-01-05：レビュー 1 回目（nova）
  2022-01-15：レビュー 2 回目（nova）
  2022-01-25：レビュー 3 回目（nova）

将来活動：
  2022-02-10：レビュー 4 回目（nova）← 1 月末から 0.3 ヶ月後
  2022-05-20：レビュー 5 回目（nova）← 1 月末から 3.6 ヶ月後
```

#### 4.4.2 0-3m ラベルの計算プロセス

**ステップ 1：基準時点の決定**

- 1 月の活動（1/5, 1/15, 1/25）→ 基準時点 = 2022-01-31（1 月末）

**ステップ 2：将来窓の設定**

- 開始：2022-01-31（0 ヶ月後）
- 終了：2022-04-30（3 ヶ月後）
- 窓：[2022-01-31, 2022-04-30)

**ステップ 3：将来活動の確認**

- 2022-02-10 の活動が窓内に存在 ✓
- 同じプロジェクト（nova）✓
- → 判定：**True（継続している）**

**ステップ 4：ラベル付与**

1 月の全活動に True を付与：

| 活動日     | ラベル | 意味                                  |
| ---------- | ------ | ------------------------------------- |
| 2022-01-05 | True   | 1 月末時点で、0-3m 後も継続すると判定 |
| 2022-01-15 | True   | （同上）                              |
| 2022-01-25 | True   | （同上）                              |

**学習への影響**：LSTM は 1 月の活動パターンと「True（継続）」ラベルの関係を学習し、「このような活動パターンを持つレビュアーは 0-3 ヶ月後も継続する」という知識を獲得する。

#### 4.4.3 3-6m ラベルの計算プロセス

**ステップ 1：基準時点の決定**

- 同じく 2022-01-31（1 月末）

**ステップ 2：将来窓の設定**

- 開始：2022-04-30（3 ヶ月後）← **0-3m を除外**
- 終了：2022-07-31（6 ヶ月後）
- 窓：[2022-04-30, 2022-07-31)

**ステップ 3：将来活動の確認**

- 2022-05-20 の活動が窓内に存在 ✓
- 同じプロジェクト（nova）✓
- → 判定：**True（継続している）**

**ステップ 4：ラベル付与**

| 活動日     | ラベル | 意味                                  |
| ---------- | ------ | ------------------------------------- |
| 2022-01-05 | True   | 1 月末時点で、3-6m 後も継続すると判定 |
| 2022-01-15 | True   | （同上）                              |
| 2022-01-25 | True   | （同上）                              |

**学習への影響**：LSTM は同じ 1 月の活動パターンでも、「3-6m 後も継続する」という**より長期的な継続パターン**を学習する。0-3m モデルとは異なる特徴量の重要度を学習する可能性がある。

#### 4.4.4 ラベル間の独立性の実証

同じレビュアー A さんで、異なるラベルを比較：

| 訓練ラベル | 将来窓                   | 将来活動 | 判定  | 意味                 |
| ---------- | ------------------------ | -------- | ----- | -------------------- |
| 0-3m       | 2022-01-31 ～ 2022-04-30 | 2/10     | True  | 短期で継続           |
| 3-6m       | 2022-04-30 ～ 2022-07-31 | 5/20     | True  | 中期でも継続         |
| 6-9m       | 2022-07-31 ～ 2022-10-31 | なし     | False | 長期では離脱         |
| 9-12m      | 2022-10-31 ～ 2023-01-31 | なし     | False | 超長期では離脱が継続 |

このように、**同じ 1 月の活動パターンに対して、異なる将来窓で異なるラベルが付与される**。これにより、モデルは「短期では継続するが長期では離脱するパターン」のような、時間スケールに応じた継続性の違いを学習できる。

### 4.5 同一プロジェクト継続判定

#### 4.5.1 制約の必要性

レビュアーが複数のプロジェクトで活動する場合、以下の問題が生じる：

1. **継続の定義の曖昧さ**：プロジェクト A で活動していたレビュアーがプロジェクト B に移動した場合、これを「継続」とみなすべきか？
2. **予測対象の不明確さ**：どのプロジェクトでの継続を予測しているのか明確でない
3. **コミュニティの違い**：異なるプロジェクトは異なるコミュニティ文化を持ち、継続要因も異なる可能性がある

#### 4.5.2 同一プロジェクト制約の定義

本研究では、**履歴期間内で活動しているプロジェクトのセット**を定義し、**そのプロジェクト内での将来活動のみを継続とみなす**：

**アルゴリズム：**

1. **履歴プロジェクトの特定**：学習期間内でレビュアーが活動した全プロジェクトを抽出
   - 例：`{nova, cinder}`
2. **将来活動のフィルタリング**：将来窓内の活動を履歴プロジェクトでフィルタ
   - 例：将来活動が `neutron` → **カウントしない**
   - 例：将来活動が `nova` → **カウントする** ✓
3. **継続判定**：フィルタ後の活動が 1 つでもあれば継続、なければ非継続

#### 4.5.3 具体例

**レビュアー B さんのケース 1：プロジェクト移動**

```
学習期間内（2022-01～2022-06）：
  2022-01-10：nova でレビュー
  2022-02-15：nova でレビュー
  2022-03-20：nova でレビュー
  → 履歴プロジェクト = {nova}

将来活動（0-3m 窓：2022-06-30～2022-09-30）：
  2022-08-05：neutron でレビュー ← nova ではない
  → 判定：False（非継続）

理由：nova での継続を予測しているが、実際には neutron に移動した
```

**レビュアー C さんのケース 2：同一プロジェクト継続**

```
学習期間内（2022-01～2022-06）：
  2022-01-10：nova でレビュー
  2022-02-15：nova でレビュー
  2022-03-20：cinder でレビュー
  → 履歴プロジェクト = {nova, cinder}

将来活動（0-3m 窓：2022-06-30～2022-09-30）：
  2022-07-10：nova でレビュー ← nova は履歴に含まれる
  2022-08-15：neutron でレビュー ← neutron は履歴に含まれない
  → 判定：True（継続）

理由：少なくとも 1 つの履歴プロジェクト（nova）で活動が継続
```

**レビュアー D さんのケース 3：複数プロジェクトで活動**

```
学習期間内（2022-01～2022-06）：
  2022-01-10：nova でレビュー
  2022-02-15：cinder でレビュー
  → 履歴プロジェクト = {nova, cinder}

将来活動（0-3m 窓：2022-06-30～2022-09-30）：
  2022-07-10：cinder でレビュー ← cinder は履歴に含まれる
  → 判定：True（継続）

理由：cinder での活動が継続（nova での活動がなくても継続とみなす）
```

#### 4.5.4 この制約の意義

1. **継続の明確な定義**：「同じコミュニティでの継続」という明確な定義を提供
2. **予測の一貫性**：履歴期間で活動していたプロジェクトでの継続のみを予測対象とする
3. **コミュニティの保持**：OSS プロジェクトにとって、コミュニティメンバーの保持は重要な課題であり、プロジェクト間の移動は実質的な「離脱」とみなせる
4. **学習の安定性**：プロジェクト固有の文化や慣習を考慮した学習が可能になる

---

## 5. 特徴量抽出

本研究では、レビュアーの状態（State）と行動（Action）を数値ベクトルとして表現する。これらの特徴量は、レビュアーの活動パターンと継続性の関係を学習するための入力となる。

### 5.1 状態特徴量（State Features）

各レビュアーの**現在の状態**を表現する 10 次元の特徴量ベクトル。各時点での開発者の経験、活動パターン、コミュニティへの貢献度を数値化する。

#### 5.1.1 経験関連特徴量

**① `experience_days`（経験日数）**

初回活動日から現在までの経過日数。開発者の経験の深さを表す。

- **範囲**：0 日～数千日
- **例**：2020-01-01 に初参加、2022-06-15 時点 → 895 日

**② `total_changes`（総変更数）**

これまでに作成（Author）した変更の累積数。開発者の生産性と貢献量を示す。

- **範囲**：0 ～数千
- **例**：過去に 120 個の変更を作成

**③ `total_reviews`（総レビュー数）**

これまでにレビューした変更の累積数。レビュアーとしての経験と責任感を示す。

- **範囲**：0 ～数千
- **例**：過去に 350 個の変更をレビュー

**④ `project_count`（活動プロジェクト数）**

参加しているプロジェクトの数。開発者の活動範囲の広さを示す。

- **範囲**：1 ～数十
- **例**：nova, neutron, cinder の 3 プロジェクトに参加

#### 5.1.2 活動パターン特徴量

**⑤ `recent_activity_frequency`（最近の活動頻度）**

最近 30 日間の 1 日あたり平均活動数。現在の活動レベルを示す。

- **範囲**：0.0 ～数回/日
- **例**：30 日間で 15 回活動 → 0.5 回/日

**⑥ `avg_activity_gap`（平均活動間隔）**

連続する活動間の平均日数。活動の規則性を示す。

- **範囲**：1 日～数十日
- **例**：平均 7.5 日ごとに活動（週 1 回程度）

**⑦ `activity_trend`（活動トレンド）**

最近 30 日と過去 30-60 日の活動数を比較し、増加/安定/減少を判定。活動量の変化を捉える。

- **エンコード**：増加=1.0、安定=0.5、減少=0.0
- **例**：最近 20 回、過去 10 回 → 増加傾向

#### 5.1.3 貢献品質特徴量

**⑧ `collaboration_score`（協力度スコア）**

協力的な活動（レビュー、マージ等）の割合。コミュニティへの貢献度を示す。

- **範囲**：0.0 ～ 1.0
- **例**：100 回中 60 回がレビュー → 0.6

**⑨ `code_quality_score`（コード品質スコア）**

品質向上に関連する活動（test、documentation、refactor、fix 等のキーワード）の割合。開発者の品質意識を示す。

- **範囲**：0.3 ～ 1.0（ベースライン 0.3）
- **例**：100 回中 40 回が品質関連 → 0.7

**⑩ `time_since_last_activity`（最終活動からの経過日数）**

最後の活動からの経過日数。活動の新鮮さを示す。

- **範囲**：0 日～数十日
- **例**：最終活動が 5 日前 → 5 日

#### 5.1.4 特徴量ベクトルの例

**ベテラン継続開発者の例：** [730, 150, 300, 3, 0.6, 7.5, 1.0, 0.8, 0.75, 2]

- 2 年の経験、多くの変更とレビュー、3 プロジェクトに参加、高頻度で活動、週 1 回程度、増加傾向、高い協力度と品質意識、最近活動

**新人開発者の例：** [30, 5, 10, 1, 0.2, 15.0, 0.0, 0.3, 0.5, 14]

- 1 ヶ月の経験、少ない変更とレビュー、1 プロジェクトのみ、低頻度、2 週に 1 回程度、減少傾向、低い協力度、普通の品質意識、2 週間前

**合計：10 次元**

---

### 5.2 行動特徴量（Action Features）

各**個別の活動（レビュー、コミット等）**を表現する 5 次元の特徴量ベクトル。活動の種類、規模、品質を数値化する。

#### 5.2.1 行動タイプと規模

**① `action_type`（行動タイプ）**

活動の種類（review, commit, merge 等）をカテゴリ値として 0.0 ～ 1.0 に正規化。

- **例**：review → 0.8、commit → 0.5、merge → 0.7

**② `intensity`（行動の強度）**

変更の規模を正規化した値。計算式：(追加行数 + 削除行数) ÷ (変更ファイル数 × 50)

- **範囲**：0.1 ～ 1.0
- **例**：100 行追加 + 50 行削除、2 ファイル → 1.0（上限）

#### 5.2.2 行動品質

**③ `quality`（行動の質）**

コミットメッセージ内の品質キーワード（fix, improve, optimize, test, document, refactor 等）に基づくスコア。

- **範囲**：0.5 ～ 1.0（ベースライン 0.5）
- **例**："Fix bug in API and add test" → 0.7

**④ `collaboration`（協力度）**

行動タイプに応じた協力度スコア。他者との協力レベルを示す。

- **範囲**：0.3 ～ 1.0
- **例**：review → 0.8（高）、commit → 0.3（低）

#### 5.2.3 コンテキスト

**⑤ `project_id`（プロジェクト ID）**

活動が行われたプロジェクトの ID。プロジェクト名を整数に変換後、0.0 ～ 1.0 に正規化。

- **例**：nova → 0.1、neutron → 0.3、cinder → 0.5

#### 5.2.4 特徴量ベクトルの例

**大規模レビューの例：** [0.8, 0.9, 0.7, 0.8, 0.1]

- review、大規模、品質向上、高い協力度、nova

**小規模コミットの例：** [0.5, 0.3, 0.5, 0.3, 0.3]

- commit、小規模、通常の品質、個人作業、neutron

**合計：5 次元**

---

### 5.3 特徴量の正規化

すべての特徴量は、モデル学習の前に StandardScaler を用いて正規化される（平均 0、分散 1 に変換）。

---

### 5.4 拡張特徴量（Enhanced Features）

より詳細な分析を行うために、32 次元の状態特徴量と 9 次元の行動特徴量からなる拡張版も用意されている。拡張特徴量には以下が含まれる：

- **時系列統計量**：過去 7 日、14 日、30 日の活動数（より細かい時間粒度）
- **プロジェクト別統計**：各プロジェクトでの活動比率、プロジェクト間の移動頻度
- **ネットワーク特徴量**：他レビュアーとの協力回数、ネットワーク中心性スコア
- **テンポラル特徴量**：曜日別、時間帯別の活動パターン（週末 vs 平日、日中 vs 夜間）

拡張特徴量の利点と注意点：

- **利点**：より複雑なパターンや細かいニュアンスを捉えられる可能性
- **注意点**：計算コストの増加、過学習のリスク、解釈性の低下

---

## 6. 逆強化学習（IRL）の理論と適用

### 6.1 強化学習の基礎

#### 6.1.1 強化学習とは

**強化学習（Reinforcement Learning, RL）**は、エージェントが環境との相互作用を通じて、報酬を最大化する行動方策を学習する機械学習の一分野である。

**基本的な構成要素**：

1. **エージェント（Agent）**：学習・意思決定を行う主体
2. **環境（Environment）**：エージェントが相互作用する対象
3. **状態（State）$s$**：環境の現在の状況を表す
4. **行動（Action）$a$**：エージェントが取ることができる選択肢
5. **報酬（Reward）$r$**：行動の良し悪しを評価するスカラー値
6. **方策（Policy）$\pi$**：状態から行動への写像 $\pi: S \to A$

#### 6.1.2 強化学習の目的

強化学習の目的は、**累積報酬を最大化する最適方策 $\pi^*$ を見つけること**：

$$
\pi^* = \arg\max_\pi \mathbb{E}\left[\sum_{t=0}^{T} \gamma^t r_t \mid \pi\right]
$$

ここで：

- $r_t$：時刻 $t$ での報酬
- $\gamma$：割引率（$0 \leq \gamma \leq 1$）
- $T$：エピソードの長さ

#### 6.1.3 報酬関数の重要性

強化学習において、**報酬関数 $R(s, a)$ は最も重要な要素**である。報酬関数が、エージェントが「何を達成すべきか」を定義する。

**問題点**：

- 多くの実世界の問題では、適切な報酬関数を設計することが極めて困難
- 報酬関数が不適切だと、意図しない行動を学習してしまう（**Reward Hacking**）

**例**：

- ゲームで「スコアを最大化」→ バグを利用してスコアを稼ぐ
- ロボット制御で「速く移動」→ 転倒を無視して無茶な動きをする

### 6.2 逆強化学習（IRL）とは

#### 6.2.1 IRL の動機

**問題**：報酬関数を手動で設計するのは困難

**解決策**：専門家の行動を観察し、その行動を説明する報酬関数を**逆算**する

これが**逆強化学習（Inverse Reinforcement Learning, IRL）**の核心的なアイデアである。

#### 6.2.2 IRL の定義

**通常の強化学習**：

```
既知: 報酬関数 R(s, a)
目標: 最適方策 π* を学習
```

**逆強化学習**：

```
既知: 専門家の軌跡 {τ₁, τ₂, ..., τₙ}
目標: 専門家の行動を説明する報酬関数 R(s, a) を学習
```

ここで、軌跡 $\tau$ は状態と行動の系列：

$$
\tau = [(s_0, a_0), (s_1, a_1), ..., (s_T, a_T)]
$$

#### 6.2.3 IRL の基本原理

IRL は以下の仮定に基づく：

**仮定**：専門家は、何らかの報酬関数を（近似的に）最大化するように行動している。

$$
\pi_{\text{expert}} \approx \arg\max_\pi \mathbb{E}_{\tau \sim \pi}[R(\tau)]
$$

**目標**：この報酬関数 $R$ を専門家の軌跡から推定する。

**直感的な理解**：

1. 専門家が頻繁に選ぶ行動 → 高い報酬を持つはず
2. 専門家が避ける行動 → 低い報酬を持つはず
3. これらの観察から報酬関数を逆算

#### 6.2.4 IRL の全体的な流れ

```
┌──────────────────┐
│ 専門家の軌跡     │
│ (観察データ)     │
└────────┬─────────┘
         │
         ▼
┌──────────────────┐
│ 逆強化学習（IRL）│
│ 報酬関数を推定   │
└────────┬─────────┘
         │
         ▼
┌──────────────────┐
│ 学習された       │
│ 報酬関数 R(s,a)  │
└────────┬─────────┘
         │
         ▼
┌──────────────────┐
│ 強化学習（RL）   │
│ 方策を学習       │
└────────┬─────────┘
         │
         ▼
┌──────────────────┐
│ 専門家のような   │
│ 行動をする方策   │
└──────────────────┘
```

### 6.3 従来の IRL 手法

#### 6.3.1 最大マージン IRL

**アイデア**：専門家の軌跡の累積報酬が、他のあらゆる方策の累積報酬より**マージン以上**高くなるように報酬関数を設定。

$$
\mathbb{E}_{\tau \sim \pi^*}[R(\tau)] \geq \mathbb{E}_{\tau \sim \pi}[R(\tau)] + m \quad \forall \pi \neq \pi^*
$$

ここで、$m > 0$ はマージン。

**特徴**：

- 明確な最適化目標
- 計算コストが高い（各反復で RL を実行）

#### 6.3.2 最大エントロピー IRL

**アイデア**：報酬関数を固定したとき、専門家の軌跡の確率を最大化。

$$
P(\tau | R) \propto \exp\left(\sum_{t=0}^{T} R(s_t, a_t)\right)
$$

**目標**：最大尤度推定

$$
R^* = \arg\max_R \sum_{i=1}^{N} \log P(\tau_i | R)
$$

**特徴**：

- 確率的な枠組み
- 複数の最適方策に対応可能

#### 6.3.3 従来手法の課題

1. **計算コストが高い**：各反復で RL や方策評価が必要
2. **不安定**：最適化が難しい
3. **実装が複雑**：多くのハイパーパラメータ

### 6.4 なぜ本研究で逆強化学習を使うのか

#### 6.4.1 従来のアプローチの限界

**従来の機械学習アプローチ（分類・回帰）**：

レビュアーの継続予測に対して、通常の教師あり学習を適用すると：

```
特徴量（スナップショット）→ モデル → 継続/離脱
```

**問題点**：

1. **時系列の扱いが不十分**

   - 単一時点の特徴量スナップショット → 過去の文脈や変化のパターンを捉えにくい
   - 「最近活動が減った」「急激に活動が増えた」といった**動的な変化**を表現困難

2. **行動の価値が不明確**

   - 「なぜこの行動が継続につながるのか」が学習されない
   - 特徴量の重要度は分かるが、**行動の意図や目的**が不明

3. **累積的な効果を無視**
   - 各時点の行動を独立に評価
   - 「過去の活動の積み重ねが将来に与える影響」を考慮できない

**具体例**：

- 従来手法：「レビュー数 = 10」→ 重要度 0.8
- 本質：「毎月コンスタントに 10 レビュー」と「最初に 100 レビュー後に休止」では意味が全く異なる

#### 6.4.2 なぜ逆強化学習が適しているのか

**IRL の利点 1：時系列パターンの自然な統合**

IRL では、時系列の活動を**軌跡**として扱う：

$$
\tau = [(s_0, a_0), (s_1, a_1), ..., (s_T, a_T)]
$$

各時点の状態と行動を統合し、**累積報酬**として評価：

$$
R_{\text{total}} = \sum_{t=0}^{T} R(s_t, a_t)
$$

これにより：

- **過去の文脈を考慮**：LSTM により時系列依存関係を学習
- **動的な変化を捉える**：活動パターンの変化を報酬の変化として表現
- **長期的な影響を評価**：累積報酬により、過去の行動が将来に与える影響を定量化

**IRL の利点 2：行動の意図・価値の学習**

IRL は「なぜ専門家がその行動を選んだのか」を学習：

- **高い報酬を与える行動**：継続につながる行動パターン
  - 例：定期的なレビュー、高品質なフィードバック、協力的な姿勢
- **低い報酬を与える行動**：離脱につながる行動パターン
  - 例：活動頻度の急激な減少、長期間の不活動

これにより、**「何が継続を促進するのか」を理解**できる。

**IRL の利点 3：解釈可能性**

報酬関数という概念を通じて：

- **継続に寄与する要因**を明確に理解できる
- **離脱のリスク要因**を特定できる
- **介入策の設計**に活用できる（どの行動を促進すべきか）

**IRL の利点 4：転移学習への応用**

学習した報酬関数は、他のタスクにも応用可能：

- 他の OSS プロジェクトへの適用
- レビュアー推薦システムへの組み込み
- 新規レビュアーの育成計画の設計

#### 6.4.3 本研究の問題に対する適合性

**本研究の特徴**：

1. **豊富な時系列データ**：レビュアーの長期的な活動履歴
2. **明確な専門家の存在**：継続したレビュアー（正例）
3. **行動の多様性**：レビューの種類、頻度、質など様々な要素

**IRL が最適である理由**：

| 本研究の特徴                 | IRL の対応                           |
| ---------------------------- | ------------------------------------ |
| 長期的な活動履歴             | LSTM による時系列モデリング          |
| 継続・離脱の二値分類         | 累積報酬 → Sigmoid → 継続確率        |
| 多様な行動パターン           | 状態・行動エンコーダーによる表現学習 |
| 継続要因の理解が重要         | 報酬関数による解釈可能性             |
| 他プロジェクトへの適用が目標 | 学習した報酬関数の転移学習           |

**従来手法との決定的な違い**：

```
【従来手法】
特徴量スナップショット → 分類器 → 継続/離脱
↓
「この特徴量の組み合わせで継続する」は分かるが、
「なぜ継続するのか」「どの行動が価値があるのか」は不明

【IRL アプローチ】
活動履歴（軌跡） → 報酬関数 → 累積報酬 → 継続確率
↓
「継続につながる行動パターン」を報酬として学習
時系列の文脈を考慮し、累積的な効果を評価
```

#### 6.4.4 具体的なシナリオ例

**シナリオ 1：活動パターンの変化を捉える**

- **レビュアー A**：毎月 10 件のレビューを 1 年間継続

  - 従来手法：「平均レビュー数 = 10」→ 継続可能性 60%
  - IRL：安定した活動パターン → 高い累積報酬 → 継続確率 85%

- **レビュアー B**：最初の 3 ヶ月で 100 件、その後 9 ヶ月休止
  - 従来手法：「平均レビュー数 = 8.3」→ 継続可能性 55%
  - IRL：活動の急激な減少 → 低い累積報酬 → 継続確率 15%

**シナリオ 2：行動の質を評価**

- **レビュアー C**：高品質なレビュー、協力的な姿勢

  - 従来手法：レビュー数だけで評価 → 継続可能性 50%
  - IRL：高品質行動に高報酬 → 高い累積報酬 → 継続確率 80%

- **レビュアー D**：形式的なレビューのみ
  - 従来手法：レビュー数だけで評価 → 継続可能性 50%
  - IRL：低品質行動に低報酬 → 低い累積報酬 → 継続確率 20%

**結論**：IRL は、時系列パターン、行動の質、累積的な効果を総合的に評価し、「なぜ継続するのか」を学習できる。

### 6.5 本研究での IRL 適用：教師あり学習的アプローチ

#### 6.5.1 本研究の問題設定への適合

**本研究の状況**：

- **専門家**：継続したレビュアー（$y = 1$）
- **非専門家**：離脱したレビュアー（$y = 0$）
- **軌跡**：レビュアーの過去の活動履歴（状態と行動の時系列）
- **目標**：継続を促進する要因（報酬関数）を学習

#### 6.5.2 教師あり学習的 IRL

従来の IRL の複雑さを避けるため、**教師あり学習的なアプローチ**を採用：

**基本アイデア**：

1. 継続者の軌跡に**高い累積報酬**を与える
2. 離脱者の軌跡に**低い累積報酬**を与える
3. この条件を満たす報酬関数をニューラルネットワークで学習

**数式での表現**：

$$
\begin{cases}
\sum_{t=1}^{T} R(s_t, a_t; \theta) \to \text{高い} & \text{if } y = 1 \text{ (継続者)} \\
\sum_{t=1}^{T} R(s_t, a_t; \theta) \to \text{低い} & \text{if } y = 0 \text{ (離脱者)}
\end{cases}
$$

**利点**：

- **計算効率**：複雑な最適化や RL の反復が不要
- **実装の簡潔性**：標準的な教師あり学習のフレームワークを利用可能
- **安定性**：勾配降下法で安定的に学習可能

#### 6.5.3 累積報酬から確率への変換

累積報酬を継続確率に変換するために、**Sigmoid 関数**を使用：

$$
P(\text{継続} | \tau) = \sigma\left(\sum_{t=1}^{T} R(s_t, a_t; \theta)\right)
$$

ここで、Sigmoid 関数：

$$
\sigma(x) = \frac{1}{1 + e^{-x}}
$$

**Sigmoid の性質**：

| 累積報酬 $\sum R$ | Sigmoid 出力 $\sigma(\sum R)$ | 解釈           |
| ----------------- | ----------------------------- | -------------- |
| $+10$             | $\approx 0.9999$              | ほぼ確実に継続 |
| $+2$              | $\approx 0.88$                | 継続しやすい   |
| $0$               | $0.5$                         | 五分五分       |
| $-2$              | $\approx 0.12$                | 離脱しやすい   |
| $-10$             | $\approx 0.00005$             | ほぼ確実に離脱 |

### 6.6 報酬関数の学習メカニズム

#### 6.6.1 損失関数との関係

**鍵となる繋がり**：

$$
\text{累積報酬} \xrightarrow{\text{Sigmoid}} \text{継続確率} \xrightarrow{\text{Focal Loss}} \text{損失}
$$

Focal Loss（詳細は次章）を最小化することで、間接的に報酬関数を最適化：

$$
\mathcal{L}(\theta) = \sum_{i=1}^{N} \text{FL}(P(\text{継続} | \tau_i; \theta), y_i)
$$

#### 6.6.2 報酬の自動調整

**継続者の軌跡（$y = 1$）の場合**：

1. 損失を減らすには → $P(\text{継続}) \to 1$ が必要
2. つまり → $\sigma(\sum R) \to 1$ が必要
3. したがって → **$\sum R$ を増やす方向に $\theta$ を更新**

**離脱者の軌跡（$y = 0$）の場合**：

1. 損失を減らすには → $P(\text{継続}) \to 0$ が必要
2. つまり → $\sigma(\sum R) \to 0$ が必要
3. したがって → **$\sum R$ を減らす方向に $\theta$ を更新**

**結論**：損失関数を最小化することで、自動的に「継続者に高報酬、離脱者に低報酬」を与える報酬関数が学習される。

#### 6.6.3 勾配降下法による最適化

パラメータ $\theta$ の更新式：

$$
\theta \leftarrow \theta - \alpha \cdot \frac{\partial \mathcal{L}}{\partial \theta}
$$

勾配の連鎖律：

$$
\frac{\partial \mathcal{L}}{\partial \theta} = \frac{\partial \mathcal{L}}{\partial P} \cdot \frac{\partial P}{\partial \sum R} \cdot \frac{\partial \sum R}{\partial \theta}
$$

ここで：

- $\frac{\partial \mathcal{L}}{\partial P}$：Focal Loss の勾配
- $\frac{\partial P}{\partial \sum R}$：Sigmoid の勾配
- $\frac{\partial \sum R}{\partial \theta}$：ニューラルネットワークの勾配

### 6.7 報酬関数のネットワーク表現

#### 6.7.1 パラメトリック報酬関数

報酬関数をディープニューラルネットワークで表現：

$$
R(s_t, a_t; \theta) = f_\theta(\text{Enc}_s(s_t), \text{Enc}_a(a_t))
$$

**ネットワークの構造**（詳細は第 7 章）：

1. **状態エンコーダー**：$s_t \in \mathbb{R}^{10} \to \mathbb{R}^{64}$
2. **行動エンコーダー**：$a_t \in \mathbb{R}^{5} \to \mathbb{R}^{64}$
3. **統合層**：$[\text{Enc}_s; \text{Enc}_a] \in \mathbb{R}^{128}$
4. **LSTM**：時系列パターンを学習
5. **継続確率予測器**：最終的な確率を出力

#### 6.7.2 暗黙的な報酬学習

本手法の特徴：報酬を**明示的なスカラー値として出力しない**。

代わりに、ネットワーク全体が：

- **継続パターン**（高報酬）：定期的な活動、高い協力度、安定した品質
- **離脱パターン**（低報酬）：活動頻度の減少、長期間の不活動

を暗黙的に学習する。

### 6.8 本手法の位置づけ

#### 6.8.1 従来の IRL との比較

| 項目           | 従来の IRL                 | 本研究のアプローチ |
| -------------- | -------------------------- | ------------------ |
| **計算コスト** | 高い（RL の反復実行）      | 低い（順伝播のみ） |
| **安定性**     | 不安定                     | 安定（勾配降下法） |
| **実装**       | 複雑                       | シンプル           |
| **報酬出力**   | 明示的                     | 暗黙的             |
| **最適化**     | マージン最大化、尤度最大化 | 損失最小化         |

#### 6.8.2 教師あり学習との違い

本手法は教師あり学習に似ているが、重要な違いがある：

**教師あり学習**：

- 状態・行動 → 継続/離脱を**直接**予測
- 特徴量の重要度を学習

**本手法（IRL 的アプローチ）**：

- 状態・行動 → **報酬** → 累積報酬 → 継続確率
- 時系列全体の**累積的な価値**を学習
- 各時点の行動が将来に与える影響を考慮

#### 6.8.3 IRL を採用する利点

1. **解釈性**：報酬という概念を通じて、継続に寄与する要因を理解しやすい
2. **時系列の統合**：累積報酬により、活動履歴全体を自然に統合
3. **理論的基盤**：強化学習の理論を活用できる
4. **転移学習**：学習した報酬関数を他のタスクに応用可能

---

## 7. 学習ステップ：損失関数・報酬関数・予測

### 7.1 概要

本研究では、**逆強化学習（IRL）** の枠組みで、継続したレビュアーの行動から報酬関数を学習し、継続予測を行う。

```
継続者の活動履歴 ─→ 報酬関数の学習 ─→ 継続確率の予測
```

---

### 7.2 損失関数：Focal Loss

#### 7.2.1 データの不均衡問題

本研究のデータは**クラス不均衡**が顕著：

| クラス         | 説明                   | 割合   |
| -------------- | ---------------------- | ------ |
| 正例（継続者） | 指定期間後も活動がある | 約 20% |
| 負例（離脱者） | 指定期間後に活動がない | 約 80% |

通常の損失関数では、多数派（離脱者）に偏り、少数派（継続者）のパターンを学習できない。

#### 7.2.2 Focal Loss の定義

**Focal Loss** はクラス不均衡に対処する損失関数：

$$
\text{FL}(p_t) = -\alpha_t (1 - p_t)^\gamma \log(p_t)
$$

**パラメータ**：

- $p_t$：正解クラスに対する予測確率
- $\alpha$：クラス重みパラメータ（本研究：0.3）
- $\gamma$：フォーカスパラメータ（本研究：3.0）

#### 7.2.3 Focal Loss の効果

**1. 易しい例の重みを減らす**

$(1 - p_t)^\gamma$ により、正しく分類できている例の損失を抑制：

| 予測確信度 $p_t$ | 損失の減衰 |
| ---------------- | ---------- |
| 0.9（高い）      | 99.9% 減少 |
| 0.5（中程度）    | 87.5% 減少 |
| 0.3（低い）      | 65.7% 減少 |

→ 誤分類している困難な例に学習を集中できる。

**2. クラス重み調整**

継続率に応じて負例（離脱者）の重みを動的に調整：

$$
w_{\text{neg}} = \frac{p_{\text{pos}}}{1 - p_{\text{pos}}}
$$

継続率が 20%の場合、$w_{\text{neg}} \approx 0.26$ となり、正例（継続者）の学習を保護。

---

### 7.3 報酬関数の学習

#### 7.3.1 逆強化学習（IRL）の原理

通常の強化学習と IRL の違い：

| アプローチ    | 既知                    | 目標                              |
| ------------- | ----------------------- | --------------------------------- |
| **通常の RL** | 報酬関数 $R(s,a)$       | 報酬を最大化する方策 $\pi^*$      |
| **IRL**       | 専門家の軌跡 $\{\tau\}$ | 専門家を説明する報酬関数 $R(s,a)$ |

本研究では：

- **専門家** = 継続したレビュアー（ラベル $y=1$）
- **目標** = 専門家の軌跡に高報酬、非専門家に低報酬を与える関数を学習

#### 7.3.2 報酬関数のネットワーク表現

報酬関数はディープニューラルネットワークで表現：

```
状態 s_t (10次元) ─→ エンコーダー ─→ 埋め込み (64次元)
                                        │
行動 a_t (5次元)  ─→ エンコーダー ─→ 埋め込み (64次元)
                                        │
                                        ▼
                                   連結 (128次元)
                                        │
                                        ▼
                                   LSTM (2層)
                                        │
                                        ▼
                                 継続確率予測器
                                        │
                                        ▼
                                  継続確率 P
```

#### 7.3.3 累積報酬と継続確率の関係

各時刻の報酬を累積し、Sigmoid 関数で確率に変換：

$$
P(\text{継続}) = \sigma\left(\sum_{t=1}^{T} R(s_t, a_t; \theta)\right)
$$

**Sigmoid の性質**：

- 累積報酬が高い → $P \approx 1$ （継続しやすい）
- 累積報酬が低い → $P \approx 0$ （離脱しやすい）

#### 7.3.4 損失関数と報酬関数の関係

Focal Loss を最小化することで、報酬関数が自動的に学習される：

**継続者の軌跡（$y=1$）**：

- 損失を減らす → $P(\text{継続}) \to 1$ が必要
- つまり → 累積報酬 $\sum R(s_t, a_t)$ を**高く**する

**離脱者の軌跡（$y=0$）**：

- 損失を減らす → $P(\text{継続}) \to 0$ が必要
- つまり → 累積報酬 $\sum R(s_t, a_t)$ を**低く**する

**結論**：損失最小化により、「継続者に高報酬、離脱者に低報酬」を与える報酬関数が学習される。

---

### 7.4 学習プロセス

#### 7.4.1 学習の全体フロー

```
1. データ準備
   ├─ 継続者の軌跡（y=1）
   └─ 離脱者の軌跡（y=0）
         │
         ▼
2. 順伝播（Forward Pass）
   軌跡 → エンコード → LSTM → 累積報酬 → Sigmoid → 継続確率 P
         │
         ▼
3. 損失計算
   L = Focal_Loss(P, y)
         │
         ▼
4. 逆伝播（Backpropagation）
   ∂L/∂θ を計算（勾配）
         │
         ▼
5. パラメータ更新
   θ ← θ - α・∂L/∂θ （Adam Optimizer）
         │
         ▼
6. 反復（100エポック）
   徐々に報酬関数が洗練される
```

#### 7.4.2 最適化アルゴリズム

**Adam Optimizer**：

- 学習率：0.0001
- 動的な学習率調整（ReduceLROnPlateau）
- Dropout（0.3）による正則化

**学習設定**：

- エポック数：100
- バッチ処理：各レビュアーの全活動履歴を 1 バッチとして処理
- 可変長シーケンス：LSTM で各レビュアーの異なる活動数に対応

---

### 7.5 予測タスク

#### 7.5.1 入力と出力

**入力**：

- レビュアーの過去の活動履歴（時系列）
  - 状態特徴量：経験日数、総活動数、活動トレンド等（10 次元）
  - 行動特徴量：活動種類、強度、品質等（5 次元）

**出力**：

- 指定期間後の継続確率：$P(\text{継続} | \text{履歴}, \text{期間})$
  - 例：0-3 ヶ月後に活動する確率

#### 7.5.2 予測の流れ

```
レビュアーの活動履歴
  │
  ▼
時刻1: (状態1, 行動1) ─┐
時刻2: (状態2, 行動2) ─┤
  ...                 ├─→ LSTM処理 → 隠れ状態
時刻T: (状態T, 行動T) ─┘
  │
  ▼
累積報酬の算出
  │
  ▼
Sigmoid変換
  │
  ▼
継続確率 P ∈ [0, 1]
```

---

### 7.6 評価指標

本研究では、モデルの予測性能を多面的に評価するために、複数の評価指標を採用する。

まず、**AUC-ROC（ROC 曲線下面積）**を用いて、閾値に依存しない全体的な分類性能を評価する。この指標により、継続者と離脱者を区別するモデルの基本的な能力を測定できる。

次に、**AUC-PR（Precision-Recall 曲線下面積）**を採用する。本研究のデータは、継続者が約 20%、離脱者が約 80%という顕著なクラス不均衡を示している。このような不均衡データにおいては、AUC-ROC よりも AUC-PR の方が少数クラス（継続者）に対する予測性能をより適切に評価できることが知られている。

さらに、**F1 スコア**を用いて、Precision と Recall のバランスを評価する。F1 スコアは、実用的な閾値設定のための重要な指標として機能する。また、**Precision（適合率）**と**Recall（再現率）**を個別に報告することで、実務応用における「継続予測の信頼性」と「実際の継続者の捕捉率」の両面から性能を理解可能にする。

最適閾値の決定には、訓練データで F1 スコアを最大化する閾値を選択し、評価データで同じ閾値を使用する手法を採用する。これにより、Precision と Recall のバランスが取れた予測を実現する。

---

### 7.7 学習の特徴

#### 7.7.1 月次集約ラベル

各月末を基準点として継続ラベルを付与：

- 同一月内の全活動に同じラベルを適用
- 学習の安定性向上

#### 7.7.2 スライディングウィンドウ

互いに排他的な時間窓で学習：

- 0-3m, 3-6m, 6-9m, 9-12m
- 各時間スケールに特有のパターンを独立して学習

#### 7.7.3 可変長シーケンス処理

各レビュアーの活動数が異なる問題に対応：

- `pack_padded_sequence`による効率的な処理
- 実際の活動履歴の長さに応じた学習

#### 7.7.4 入力データの不足への対処

本研究では、活動履歴が少ない、または欠損しているレビュアーに対して、以下の戦略を採用する。

**（1）最小活動回数フィルター**

予測対象とするレビュアーには、最低限の活動履歴を要求する。具体的には、履歴期間内に**最低 3 回以上の活動**があるレビュアーのみを対象とする。これにより、統計的に信頼性の低いデータを除外し、モデルの予測精度を向上させる。

**（2）シーケンス長の調整**

LSTM への入力は固定長のシーケンスとして処理される。活動数がシーケンス長（デフォルト 15）に満たない場合、以下の処理を行う：

- **パディング（活動数 < シーケンス長）**：最初の活動を繰り返してシーケンス長に合わせる
  - 例：8 個の活動 → 最初の活動を 7 回繰り返し + 元の 8 個 = 計 15 個
- **トランケート（活動数 > シーケンス長）**：最新のシーケンス長分の活動のみを使用
  - 例：20 個の活動 → 最新 15 個のみを使用

この手法により、全てのレビュアーに対して一貫した形式の入力を提供しつつ、実際の活動履歴の長さを`pack_padded_sequence`によって考慮することで、パディング部分が学習に与える影響を最小化している。

**（3）欠損値の処理**

特徴量の計算において値が得られない場合、以下のデフォルト値を使用する：

- 経験日数：初回活動日からの経過日数（最低 0）
- 活動数：0（活動がない場合）
- 活動頻度：0.0（活動がない場合）
- 協力度スコア：0.3（ベースライン値）
- 品質スコア：0.5（中立値）

これらの対処により、入力データが部分的に欠損している場合でも、モデルは安定して予測を行うことができる。

#### 7.7.5 予測時点での時系列的整合性とデータリーク防止

本研究では、予測時点（cutoff 日）における時系列的な整合性を厳密に保つことで、データリーク（将来の情報の使用）を防止している。

**時間的分離の定義**

予測時点を$t_{\text{cutoff}}$とし、履歴期間を$h$ヶ月、将来窓を$[f_{\text{start}}, f_{\text{end}}]$ヶ月とする。このとき：

- **履歴期間**：$[t_{\text{cutoff}} - h, t_{\text{cutoff}})$ （cutoff 日を含まない）
- **将来窓**：$[t_{\text{cutoff}} + f_{\text{start}}, t_{\text{cutoff}} + f_{\text{end}})$

例えば、$t_{\text{cutoff}} = $ 2023-01-01、$h = 12$、$f*{\text{start}} = 0$、$f*{\text{end}} = 3$の場合：

- 履歴期間：2022-01-01 ～ 2023-01-01（特徴量計算に使用）
- 将来窓：2023-01-01 ～ 2023-04-01（ラベル付けに使用）

**離脱傾向にあるレビュアーの扱い**

履歴期間内に活動があれば、その後活動していないレビュアーも予測対象に含まれる。例えば、2022 年 11 月に最後の活動があり、その後活動していないレビュアーの場合：

- 履歴期間内の全活動（～ 2022-11-10）から特徴量を計算
- `time_since_last_activity` = 52 日（2022-11-10 から 2023-01-01 まで）
- `recent_activity_frequency` = 0.0（最近 30 日間の活動なし）
- `activity_trend` = 0.0（減少傾向）

これらの特徴量により、「最近活動していない」という離脱の兆候を数値的に表現し、モデルが学習できるようにしている。重要なのは、cutoff 日**以降**の情報は一切使用していないという点である。

**データリーク防止の検証**

実装において、以下の制約により将来の情報の漏洩を防いでいる：

1. 履歴期間のデータ抽出：`df[date_col] < cutoff_date`（cutoff 日を含まない）
2. 将来窓のデータ抽出：`df[date_col] >= cutoff_date + offset`（明確な時間的分離）
3. 特徴量計算：全て`cutoff_date`時点での情報のみを使用

この設計により、実運用での予測精度を正確に評価できる。

---

### 7.8 まとめ

本研究の学習プロセスは以下の 3 ステップで構成される：

1. **Focal Loss による不均衡データ対処**

   - 少数派（継続者）のパターンを効果的に学習

2. **IRL による報酬関数学習**

   - 継続者の軌跡から「何が継続を促すか」を自動発見

3. **LSTM による時系列パターン学習**
   - 過去の活動履歴全体から継続確率を予測

これにより、明示的なルールを設計せずに、データから継続予測モデルを学習できる。

---

## 8. 実験設定

### 8.1 データ分割

本研究では、時系列データを以下のように分割する：

- **学習期間（Training Period）**：2021-01-01 ～ 2023-01-01（2 年間）
- **評価期間（Evaluation Period）**：2023-01-01 ～ 2024-01-01（1 年間）

**Cutoff 日**：2023-01-01 を基準とし、この日付以前の履歴から将来の継続を予測する。

### 8.2 予測対象の定義

**予測対象レビュアー**：Cutoff 日（2023-01-01）から過去 12 ヶ月間（2022-01-01 ～ 2023-01-01）に活動があったレビュアー全員

**継続の判定**：Cutoff 日から指定された期間内（例：0-3 ヶ月）に、履歴期間内で活動していたプロジェクトで活動があるか

---

## 9. 訓練プロセス

### 9.1 最適化アルゴリズム

#### 9.1.1 Adam Optimizer

**パラメータ**：

- 学習率：0.0001
- $\beta_1 = 0.9$、$\beta_2 = 0.999$（デフォルト）
- $\epsilon = 10^{-8}$（デフォルト）

Adam は適応的学習率手法であり、各パラメータに対して個別の学習率を調整する。これにより、効率的な収束が期待できる。

#### 9.1.2 学習率スケジューラ

**ReduceLROnPlateau** を採用：

**パラメータ**：

- `mode='min'`：損失の減少を監視
- `patience=5`：5 エポック改善がなければ学習率を削減
- `factor=0.5`：学習率を半減
- `min_lr=1e-6`：学習率の下限

**効果**：訓練が停滞した際に学習率を動的に調整し、より細かい最適化を実現する。

### 9.2 訓練アルゴリズム

各エポックにおいて、以下の手順を実行：

1. **状態特徴量と行動特徴量を抽出**：各軌跡から 10 次元の状態と 5 次元の行動を取得
2. **動的に状態シーケンスを構築**：各ステップまでの履歴を集約
3. **LSTM で時系列パターンを学習**：過去の活動シーケンスから隠れ状態を生成
4. **各ステップで継続確率を予測**：全てのタイムステップで予測を実施
5. **Focal Loss を計算**：予測と実際のラベルの差を評価
6. **重み付き損失でバックプロパゲーション**：負例の重みを考慮して勾配を計算
7. **パラメータ更新**：Adam で重みを更新
8. **学習率スケジューラ更新**：エポック終了後、損失に基づいて学習率を調整

---

## 10. 評価方法

### 10.1 評価指標の選定

本研究では、AUC-ROC、AUC-PR、F1 スコア、Precision、Recall、継続率を評価指標として使用する。

AUC-ROC は、全体的な分類性能を閾値非依存で評価するための指標である。一方、本研究のデータは継続者 20%、離脱者 80%という顕著なクラス不均衡を示すため、AUC-PR をより重視する。AUC-PR は、不均衡データにおいて少数クラスの予測性能をより適切に評価できることが知られている。

F1 スコアは、Precision と Recall のバランスを考慮した総合指標として採用する。さらに、Precision と Recall を個別に報告することで、実務での意思決定に必要な個別性能を明確にする。また、継続率をデータの基本統計として記録し、実験設定の透明性を確保する。

### 10.2 最適閾値の決定

予測確率を二値分類に変換するための最適閾値は、訓練データで F1 スコアを最大化する値として決定する。この閾値を評価データでも同様に使用することで、Precision と Recall のバランスが取れた一貫性のある予測を実現する。

### 10.3 クロス評価（Cross-Evaluation）

異なる訓練ラベルと評価期間の組み合わせで性能を測定：

| 訓練ラベル ↓ / 評価期間 → | 0-3m | 3-6m | 6-9m | 9-12m |
| ------------------------- | ---- | ---- | ---- | ----- |
| **0-3m**                  | ✓    | ✓    | ✓    | ✓     |
| **3-6m**                  | ✓    | ✓    | ✓    | ✓     |
| **6-9m**                  | ✓    | ✓    | ✓    | ✓     |
| **9-12m**                 | ✓    | ✓    | ✓    | ✓     |

**合計：4 訓練 × 4 評価 = 16 評価**

**期待される結果：**

- **対角線（train=eval）で高性能**：訓練時の時間スケールと評価時の時間スケールが一致する場合に最も性能が高いはず

---

## 11. 実験設定

### 11.1 データセット

| 設定             | 複数プロジェクト  | nova 単一         |
| ---------------- | ----------------- | ----------------- |
| **プロジェクト** | OpenStack 全体    | openstack/nova    |
| **レビュアー数** | 623               | 241               |
| **活動数**       | 60,813            | 10,908            |
| **期間**         | 2018-2023（5 年） | 2018-2023（5 年） |

### 11.2 訓練設定

| パラメータ         | 値                       |
| ------------------ | ------------------------ |
| **学習期間**       | 2021-01-01 ～ 2023-01-01 |
| **評価期間**       | 2023-01-01 ～ 2024-01-01 |
| **履歴ウィンドウ** | 12 ヶ月                  |
| **エポック数**     | 20                       |
| **バッチサイズ**   | 32                       |
| **学習率**         | 0.0001                   |
| **hidden_dim**     | 128                      |
| **LSTM 層数**      | 2                        |
| **Dropout 率**     | 0.3, 0.2                 |

**データ品質フィルター**：

- 履歴期間内に最低 3 回以上の活動があるレビュアーのみを対象とする
- 活動数が 3 回未満のレビュアーは、統計的信頼性が低いため除外される
- これにより、予測モデルの精度と安定性を向上させる

### 11.3 ハイパーパラメータ

| パラメータ    | 値  | 説明                          |
| ------------- | --- | ----------------------------- |
| `state_dim`   | 10  | 状態特徴量の次元              |
| `action_dim`  | 5   | 行動特徴量の次元              |
| `hidden_dim`  | 128 | 隠れ層の次元                  |
| `num_layers`  | 2   | LSTM 層数                     |
| `dropout`     | 0.3 | Dropout 率                    |
| `alpha`       | 0.3 | Focal Loss パラメータ         |
| `gamma`       | 3.0 | Focal Loss パラメータ         |
| `lr_patience` | 5   | 学習率スケジューラの patience |
| `lr_factor`   | 0.5 | 学習率削減率                  |

---

## 12. 実験結果の分析方法

### 12.1 ヒートマップ可視化

訓練ラベル × 評価期間の AUC-ROC 行列をヒートマップで可視化。seaborn を使用し、x 軸に評価期間、y 軸に訓練ラベル、色で AUC-ROC 値（0 ～ 1）を表現する。

### 12.2 統計的分析

各訓練ラベルごとに平均 AUC-ROC、F1 スコア、Precision、Recall を算出し、時間スケールによる性能の違いを分析する。また、訓練ラベルと評価期間の組み合わせによる性能変化を観察し、時間的な汎化性能を評価する。

---

## 13. まとめ

### 13.1 手法の特徴

本研究では、以下の 6 つの主要な特徴を持つ手法を提案する：

1. **スライディングウィンドウラベル**：互いに排他的な時間窓を定義し、各時間スケールに特有の継続パターンを独立して学習
2. **月次集約ラベル**：各月末を基準点として一貫したラベルを付与し、学習の安定性を向上
3. **同一プロジェクト継続判定**：履歴期間内で活動したプロジェクトでの継続のみを評価し、継続の定義を明確化
4. **LSTM ベースの時系列モデリング**：可変長シーケンスに対応し、過去の活動パターンから時系列的な継続パターンを学習
5. **Focal Loss**：クラス不均衡に対処し、困難な事例（少数クラス）に焦点を当てた学習を実現
6. **逆強化学習フレームワーク**：継続したレビュアーの行動から報酬関数を学習し、継続を促進する要因を発見

### 13.2 今後の展開

- より複雑な特徴量の導入
- アテンションメカニズムの追加
- 説明可能性の向上（どの特徴が重要か）
- リアルタイム予測システムの構築

---

## 参考文献

1. Abbeel, P., & Ng, A. Y. (2004). Apprenticeship learning via inverse reinforcement learning. _ICML_.
2. Lin, T. Y., Goyal, P., Girshick, R., He, K., & Dollár, P. (2017). Focal loss for dense object detection. _ICCV_.
3. Hochreiter, S., & Schmidhuber, J. (1997). Long short-term memory. _Neural computation_, 9(8), 1735-1780.
4. Zhou, S., Pandita, R., Kurian, J., & Redmiles, D. (2012). Predicting long-term participation in open source projects. _ICSE_.
5. Steinmacher, I., Wiese, I. S., Conte, T., Gerosa, M. A., & Redmiles, D. (2014). The hard life of open source software project newcomers. _CHASE_.

---

**作成日：** 2025-10-26  
**バージョン：** 1.0  
**著者：** gerrit-retention 研究チーム
