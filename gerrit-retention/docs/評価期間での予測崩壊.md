# 評価期間での予測崩壊の問題

## 🚨 深刻な問題

### 訓練時のメトリクス（train_xxx/metrics.json）

| 訓練ラベル | Precision | Recall | 問題           |
| ---------- | --------- | ------ | -------------- |
| 0-3m       | 0.923     | 0.522  | Recall低い     |
| 3-6m       | 1.000     | 0.522  | Recall低い     |
| 6-9m       | 0.724     | 0.600  | Recall低い     |
| 9-12m      | 0.774     | 0.558  | Recall低い     |

**問題**: 保守的すぎる予測（半分見逃し）

---

### 評価時のメトリクス（train_xxx/eval_yyy/metrics.json）

| 訓練＼評価 | eval_0-3m            | eval_3-6m            | eval_6-9m            | eval_9-12m           |
| ---------- | -------------------- | -------------------- | -------------------- | -------------------- |
| train_0-3m | Prec=0.275, Rec=1.00 | Prec=0.275, Rec=1.00 | Prec=0.210, Rec=1.00 | Prec=0.257, Rec=1.00 |
| train_3-6m | Prec=0.275, Rec=1.00 | Prec=0.275, Rec=1.00 | Prec=0.210, Rec=1.00 | Prec=0.257, Rec=1.00 |
| train_6-9m | Prec=0.275, Rec=1.00 | Prec=0.275, Rec=1.00 | Prec=0.210, Rec=1.00 | Prec=0.257, Rec=1.00 |
| train_9-12m | Prec=0.275, Rec=1.00 | Prec=0.275, Rec=1.00 | Prec=0.210, Rec=1.00 | Prec=0.257, Rec=1.00 |

**問題**: 全員を「継続」と予測している！

---

## 📊 詳細分析

### Recall = 1.00 の意味

```
Recall = 1.00
→ 継続者を100%検出
→ つまり「全員を継続」と予測している
```

### Precision = 継続率

```
eval_0-3m: Precision = 0.275 = 継続率27.5%
eval_6-9m: Precision = 0.210 = 継続率21.0%

→ 全員を「継続」と予測した場合のPrecision
→ ベースライン予測と同じ
```

### 具体例

```
評価データ: 100人
  実際の継続者: 27人（継続率27%）
  実際の離脱者: 73人

モデルの予測:
  「継続」と予測: 100人（全員！）
  「離脱」と予測: 0人
  
結果:
  Precision = 27/100 = 0.27 ← 継続率と同じ
  Recall = 27/27 = 1.00 ← 全員を継続と予測
```

---

## 🔍 原因分析

### 仮説1: 閾値が高すぎる（または低すぎる）

```python
# 訓練時
optimal_threshold = 0.11  # 非常に低い閾値

# 評価時
# 同じ閾値を使っているはず...
# でも結果が逆？
```

**可能性**:
- 評価時に閾値が適用されていない
- 評価時のデフォルト閾値（0.5）を使っている
- 予測確率の分布が訓練時と評価時で大きく異なる

### 仮説2: 予測確率が全て高い

```python
# 評価時の予測確率（推測）
全員: 0.6-1.0 の確率
→ 閾値0.5以上
→ 全員「継続」と判定
```

### 仮説3: モデルの汎化失敗

```python
訓練データ: 特定のパターンを学習
評価データ: 全く異なる分布
→ モデルが混乱
→ 全員を継続と予測
```

### 仮説4: 評価スクリプトのバグ

```python
# 評価時のコード
if prob > threshold:
    prediction = 1  # 継続

# バグの可能性
threshold = 0.0  # デフォルト値？
# または
optimal_threshold が読み込まれていない
```

---

## 🔍 確認が必要な項目

### 1. 予測確率の分布

```python
# predictions.csv を確認
import pandas as pd

# 訓練時の予測
train_pred = pd.read_csv('outputs/sliding_cross_eval_nova/train_0-3m/predictions.csv')
print(train_pred['prediction_prob'].describe())

# 評価時の予測
eval_pred = pd.read_csv('outputs/sliding_cross_eval_nova/train_0-3m/eval_0-3m/predictions.csv')
print(eval_pred['prediction_prob'].describe())
```

**期待される結果**:
- 訓練時: 0.0-1.0の広い分布
- 評価時: ？（確認必要）

### 2. 閾値の適用

```python
# 評価スクリプトを確認
# train_irl_sliding_window.py の evaluate_model 関数

# 正しい実装
predictions = (probabilities >= optimal_threshold).astype(int)

# バグの可能性
predictions = (probabilities >= 0.5).astype(int)  # ハードコード？
```

### 3. 訓練データと評価データの違い

```python
# 訓練データの特徴量分布
train_data = ...
print(train_data.describe())

# 評価データの特徴量分布
eval_data = ...
print(eval_data.describe())

# 大きく異なる？
```

---

## 🛠️ 緊急修正が必要

### Step 1: 予測ファイルの確認

```bash
# 訓練時
cat outputs/sliding_cross_eval_nova/train_0-3m/predictions.csv

# 評価時
cat outputs/sliding_cross_eval_nova/train_0-3m/eval_0-3m/predictions.csv
```

### Step 2: スクリプトの確認

```bash
# 評価時の閾値適用を確認
grep -A 5 "optimal_threshold" scripts/training/irl/train_irl_sliding_window.py
```

### Step 3: 修正

**Option A: 閾値の問題**
```python
# train_irl_sliding_window.py の evaluate_model 関数

# 修正前（推測）
optimal_threshold = 0.5  # デフォルト

# 修正後
optimal_threshold = find_optimal_threshold(...)
predictions = (probabilities >= optimal_threshold).astype(int)
```

**Option B: 予測確率の問題**
```python
# モデルが評価データで過信している
# → Temperature Scaling で調整

probabilities = torch.sigmoid(logits / temperature)
```

---

## 📊 期待される改善

### 修正前（現状）

```
評価時:
  全員を「継続」と予測
  Precision: 0.21-0.28 (継続率と同じ)
  Recall: 1.00 (全員検出)
  → モデルが機能していない
```

### 修正後（期待）

```
評価時:
  適切な閾値で予測
  Precision: 0.6-0.8 (予測の質向上)
  Recall: 0.6-0.8 (バランス改善)
  → モデルが機能する
```

---

## 🚨 重要な発見

**訓練時のメトリクス（Precision高・Recall低）は意味がない可能性**

理由:
1. 評価時に完全に逆の結果（Precision低・Recall高）
2. 評価時は全員を継続と予測（モデル崩壊）
3. 訓練と評価で一貫性がない

**結論**:
- スクリプトのバグ修正が最優先
- Focal Loss調整は後回し
- まず正しく予測できるようにする

---

## 次のステップ

1. ⚠️ **predictions.csv を確認** → 予測確率の分布を見る
2. ⚠️ **評価スクリプトを確認** → 閾値適用のバグを探す
3. ⚠️ **修正して再実行** → 正しい評価結果を得る
4. ⬜ **Focal Loss調整** → 修正後に検討

**最優先**: 評価スクリプトのデバッグ！
