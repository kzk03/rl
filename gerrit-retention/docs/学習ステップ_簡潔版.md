# 7. 学習ステップ：損失関数・報酬関数・予測

## 7.1 概要

本研究では、**逆強化学習（IRL）** の枠組みで、継続したレビュアーの行動から報酬関数を学習し、継続予測を行う。

```
継続者の活動履歴 ─→ 報酬関数の学習 ─→ 継続確率の予測
```

---

## 7.2 損失関数：Focal Loss

### 7.2.1 データの不均衡問題

本研究のデータは**クラス不均衡**が顕著：

| クラス         | 説明                   | 割合   |
| -------------- | ---------------------- | ------ |
| 正例（継続者） | 指定期間後も活動がある | 約 20% |
| 負例（離脱者） | 指定期間後に活動がない | 約 80% |

通常の損失関数では、多数派（離脱者）に偏り、少数派（継続者）のパターンを学習できない。

### 7.2.2 Focal Loss の定義

**Focal Loss** はクラス不均衡に対処する損失関数：

$$
\text{FL}(p_t) = -\alpha_t (1 - p_t)^\gamma \log(p_t)
$$

**パラメータ**：

- $p_t$：正解クラスに対する予測確率
- $\alpha$：クラス重みパラメータ（本研究：0.3）
- $\gamma$：フォーカスパラメータ（本研究：3.0）

### 7.2.3 Focal Loss の効果

**1. 易しい例の重みを減らす**

$(1 - p_t)^\gamma$ により、正しく分類できている例の損失を抑制：

| 予測確信度 $p_t$ | 損失の減衰 |
| ---------------- | ---------- |
| 0.9（高い）      | 99.9% 減少 |
| 0.5（中程度）    | 87.5% 減少 |
| 0.3（低い）      | 65.7% 減少 |

→ 誤分類している困難な例に学習を集中できる。

**2. クラス重み調整**

継続率に応じて負例（離脱者）の重みを動的に調整：

$$
w_{\text{neg}} = \frac{p_{\text{pos}}}{1 - p_{\text{pos}}}
$$

継続率が 20%の場合、$w_{\text{neg}} \approx 0.26$ となり、正例（継続者）の学習を保護。

---

## 7.3 報酬関数の学習

### 7.3.1 逆強化学習（IRL）の原理

通常の強化学習と IRL の違い：

| アプローチ    | 既知                    | 目標                              |
| ------------- | ----------------------- | --------------------------------- |
| **通常の RL** | 報酬関数 $R(s,a)$       | 報酬を最大化する方策 $\pi^*$      |
| **IRL**       | 専門家の軌跡 $\{\tau\}$ | 専門家を説明する報酬関数 $R(s,a)$ |

本研究では：

- **専門家** = 継続したレビュアー（ラベル $y=1$）
- **目標** = 専門家の軌跡に高報酬、非専門家に低報酬を与える関数を学習

### 7.3.2 報酬関数のネットワーク表現

報酬関数はディープニューラルネットワークで表現：

```
状態 s_t (10次元) ─→ エンコーダー ─→ 埋め込み (64次元)
                                        │
行動 a_t (5次元)  ─→ エンコーダー ─→ 埋め込み (64次元)
                                        │
                                        ▼
                                   連結 (128次元)
                                        │
                                        ▼
                                   LSTM (2層)
                                        │
                                        ▼
                                 継続確率予測器
                                        │
                                        ▼
                                  継続確率 P
```

### 7.3.3 累積報酬と継続確率の関係

各時刻の報酬を累積し、Sigmoid 関数で確率に変換：

$$
P(\text{継続}) = \sigma\left(\sum_{t=1}^{T} R(s_t, a_t; \theta)\right)
$$

**Sigmoid の性質**：

- 累積報酬が高い → $P \approx 1$ （継続しやすい）
- 累積報酬が低い → $P \approx 0$ （離脱しやすい）

### 7.3.4 損失関数と報酬関数の関係

Focal Loss を最小化することで、報酬関数が自動的に学習される：

**継続者の軌跡（$y=1$）**：

- 損失を減らす → $P(\text{継続}) \to 1$ が必要
- つまり → 累積報酬 $\sum R(s_t, a_t)$ を**高く**する

**離脱者の軌跡（$y=0$）**：

- 損失を減らす → $P(\text{継続}) \to 0$ が必要
- つまり → 累積報酬 $\sum R(s_t, a_t)$ を**低く**する

**結論**：損失最小化により、「継続者に高報酬、離脱者に低報酬」を与える報酬関数が学習される。

---

## 7.4 学習プロセス

### 7.4.1 学習の全体フロー

```
1. データ準備
   ├─ 継続者の軌跡（y=1）
   └─ 離脱者の軌跡（y=0）
         │
         ▼
2. 順伝播（Forward Pass）
   軌跡 → エンコード → LSTM → 累積報酬 → Sigmoid → 継続確率 P
         │
         ▼
3. 損失計算
   L = Focal_Loss(P, y)
         │
         ▼
4. 逆伝播（Backpropagation）
   ∂L/∂θ を計算（勾配）
         │
         ▼
5. パラメータ更新
   θ ← θ - α・∂L/∂θ （Adam Optimizer）
         │
         ▼
6. 反復（100エポック）
   徐々に報酬関数が洗練される
```

### 7.4.2 最適化アルゴリズム

**Adam Optimizer**：

- 学習率：0.0001
- 動的な学習率調整（ReduceLROnPlateau）
- Dropout（0.3）による正則化

**学習設定**：

- エポック数：100
- バッチ処理：各レビュアーの全活動履歴を 1 バッチとして処理
- 可変長シーケンス：LSTM で各レビュアーの異なる活動数に対応

---

## 7.5 予測タスク

### 7.5.1 入力と出力

**入力**：

- レビュアーの過去の活動履歴（時系列）
  - 状態特徴量：経験日数、総活動数、活動トレンド等（10 次元）
  - 行動特徴量：活動種類、強度、品質等（5 次元）

**出力**：

- 指定期間後の継続確率：$P(\text{継続} | \text{履歴}, \text{期間})$
  - 例：0-3 ヶ月後に活動する確率

### 7.5.2 予測の流れ

```
レビュアーの活動履歴
  │
  ▼
時刻1: (状態1, 行動1) ─┐
時刻2: (状態2, 行動2) ─┤
  ...                 ├─→ LSTM処理 → 隠れ状態
時刻T: (状態T, 行動T) ─┘
  │
  ▼
累積報酬の算出
  │
  ▼
Sigmoid変換
  │
  ▼
継続確率 P ∈ [0, 1]
```

---

## 7.6 評価指標

モデルの性能を多面的に評価：

| 指標          | 定義                          | 意味                                       |
| ------------- | ----------------------------- | ------------------------------------------ |
| **AUC-ROC**   | ROC 曲線下面積                | 全体的な分類性能（0.5=ランダム、1.0=完璧） |
| **AUC-PR**    | PR 曲線下面積                 | 不均衡データでの性能（継続者を正しく予測） |
| **F1 スコア** | Precision × Recall の調和平均 | バランスの取れた性能指標                   |
| **Precision** | 継続予測の適合率              | 継続と予測したうち実際に継続した割合       |
| **Recall**    | 継続者の再現率                | 実際の継続者のうち正しく予測できた割合     |

**最適閾値の決定**：

- F1 スコアを最大化する閾値を訓練データで決定
- 評価データで同じ閾値を使用

---

## 7.7 学習の特徴

### 7.7.1 月次集約ラベル

各月末を基準点として継続ラベルを付与：

- 同一月内の全活動に同じラベルを適用
- 学習の安定性向上

### 7.7.2 スライディングウィンドウ

互いに排他的な時間窓で学習：

- 0-3m, 3-6m, 6-9m, 9-12m
- 各時間スケールに特有のパターンを独立して学習

### 7.7.3 可変長シーケンス処理

各レビュアーの活動数が異なる問題に対応：

- `pack_padded_sequence`による効率的な処理
- 実際の活動履歴の長さに応じた学習

---

## 7.8 まとめ

本研究の学習プロセスは以下の 3 ステップで構成される：

1. **Focal Loss による不均衡データ対処**

   - 少数派（継続者）のパターンを効果的に学習

2. **IRL による報酬関数学習**

   - 継続者の軌跡から「何が継続を促すか」を自動発見

3. **LSTM による時系列パターン学習**
   - 過去の活動履歴全体から継続確率を予測

これにより、明示的なルールを設計せずに、データから継続予測モデルを学習できる。
