# IRL æ™‚ç³»åˆ—ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦è¨­è¨ˆå¤‰æ›´æ¡ˆ

## ğŸ“‹ æ¦‚è¦

æœ¬ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã§ã¯ã€é€†å¼·åŒ–å­¦ç¿’ï¼ˆIRLï¼‰ã«ã‚ˆã‚‹é•·æœŸè²¢çŒ®è€…äºˆæ¸¬ã®è¨­è¨ˆå¤‰æ›´ã«ã¤ã„ã¦èª¬æ˜ã—ã¾ã™ã€‚

### å¤‰æ›´ã®ç›®çš„

ç¾çŠ¶ã®ã€Œn ãƒ¶æœˆå¾Œã®å˜ä¸€æ™‚ç‚¹ã§ã®è²¢çŒ®äºˆæ¸¬ã€ã‹ã‚‰ã€ã€Œç‰¹å®šæœŸé–“å†…ï¼ˆ0-2 ãƒ¶æœˆã€3-5 ãƒ¶æœˆãªã©ï¼‰ã®è²¢çŒ®äºˆæ¸¬ã€ã¸å¤‰æ›´ã—ã€ã‚ˆã‚Šå®Ÿç”¨çš„ãªäºˆæ¸¬ã‚’å®Ÿç¾ã—ã¾ã™ã€‚

---

## ğŸ”„ ç¾çŠ¶ã®è¨­è¨ˆ

### ç¾åœ¨ã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ

```
ã‚¿ã‚¤ãƒ ãƒ©ã‚¤ãƒ³:
|-------- å­¦ç¿’æœŸé–“ --------|------ äºˆæ¸¬ ------|
[history_start]    [snapshot_date]    [target_end]
                          |                |
                          |                +-- target_months å¾Œ
                          +-- åŸºæº–æ—¥
```

**ç‰¹å¾´:**

- ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆæ—¥ï¼ˆåŸºæº–æ—¥ï¼‰ã‚’å›ºå®š
- å­¦ç¿’æœŸé–“: `snapshot_date - history_months` ï½ `snapshot_date`
- äºˆæ¸¬æ™‚ç‚¹: `snapshot_date + target_months`ï¼ˆå˜ä¸€æ™‚ç‚¹ï¼‰
- ãƒ©ãƒ™ãƒ«: äºˆæ¸¬æ™‚ç‚¹ã« 1 ä»¶ã§ã‚‚è²¢çŒ®ãŒã‚ã‚Œã° `continued=True`

### ç¾åœ¨ã®å®Ÿè£…ï¼ˆä¾‹ï¼‰

```python
def extract_trajectories_with_window(df, snapshot_date, history_months, target_months):
    """
    ä¾‹: snapshot_date=2020-01-01, history=6m, target=3m

    å­¦ç¿’æœŸé–“: [2019-07-01, 2020-01-01)  â† éå»6ãƒ¶æœˆ
    äºˆæ¸¬æœŸé–“: [2020-01-01, 2020-04-01)  â† æœªæ¥3ãƒ¶æœˆï¼ˆå˜ä¸€æœŸé–“ï¼‰

    ãƒ©ãƒ™ãƒ«: äºˆæ¸¬æœŸé–“ä¸­ã«1ä»¶ã§ã‚‚æ´»å‹•ãŒã‚ã‚Œã° continued=True
    """
    history_start = snapshot_date - pd.DateOffset(months=history_months)
    target_end = snapshot_date + pd.DateOffset(months=target_months)

    history_df = df[(df[date_col] >= history_start) &
                    (df[date_col] < snapshot_date)]
    target_df = df[(df[date_col] >= snapshot_date) &
                   (df[date_col] < target_end)]

    # ãƒ¬ãƒ“ãƒ¥ã‚¢ãƒ¼ã”ã¨ã®ç¶™ç¶šåˆ¤å®š
    for reviewer in reviewers:
        continued = len(target_df[target_df[reviewer_col] == reviewer]) > 0
```

### ç¾çŠ¶ã®å•é¡Œç‚¹

1. **å˜ä¸€æ™‚ç‚¹ã®äºˆæ¸¬**: n ãƒ¶æœˆå¾Œã¨ã„ã† 1 ã¤ã®æœŸé–“ã§ã—ã‹äºˆæ¸¬ã§ããªã„
2. **å›ºå®šã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆ**: å­¦ç¿’æœŸé–“å†…ã®æ™‚é–“çš„ãªå¤‰åŒ–ã‚’æ´»ç”¨ã§ããªã„
3. **æœŸé–“ã®ç²’åº¦**: çŸ­æœŸãƒ»ä¸­æœŸãƒ»é•·æœŸãªã©ã€è¤‡æ•°ã®æ™‚é–“ã‚¹ã‚±ãƒ¼ãƒ«ã§ã®äºˆæ¸¬ãŒå›°é›£
4. **å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã®éåŠ¹ç‡**: å­¦ç¿’æœŸé–“å†…ã®å„æ™‚ç‚¹ã®ãƒ‡ãƒ¼ã‚¿ã‚’ 1 ã¤ã®ãƒ©ãƒ™ãƒ«ã«ã¾ã¨ã‚ã¦ã„ã‚‹

---

## ğŸ¯ å¤‰æ›´å¾Œã®è¨­è¨ˆ

### é‡è¦ãªè¨­è¨ˆæ–¹é‡ã®è»¢æ›

**å¾“æ¥ï¼ˆæ•™å¸«ã‚ã‚Šå­¦ç¿’çš„ IRLï¼‰:**

```python
trajectory = {
    'developer': {...},
    'activity_history': [...],  # ç‰¹å¾´é‡
    'continued': True,  # â† æ­£è§£ãƒ©ãƒ™ãƒ«ï¼ˆæå¤±è¨ˆç®—ã«ä½¿ç”¨ï¼‰
}

# è¨“ç·´æ™‚
loss = BCE(predicted_continuation, target_continuation)
# target_continuation ã¯ trajectory['continued'] ã‹ã‚‰æ¥ã‚‹
```

**æ–°è¨­è¨ˆï¼ˆç‰¹å¾´é‡ãƒ™ãƒ¼ã‚¹ IRLï¼‰:**

```python
trajectory = {
    'developer': {...},
    'activity_history': [...],
    # çŠ¶æ…‹ã®ç‰¹å¾´é‡ã¨ã—ã¦ã€Œnãƒ¶æœˆå¾Œã®è²¢çŒ®æœ‰ç„¡ã€ã‚’å«ã‚ã‚‹
    'temporal_contribution_features': {
        '1m_later': True,   # â† ç‰¹å¾´é‡ã®ä¸€éƒ¨
        '2m_later': False,
        '3m_later': True,
    },
    # æ­£è§£ãƒ©ãƒ™ãƒ«ã¯æ¸¡ã•ãªã„
}

# è¨“ç·´æ™‚
# è»Œè·¡ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆç‰¹å¾´é‡ã®çµ„ã¿åˆã‚ã›ï¼‰ã‹ã‚‰å ±é…¬é–¢æ•°ã‚’å­¦ç¿’
# æ˜ç¤ºçš„ãªæ•™å¸«ãƒ©ãƒ™ãƒ«ã¯ä½¿ã‚ãªã„
```

### æ–°ã—ã„ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ: å­¦ç¿’æœŸé–“å†…å‹•çš„ãƒ©ãƒ™ãƒªãƒ³ã‚°

```
ã‚¿ã‚¤ãƒ ãƒ©ã‚¤ãƒ³ï¼ˆ1å¹´é–“ã®å­¦ç¿’æœŸé–“ã®ä¾‹ï¼‰:
|-------------- å­¦ç¿’æœŸé–“å…¨ä½“ï¼ˆ12ãƒ¶æœˆï¼‰ ---------------|
[train_start]                             [train_end]
     |      |      |      |      |      |      |
     t1     t2     t3     t4     t5     t6     t7   â† ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°æ™‚ç‚¹
     |                                          |
     |                                          |
     +-- t1æ™‚ç‚¹ã®çŠ¶æ…‹ç‰¹å¾´é‡:                    +-- t7æ™‚ç‚¹ã®çŠ¶æ…‹ç‰¹å¾´é‡:
         - éå»ã®æ´»å‹•å±¥æ­´                           - éå»ã®æ´»å‹•å±¥æ­´
         - 1må¾Œè²¢çŒ®: âœ“ (ç‰¹å¾´é‡)                     - 1må¾Œè²¢çŒ®: âœ— (ç‰¹å¾´é‡)
         - 2må¾Œè²¢çŒ®: âœ“ (ç‰¹å¾´é‡)                     - 2må¾Œè²¢çŒ®: âœ“ (ç‰¹å¾´é‡)
         - 3må¾Œè²¢çŒ®: âœ— (ç‰¹å¾´é‡)                     - 3må¾Œè²¢çŒ®: âœ— (ç‰¹å¾´é‡)

         â€» æ­£è§£ãƒ©ãƒ™ãƒ«ã¯æ¸¡ã•ãªã„
```

### ä¸»è¦ãªå¤‰æ›´ç‚¹

#### 1. æœŸé–“ãƒ™ãƒ¼ã‚¹ã®ãƒ©ãƒ™ãƒ«å®šç¾©

**å¾“æ¥:**

```python
# å˜ä¸€æ™‚ç‚¹ã§ã®ãƒ©ãƒ™ãƒ«
label = has_activity_at(snapshot_date + n_months)
```

**å¤‰æ›´å¾Œ:**

```python
# æœŸé–“ãƒ™ãƒ¼ã‚¹ã®ãƒ©ãƒ™ãƒ«
label_0_2m = has_activity_in_period(t, t + 2_months)
label_3_5m = has_activity_in_period(t + 3_months, t + 5_months)
label_6_8m = has_activity_in_period(t + 6_months, t + 8_months)
```

#### 2. å­¦ç¿’æœŸé–“å†…ã®å‹•çš„ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°

**å¾“æ¥:** ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆæ—¥ 1 ç‚¹ã®ã¿

**å¤‰æ›´å¾Œ:** å­¦ç¿’æœŸé–“å†…ã®è¤‡æ•°æ™‚ç‚¹ã§ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°

```python
# å­¦ç¿’æœŸé–“: 2019-01-01 ï½ 2020-01-01 (12ãƒ¶æœˆ)
# ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°é–“éš”: 1ãƒ¶æœˆ

sampling_points = [
    2019-01-01,  # t1
    2019-02-01,  # t2
    2019-03-01,  # t3
    ...
    2019-12-01   # t12
]

# å„æ™‚ç‚¹ã§è¤‡æ•°ã®äºˆæ¸¬æœŸé–“ã®ãƒ©ãƒ™ãƒ«ã‚’ç”Ÿæˆ
for t in sampling_points:
    for period_def in prediction_periods:
        label = generate_label(t, period_def)
```

#### 3. å°†æ¥è²¢çŒ®ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ç‰¹å¾´é‡ã¨ã—ã¦çµ„ã¿è¾¼ã‚€

**å®šç¾©ä¾‹:**

```yaml
# çŠ¶æ…‹ç‰¹å¾´é‡ã«å«ã‚ã‚‹å°†æ¥ã®è²¢çŒ®ãƒ‘ã‚¿ãƒ¼ãƒ³
temporal_contribution_features:
  - name: "contrib_1m_later"
    offset_start_months: 0
    offset_end_months: 1
    description: "0-1ãƒ¶æœˆå¾Œã®è²¢çŒ®æœ‰ç„¡ï¼ˆäºŒå€¤ï¼‰"
    feature_type: "binary"

  - name: "contrib_2m_later"
    offset_start_months: 1
    offset_end_months: 2
    description: "1-2ãƒ¶æœˆå¾Œã®è²¢çŒ®æœ‰ç„¡ï¼ˆäºŒå€¤ï¼‰"
    feature_type: "binary"

  - name: "contrib_3m_later"
    offset_start_months: 2
    offset_end_months: 3
    description: "2-3ãƒ¶æœˆå¾Œã®è²¢çŒ®æœ‰ç„¡ï¼ˆäºŒå€¤ï¼‰"
    feature_type: "binary"
# ã“ã‚Œã‚‰ã¯çŠ¶æ…‹ç‰¹å¾´é‡ã®ä¸€éƒ¨ã¨ã—ã¦æ‰±ã‚ã‚Œã€
# æ­£è§£ãƒ©ãƒ™ãƒ«ã¨ã—ã¦ã¯ä½¿ç”¨ã—ãªã„
```

### æ–°ã—ã„ãƒ‡ãƒ¼ã‚¿æ§‹é€ 

#### è»Œè·¡ãƒ‡ãƒ¼ã‚¿ï¼ˆç‰¹å¾´é‡ãƒ™ãƒ¼ã‚¹ï¼‰

```python
trajectory = {
    'developer': developer_info,
    'activity_history': activity_history,  # ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°æ™‚ç‚¹ã¾ã§ã®å±¥æ­´
    'context_date': sampling_point,        # ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°æ™‚ç‚¹

    # å°†æ¥ã®è²¢çŒ®ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ã€ŒçŠ¶æ…‹ç‰¹å¾´é‡ã€ã¨ã—ã¦å«ã‚ã‚‹
    'temporal_contribution_features': {
        'contrib_1m_later': True,   # 0-1ãƒ¶æœˆå¾Œã«è²¢çŒ®ã‚ã‚Šï¼ˆç‰¹å¾´é‡ï¼‰
        'contrib_2m_later': False,  # 1-2ãƒ¶æœˆå¾Œã«è²¢çŒ®ãªã—ï¼ˆç‰¹å¾´é‡ï¼‰
        'contrib_3m_later': True,   # 2-3ãƒ¶æœˆå¾Œã«è²¢çŒ®ã‚ã‚Šï¼ˆç‰¹å¾´é‡ï¼‰
    },

    # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿
    'sampling_point': sampling_point,
    'history_window_months': 6,    # ã“ã®ã‚µãƒ³ãƒ—ãƒ«ã®å­¦ç¿’ã«ä½¿ã£ãŸæœŸé–“

    # æ­£è§£ãƒ©ãƒ™ãƒ«ã¯å«ã‚ãªã„ï¼ˆæ•™å¸«ãªã—å­¦ç¿’çš„ã‚¢ãƒ—ãƒ­ãƒ¼ãƒï¼‰
}
```

**é‡è¦ãªé•ã„:**

- å¾“æ¥: `'labels'` ã¨ã—ã¦æ­£è§£ãƒ©ãƒ™ãƒ«ã‚’æ ¼ç´ â†’ æå¤±è¨ˆç®—ã«ä½¿ç”¨
- æ–°è¨­è¨ˆ: `'temporal_contribution_features'` ã¨ã—ã¦çŠ¶æ…‹ç‰¹å¾´é‡ã«æ ¼ç´ â†’ ãƒ¢ãƒ‡ãƒ«ã¸ã®å…¥åŠ›ã«ä½¿ç”¨

---

## ğŸ’» å®Ÿè£…æ¡ˆ

### 1. ãƒ‡ãƒ¼ã‚¿æŠ½å‡ºé–¢æ•°ã®å¤‰æ›´

```python
def extract_temporal_trajectories_with_future_features(
    df: pd.DataFrame,
    train_start: pd.Timestamp,
    train_end: pd.Timestamp,
    history_window_months: int,
    temporal_contribution_periods: List[Dict[str, Any]],
    sampling_interval_months: int = 1,
    min_history_events: int = 1
) -> List[Dict[str, Any]]:
    """
    å­¦ç¿’æœŸé–“å†…ã®è¤‡æ•°æ™‚ç‚¹ã‹ã‚‰ã€å°†æ¥ã®è²¢çŒ®ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ç‰¹å¾´é‡ã¨ã—ã¦æŒã¤è»Œè·¡ã‚’æŠ½å‡º

    é‡è¦: ã“ã®é–¢æ•°ã¯å°†æ¥ã®è²¢çŒ®æƒ…å ±ã‚’ã€Œæ­£è§£ãƒ©ãƒ™ãƒ«ã€ã§ã¯ãªãã€ŒçŠ¶æ…‹ç‰¹å¾´é‡ã€ã¨ã—ã¦æ‰±ã„ã¾ã™

    Args:
        df: ãƒ¬ãƒ“ãƒ¥ãƒ¼ãƒ‡ãƒ¼ã‚¿
        train_start: å­¦ç¿’æœŸé–“ã®é–‹å§‹æ—¥
        train_end: å­¦ç¿’æœŸé–“ã®çµ‚äº†æ—¥
        history_window_months: å„ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°æ™‚ç‚¹ã§ä½¿ç”¨ã™ã‚‹å±¥æ­´ã®é•·ã•ï¼ˆãƒ¶æœˆï¼‰
        temporal_contribution_periods: ç‰¹å¾´é‡ã¨ã—ã¦å«ã‚ã‚‹å°†æ¥ã®è²¢çŒ®æœŸé–“ã®å®šç¾©
            [{'name': 'contrib_1m', 'start_months': 0, 'end_months': 1}, ...]
        sampling_interval_months: ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°é–“éš”ï¼ˆãƒ¶æœˆï¼‰
        min_history_events: ã‚µãƒ³ãƒ—ãƒ«ã«å¿…è¦ãªæœ€å°ã‚¤ãƒ™ãƒ³ãƒˆæ•°

    Returns:
        List[Dict]: è»Œè·¡ãƒ‡ãƒ¼ã‚¿ã®ãƒªã‚¹ãƒˆ
            å„è»Œè·¡ã«ã¯ 'temporal_contribution_features' ãŒå«ã¾ã‚Œã‚‹ï¼ˆæ­£è§£ãƒ©ãƒ™ãƒ«ã§ã¯ãªã„ï¼‰
    """
    logger.info(f"æ™‚ç³»åˆ—è»Œè·¡ã‚’æŠ½å‡ºä¸­: {train_start} ï½ {train_end}")
    logger.info(f"  å±¥æ­´ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦: {history_window_months}ãƒ¶æœˆ")
    logger.info(f"  ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°é–“éš”: {sampling_interval_months}ãƒ¶æœˆ")
    logger.info(f"  å°†æ¥è²¢çŒ®ç‰¹å¾´é‡: {len(temporal_contribution_periods)}ç¨®é¡")
    logger.info(f"  â€» ã“ã‚Œã‚‰ã¯æ­£è§£ãƒ©ãƒ™ãƒ«ã§ã¯ãªãçŠ¶æ…‹ç‰¹å¾´é‡ã¨ã—ã¦ä½¿ç”¨")

    trajectories = []
    reviewer_col = 'reviewer_email'
    date_col = 'request_time'

    # ãƒ‡ãƒ¼ã‚¿ã®æ—¥ä»˜ã‚«ãƒ©ãƒ ã‚’å¤‰æ›
    df = df.copy()
    df[date_col] = pd.to_datetime(df[date_col])

    # ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°æ™‚ç‚¹ã‚’ç”Ÿæˆ
    sampling_points = []
    current = train_start

    # æœ€å¤§ç‰¹å¾´é‡æœŸé–“ã‚’è¨ˆç®—ï¼ˆå°†æ¥ã®è²¢çŒ®ãƒ‡ãƒ¼ã‚¿ãŒå¿…è¦ãªæœ€å¤§æœŸé–“ï¼‰
    max_feature_months = max(
        period['end_months'] for period in temporal_contribution_periods
    )

    # ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°æ™‚ç‚¹ã¯ã€å°†æ¥ã®è²¢çŒ®ãƒ‡ãƒ¼ã‚¿ã‚’ç¢ºä¿ã§ãã‚‹ç¯„å›²å†…ã§è¨­å®š
    max_sampling_date = train_end - pd.DateOffset(months=max_feature_months)

    while current <= max_sampling_date:
        # å±¥æ­´ã‚’ååˆ†ã«ç¢ºä¿ã§ãã‚‹ã‹ç¢ºèª
        history_start = current - pd.DateOffset(months=history_window_months)
        if history_start >= df[date_col].min():
            sampling_points.append(current)
        current += pd.DateOffset(months=sampling_interval_months)

    logger.info(f"ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°æ™‚ç‚¹: {len(sampling_points)}ç‚¹")

    # å…¨ãƒ¬ãƒ“ãƒ¥ã‚¢ãƒ¼ã‚’å–å¾—
    reviewers = df[reviewer_col].unique()

    for sampling_point in sampling_points:
        # å±¥æ­´æœŸé–“ã‚’å®šç¾©
        history_start = sampling_point - pd.DateOffset(months=history_window_months)
        history_end = sampling_point

        for reviewer in reviewers:
            # ã“ã®ãƒ¬ãƒ“ãƒ¥ã‚¢ãƒ¼ã®å±¥æ­´ãƒ‡ãƒ¼ã‚¿
            history_df = df[
                (df[reviewer_col] == reviewer) &
                (df[date_col] >= history_start) &
                (df[date_col] < history_end)
            ]

            # æœ€å°ã‚¤ãƒ™ãƒ³ãƒˆæ•°ã‚’æº€ãŸã•ãªã„å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—
            if len(history_df) < min_history_events:
                continue

            # å°†æ¥ã®è²¢çŒ®ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’çŠ¶æ…‹ç‰¹å¾´é‡ã¨ã—ã¦è¨ˆç®—
            temporal_contrib_features = {}
            temporal_contrib_metadata = {}

            for period in temporal_contribution_periods:
                feature_name = period['name']
                start_months = period['start_months']
                end_months = period['end_months']

                # å°†æ¥ã®æœŸé–“ç¯„å›²
                future_start = sampling_point + pd.DateOffset(months=start_months)
                future_end = sampling_point + pd.DateOffset(months=end_months)

                # ã“ã®æœŸé–“å†…ã®æ´»å‹•ã‚’ç¢ºèª
                future_df = df[
                    (df[reviewer_col] == reviewer) &
                    (df[date_col] >= future_start) &
                    (df[date_col] < future_end)
                ]

                # ç‰¹å¾´é‡: æœŸé–“å†…ã«1ä»¶ã§ã‚‚æ´»å‹•ãŒã‚ã‚Œã° True
                # â€» ã“ã‚Œã¯æ­£è§£ãƒ©ãƒ™ãƒ«ã§ã¯ãªãã€çŠ¶æ…‹ã®ç‰¹å¾´é‡ã¨ã—ã¦æ‰±ã†
                has_activity = len(future_df) > 0
                temporal_contrib_features[feature_name] = has_activity

                # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿
                temporal_contrib_metadata[feature_name] = {
                    'future_start': future_start,
                    'future_end': future_end,
                    'activity_count': len(future_df),
                }

            # æ´»å‹•å±¥æ­´ã‚’æ§‹ç¯‰
            activity_history = []
            for _, row in history_df.iterrows():
                activity = {
                    'timestamp': row[date_col],
                    'action_type': 'review',
                    'project': row.get('project', 'unknown'),
                }
                activity_history.append(activity)

            # é–‹ç™ºè€…æƒ…å ±
            developer_info = {
                'developer_id': reviewer,
                'first_seen': history_df[date_col].min(),
                'changes_reviewed': len(history_df),
                'projects': history_df['project'].unique().tolist() if 'project' in history_df.columns else []
            }

            # è»Œè·¡ã‚’ä½œæˆï¼ˆç‰¹å¾´é‡ãƒ™ãƒ¼ã‚¹ï¼‰
            trajectory = {
                'developer': developer_info,
                'activity_history': activity_history,
                'context_date': sampling_point,

                # å°†æ¥ã®è²¢çŒ®ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ã€ŒçŠ¶æ…‹ç‰¹å¾´é‡ã€ã¨ã—ã¦æ ¼ç´
                # â€» ã“ã‚Œã¯æ­£è§£ãƒ©ãƒ™ãƒ«ã§ã¯ãªã„
                'temporal_contribution_features': temporal_contrib_features,
                'temporal_contribution_metadata': temporal_contrib_metadata,

                # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿
                'sampling_point': sampling_point,
                'history_start': history_start,
                'history_end': history_end,
                'history_window_months': history_window_months,
                'history_event_count': len(history_df),
            }

            trajectories.append(trajectory)

    logger.info(f"è»Œè·¡æŠ½å‡ºå®Œäº†: {len(trajectories)}ã‚µãƒ³ãƒ—ãƒ«")

    # å„ç‰¹å¾´é‡ã®åˆ†å¸ƒã‚’è¡¨ç¤º
    for period in temporal_contribution_periods:
        feature_name = period['name']
        positive_count = sum(
            1 for t in trajectories
            if t['temporal_contribution_features'][feature_name]
        )
        positive_rate = positive_count / len(trajectories) if trajectories else 0
        logger.info(f"  {feature_name}: Trueç‡ {positive_rate:.1%} "
                   f"({positive_count}/{len(trajectories)}) â€»ç‰¹å¾´é‡ã¨ã—ã¦")

    return trajectories
```

### 2. çŠ¶æ…‹ç‰¹å¾´é‡æŠ½å‡ºã®å¤‰æ›´ï¼ˆæœ€é‡è¦ï¼‰

**å¾“æ¥ã®çŠ¶æ…‹æŠ½å‡º:**

```python
def extract_developer_state(self, developer, activity_history, context_date):
    """å¾“æ¥: éå»ã®å±¥æ­´ã®ã¿ã‹ã‚‰ç‰¹å¾´é‡ã‚’æŠ½å‡º"""
    state = {
        'experience_days': ...,
        'total_reviews': ...,
        # éå»ã®æƒ…å ±ã®ã¿
    }
    return state
```

**æ–°ã—ã„çŠ¶æ…‹æŠ½å‡ºï¼ˆå°†æ¥ã®è²¢çŒ®ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’å«ã‚€ï¼‰:**

```python
def extract_developer_state(self, trajectory):
    """
    æ–°è¨­è¨ˆ: éå»ã®å±¥æ­´ + å°†æ¥ã®è²¢çŒ®ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ç‰¹å¾´é‡ã¨ã—ã¦æŠ½å‡º

    Args:
        trajectory: è»Œè·¡ãƒ‡ãƒ¼ã‚¿ï¼ˆtemporal_contribution_features ã‚’å«ã‚€ï¼‰

    Returns:
        çŠ¶æ…‹ãƒ™ã‚¯ãƒˆãƒ«ï¼ˆå°†æ¥ã®è²¢çŒ®ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’å«ã‚€ï¼‰
    """
    developer = trajectory['developer']
    activity_history = trajectory['activity_history']
    context_date = trajectory['context_date']

    # å¾“æ¥ã®éå»ãƒ™ãƒ¼ã‚¹ã®ç‰¹å¾´é‡
    state_features = []

    # 1. éå»ã®æ´»å‹•ã‹ã‚‰è¨ˆç®—ã•ã‚Œã‚‹ç‰¹å¾´é‡
    experience_days = (context_date - developer['first_seen']).days
    state_features.append(experience_days / 365.0)
    state_features.append(developer.get('changes_reviewed', 0) / 100.0)
    state_features.append(len(activity_history) / 100.0)
    # ... ä»–ã®éå»ãƒ™ãƒ¼ã‚¹ç‰¹å¾´é‡

    # 2. æ–°è¦: å°†æ¥ã®è²¢çŒ®ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ç‰¹å¾´é‡ã¨ã—ã¦è¿½åŠ 
    # â€» ã“ã‚Œã¯æ­£è§£ãƒ©ãƒ™ãƒ«ã§ã¯ãªãã€çŠ¶æ…‹ã‚’è¨˜è¿°ã™ã‚‹ç‰¹å¾´é‡ã®ä¸€éƒ¨
    temporal_contrib_feats = trajectory.get('temporal_contribution_features', {})

    # å„æœŸé–“ã®è²¢çŒ®æœ‰ç„¡ã‚’äºŒå€¤ç‰¹å¾´é‡ã¨ã—ã¦è¿½åŠ 
    for feature_name in sorted(temporal_contrib_feats.keys()):
        has_contribution = temporal_contrib_feats[feature_name]
        state_features.append(1.0 if has_contribution else 0.0)

    return np.array(state_features, dtype=np.float32)
```

**é‡è¦ãªå¤‰æ›´ç‚¹:**

- çŠ¶æ…‹ãƒ™ã‚¯ãƒˆãƒ«ã®æ¬¡å…ƒãŒå¢—åŠ ï¼ˆå°†æ¥ã®è²¢çŒ®ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†ã ã‘ï¼‰
- ä¾‹: å¾“æ¥ 10 æ¬¡å…ƒ â†’ æ–°è¨­è¨ˆ 13 æ¬¡å…ƒï¼ˆ3 ã¤ã®å°†æ¥è²¢çŒ®ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’è¿½åŠ ã—ãŸå ´åˆï¼‰
- ã“ã‚Œã‚‰ã¯ã€ŒçŠ¶æ…‹ã‚’è¨˜è¿°ã™ã‚‹ç‰¹å¾´é‡ã€ã§ã‚ã‚Šã€ã€Œäºˆæ¸¬ã™ã‚‹å¯¾è±¡ï¼ˆæ­£è§£ãƒ©ãƒ™ãƒ«ï¼‰ã€ã§ã¯ãªã„

### 3. IRL ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®å¤‰æ›´

**ç¾çŠ¶:** æ•™å¸«ã‚ã‚Šå­¦ç¿’çš„ã«ç¶™ç¶šç¢ºç‡ã‚’å‡ºåŠ›

```python
class RetentionIRLNetwork(nn.Module):
    def forward(self, state, action):
        # ...
        reward = self.reward_head(combined)
        continuation = torch.sigmoid(self.continuation_head(combined))
        return reward, continuation  # â† æ•™å¸«ãƒ©ãƒ™ãƒ«ã¨æ¯”è¼ƒã™ã‚‹ãŸã‚ã®å‡ºåŠ›
```

**å¤‰æ›´æ¡ˆ:** å ±é…¬ã®ã¿ã‚’å‡ºåŠ›ï¼ˆç¶™ç¶šç¢ºç‡ã®äºˆæ¸¬ã¯è¡Œã‚ãªã„ï¼‰

```python
class FeatureBasedIRLNetwork(nn.Module):
    """
    ç‰¹å¾´é‡ãƒ™ãƒ¼ã‚¹IRLãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯

    é‡è¦: å°†æ¥ã®è²¢çŒ®ãƒ‘ã‚¿ãƒ¼ãƒ³ã¯çŠ¶æ…‹ç‰¹å¾´é‡ã«å«ã¾ã‚Œã¦ãŠã‚Šã€
    ãƒ¢ãƒ‡ãƒ«ã¯å ±é…¬é–¢æ•°ã®ã¿ã‚’å­¦ç¿’ã™ã‚‹ï¼ˆç¶™ç¶šç¢ºç‡ã®äºˆæ¸¬ãƒ˜ãƒƒãƒ‰ã¯å‰Šé™¤ï¼‰
    """
    def __init__(
        self,
        state_dim: int,  # â† å°†æ¥ã®è²¢çŒ®ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†ã ã‘å¢—åŠ 
        action_dim: int,
        hidden_dim: int = 64,
        lstm_hidden: int = 128,
        sequence: bool = False
    ):
        super().__init__()
        self.state_dim = state_dim
        self.action_dim = action_dim
        self.sequence = sequence

        # çŠ¶æ…‹ãƒ»è¡Œå‹•ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼
        self.state_encoder = nn.Sequential(
            nn.Linear(state_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU()
        )

        self.action_encoder = nn.Sequential(
            nn.Linear(action_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU()
        )

        # LSTMï¼ˆæ™‚ç³»åˆ—ãƒ¢ãƒ¼ãƒ‰ï¼‰
        if sequence:
            self.lstm = nn.LSTM(
                input_size=hidden_dim,
                hidden_size=lstm_hidden,
                batch_first=True
            )
            final_dim = lstm_hidden
        else:
            final_dim = hidden_dim

        # å ±é…¬äºˆæ¸¬ãƒ˜ãƒƒãƒ‰ã®ã¿ï¼ˆç¶™ç¶šäºˆæ¸¬ãƒ˜ãƒƒãƒ‰ã¯ä¸è¦ï¼‰
        self.reward_head = nn.Sequential(
            nn.Linear(final_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, 1)
        )

    def forward(self, state, action):
        """
        Args:
            state: [batch, (seq_len,) state_dim]
                   â€» state_dim ã«ã¯å°†æ¥ã®è²¢çŒ®ãƒ‘ã‚¿ãƒ¼ãƒ³ãŒå«ã¾ã‚Œã‚‹
            action: [batch, (seq_len,) action_dim]

        Returns:
            reward: [batch, 1] - çŠ¶æ…‹-è¡Œå‹•ãƒšã‚¢ã®å ±é…¬
        """
        state_enc = self.state_encoder(state)
        action_enc = self.action_encoder(action)

        combined = state_enc + action_enc

        if self.sequence:
            # LSTMå‡¦ç†
            lstm_out, _ = self.lstm(combined)
            # æœ€çµ‚ã‚¹ãƒ†ãƒƒãƒ—ã®ã¿ä½¿ç”¨
            if lstm_out.dim() == 3:
                final = lstm_out[:, -1, :]
            else:
                final = lstm_out
        else:
            final = combined

        # å ±é…¬äºˆæ¸¬ã®ã¿
        reward = self.reward_head(final)

        return reward
```

### 4. è¨“ç·´ãƒ«ãƒ¼ãƒ—ã®å¤‰æ›´ï¼ˆé‡è¦ï¼‰

**å¾“æ¥ã®æ•™å¸«ã‚ã‚Šå­¦ç¿’çš„ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ:**

```python
# âŒ å¾“æ¥: æ­£è§£ãƒ©ãƒ™ãƒ«ã‚’ä½¿ç”¨
for trajectory in trajectories:
    predicted_continuation = model(state, action)
    true_label = trajectory['continued']  # æ­£è§£ãƒ©ãƒ™ãƒ«
    loss = BCE(predicted_continuation, true_label)
```

**æ–°è¨­è¨ˆ: ç‰¹å¾´é‡ãƒ™ãƒ¼ã‚¹ã®è‡ªå·±æ•™å¸«ã‚ã‚Šå­¦ç¿’:**

```python
def train_irl_feature_based(
    self,
    trajectories: List[Dict[str, Any]],
    epochs: int = 100
) -> Dict[str, Any]:
    """
    ç‰¹å¾´é‡ãƒ™ãƒ¼ã‚¹IRLè¨“ç·´

    é‡è¦: æ­£è§£ãƒ©ãƒ™ãƒ«ã¯ä½¿ç”¨ã›ãšã€å°†æ¥ã®è²¢çŒ®ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆç‰¹å¾´é‡ï¼‰ã‹ã‚‰
    å ±é…¬é–¢æ•°ã‚’å­¦ç¿’ã™ã‚‹

    Args:
        trajectories: è»Œè·¡ãƒ‡ãƒ¼ã‚¿
            å„è»Œè·¡ã«ã¯ 'temporal_contribution_features' ãŒå«ã¾ã‚Œã‚‹
        epochs: ã‚¨ãƒãƒƒã‚¯æ•°

    Returns:
        è¨“ç·´çµæœ
    """
    logger.info(f"ç‰¹å¾´é‡ãƒ™ãƒ¼ã‚¹IRLè¨“ç·´é–‹å§‹: {len(trajectories)}è»Œè·¡, {epochs}ã‚¨ãƒãƒƒã‚¯")
    logger.info("â€» æ­£è§£ãƒ©ãƒ™ãƒ«ã¯ä½¿ç”¨ã›ãšã€ç‰¹å¾´é‡ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã‹ã‚‰å­¦ç¿’")

    training_losses = []

    for epoch in range(epochs):
        epoch_loss = 0.0
        batch_count = 0

        for trajectory in trajectories:
            try:
                # çŠ¶æ…‹ã‚’æŠ½å‡ºï¼ˆå°†æ¥ã®è²¢çŒ®ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’å«ã‚€ï¼‰
                state = self.extract_developer_state(trajectory)
                actions = self.extract_developer_actions(
                    trajectory['activity_history'],
                    trajectory['context_date']
                )

                if not actions:
                    continue

                # æ™‚ç³»åˆ—å‡¦ç†
                if self.sequence:
                    # ã‚·ãƒ¼ã‚±ãƒ³ã‚¹æ§‹ç¯‰
                    if len(actions) < self.seq_len:
                        padded_actions = [actions[0]] * (self.seq_len - len(actions)) + actions
                    else:
                        padded_actions = actions[-self.seq_len:]

                    state_seq = torch.stack([
                        self.state_to_tensor(state) for _ in range(self.seq_len)
                    ]).unsqueeze(0)
                    action_seq = torch.stack([
                        self.action_to_tensor(action) for action in padded_actions
                    ]).unsqueeze(0)

                    predicted_reward = self.network(state_seq, action_seq)
                else:
                    # éæ™‚ç³»åˆ—ãƒ¢ãƒ¼ãƒ‰
                    state_tensor = self.state_to_tensor(state).unsqueeze(0)
                    action_tensor = self.action_to_tensor(actions[-1]).unsqueeze(0)
                    predicted_reward = self.network(state_tensor, action_tensor)

                # æå¤±è¨ˆç®—ï¼ˆè¤‡æ•°ã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒãŒå¯èƒ½ï¼‰
                # ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ1: MaxEnt IRLé¢¨ - å°†æ¥è²¢çŒ®ãŒã‚ã‚‹çŠ¶æ…‹ã®å ±é…¬ã‚’æœ€å¤§åŒ–
                temporal_features = trajectory['temporal_contribution_features']

                # å°†æ¥ã®è²¢çŒ®ãƒ‘ã‚¿ãƒ¼ãƒ³ã‹ã‚‰ã€Œæœ›ã¾ã—ã•ã€ã‚’è¨ˆç®—
                # ä¾‹: ã„ãšã‚Œã‹ã®æœŸé–“ã§è²¢çŒ®ãŒã‚ã‚Œã°æœ›ã¾ã—ã„
                has_any_contribution = any(temporal_features.values())

                if has_any_contribution:
                    # è²¢çŒ®ãŒã‚ã‚‹çŠ¶æ…‹ â†’ å ±é…¬ã‚’é«˜ãã—ãŸã„
                    target_reward = torch.tensor([[1.0]], device=self.device)
                else:
                    # è²¢çŒ®ãŒãªã„çŠ¶æ…‹ â†’ å ±é…¬ã‚’ä½ãã—ãŸã„
                    target_reward = torch.tensor([[0.0]], device=self.device)

                # MSEæå¤±
                loss = F.mse_loss(predicted_reward, target_reward)

                # é€†ä¼æ’­
                self.optimizer.zero_grad()
                loss.backward()
                self.optimizer.step()

                # æå¤±ã‚’è¨˜éŒ²
                epoch_loss += loss.item()
                batch_count += 1

            except Exception as e:
                logger.warning(f"è»Œè·¡å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
                continue

        if batch_count > 0:
            # å¹³å‡æå¤±ã‚’è¨ˆç®—
            epoch_loss /= batch_count
            training_losses.append(epoch_loss)

            if (epoch + 1) % 10 == 0:
                logger.info(f"Epoch {epoch+1}/{epochs}: æå¤±={epoch_loss:.4f}")

    return {
        'losses': training_losses,
        'final_loss': training_losses[-1] if training_losses else 0
    }
```

**é‡è¦ãªãƒã‚¤ãƒ³ãƒˆ:**

1. **æ­£è§£ãƒ©ãƒ™ãƒ«ã¯ä½¿ã‚ãªã„**: `trajectory['labels']` ã®ä»£ã‚ã‚Šã« `trajectory['temporal_contribution_features']` ã‚’ä½¿ç”¨
2. **ç‰¹å¾´é‡ã‹ã‚‰ç›®æ¨™ã‚’ç”Ÿæˆ**: å°†æ¥ã®è²¢çŒ®ãƒ‘ã‚¿ãƒ¼ãƒ³ã‹ã‚‰å ±é…¬ã®ç›®æ¨™å€¤ã‚’è‡ªå‹•çš„ã«è¨­å®š
3. **ã‚·ãƒ³ãƒ—ãƒ«ãªæå¤±**: MSE æå¤±ã®ã¿ï¼ˆç¶™ç¶šç¢ºç‡ã®äºˆæ¸¬ãƒ˜ãƒƒãƒ‰ãŒãªã„ãŸã‚ï¼‰

**ä»£æ›¿ã‚¢ãƒ—ãƒ­ãƒ¼ãƒï¼ˆã‚ˆã‚Šæ´—ç·´ã•ã‚ŒãŸæ–¹æ³•ï¼‰:**

```python
# ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ2: ç¶™ç¶šçš„ãªå ±é…¬ã‚¹ã‚±ãƒ¼ãƒ«
# å°†æ¥ã®è²¢çŒ®å›æ•°ã«å¿œã˜ã¦å ±é…¬ã‚’æ®µéšçš„ã«è¨­å®š
def calculate_target_reward_from_features(temporal_features):
    """
    å°†æ¥ã®è²¢çŒ®ãƒ‘ã‚¿ãƒ¼ãƒ³ã‹ã‚‰ç›®æ¨™å ±é…¬ã‚’è¨ˆç®—
    """
    contribution_count = sum(1 for v in temporal_features.values() if v)
    total_periods = len(temporal_features)

    # è²¢çŒ®æœŸé–“ã®å‰²åˆã‚’å ±é…¬ã«å¤‰æ›
    reward = contribution_count / total_periods
    return reward

# è¨“ç·´ãƒ«ãƒ¼ãƒ—å†…ã§ä½¿ç”¨
temporal_features = trajectory['temporal_contribution_features']
target_reward_value = calculate_target_reward_from_features(temporal_features)
target_reward = torch.tensor([[target_reward_value]], device=self.device)
loss = F.mse_loss(predicted_reward, target_reward)
```

**ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ 3: ã‚³ãƒ³ãƒˆãƒ©ã‚¹ãƒˆå­¦ç¿’**

```python
# åŒã˜ãƒãƒƒãƒå†…ã§ç•°ãªã‚‹è²¢çŒ®ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æŒã¤ã‚µãƒ³ãƒ—ãƒ«ã‚’å¯¾æ¯”
def train_irl_contrastive(self, trajectories, epochs=100):
    """
    ã‚³ãƒ³ãƒˆãƒ©ã‚¹ãƒˆå­¦ç¿’ãƒ™ãƒ¼ã‚¹ã®IRLè¨“ç·´

    å°†æ¥ã®è²¢çŒ®ãƒ‘ã‚¿ãƒ¼ãƒ³ãŒä¼¼ã¦ã„ã‚‹ã‚µãƒ³ãƒ—ãƒ«ã¯ä¼¼ãŸå ±é…¬ã‚’æŒã¡ã€
    ç•°ãªã‚‹ã‚µãƒ³ãƒ—ãƒ«ã¯ç•°ãªã‚‹å ±é…¬ã‚’æŒã¤ã‚ˆã†ã«å­¦ç¿’
    """
    for epoch in range(epochs):
        # ãƒãƒƒãƒã‚’ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
        batch = random.sample(trajectories, min(32, len(trajectories)))

        # å„ã‚µãƒ³ãƒ—ãƒ«ã®å ±é…¬ã‚’äºˆæ¸¬
        rewards = []
        features_list = []

        for traj in batch:
            state = self.extract_developer_state(traj)
            actions = self.extract_developer_actions(traj['activity_history'], traj['context_date'])

            # ... state_tensor, action_tensorã‚’ä½œæˆ
            reward = self.network(state_tensor, action_tensor)
            rewards.append(reward)

            # å°†æ¥ã®è²¢çŒ®ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’è¨˜éŒ²
            temporal_feats = traj['temporal_contribution_features']
            features_list.append(temporal_feats)

        # ã‚³ãƒ³ãƒˆãƒ©ã‚¹ãƒˆæå¤±
        # ä¼¼ãŸå°†æ¥ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æŒã¤ã‚µãƒ³ãƒ—ãƒ«ã®å ±é…¬ã‚’è¿‘ã¥ã‘ã‚‹
        loss = contrastive_loss(rewards, features_list)

        # é€†ä¼æ’­
        self.optimizer.zero_grad()
        loss.backward()
        self.optimizer.step()
```

### 4. äºˆæ¸¬é–¢æ•°ã®å¤‰æ›´

```python
def predict_continuation_multi_period(
    self,
    developer: Dict[str, Any],
    activity_history: List[Dict[str, Any]],
    context_date: datetime
) -> Dict[str, Any]:
    """
    è¤‡æ•°æœŸé–“ã®ç¶™ç¶šç¢ºç‡ã‚’äºˆæ¸¬

    Returns:
        {
            'continuation_probabilities': {
                'short_term': 0.85,
                'mid_term': 0.62,
                'long_term': 0.43
            },
            'predictions': {
                'short_term': True,   # >= 0.5
                'mid_term': True,
                'long_term': False
            },
            'reasoning': "..."
        }
    """
    self.network.eval()

    with torch.no_grad():
        state = self.extract_developer_state(developer, activity_history, context_date)
        actions = self.extract_developer_actions(activity_history, context_date)

        if not actions:
            return {
                'continuation_probabilities': {
                    period: 0.0 for period in self.network.prediction_periods
                },
                'predictions': {
                    period: False for period in self.network.prediction_periods
                },
                'reasoning': 'æ´»å‹•å±¥æ­´ãŒä¸è¶³ã—ã¦ã„ã¾ã™'
            }

        # æ¨è«–
        if self.sequence:
            # ã‚·ãƒ¼ã‚±ãƒ³ã‚¹æ§‹ç¯‰
            if len(actions) < self.seq_len:
                padded_actions = [actions[0]] * (self.seq_len - len(actions)) + actions
            else:
                padded_actions = actions[-self.seq_len:]

            state_seq = torch.stack([
                self.state_to_tensor(state) for _ in range(self.seq_len)
            ]).unsqueeze(0)
            action_seq = torch.stack([
                self.action_to_tensor(action) for action in padded_actions
            ]).unsqueeze(0)

            predicted_reward, predicted_continuations = self.network(state_seq, action_seq)
        else:
            state_tensor = self.state_to_tensor(state).unsqueeze(0)
            action_tensor = self.action_to_tensor(actions[-1]).unsqueeze(0)
            predicted_reward, predicted_continuations = self.network(state_tensor, action_tensor)

        # çµæœã‚’æŠ½å‡º
        continuation_probs = {
            period: float(pred.item())
            for period, pred in predicted_continuations.items()
        }

        predictions = {
            period: prob >= 0.5
            for period, prob in continuation_probs.items()
        }

        # ç†ç”±ä»˜ã‘
        reasoning_parts = []
        for period, prob in continuation_probs.items():
            if prob >= 0.7:
                reasoning_parts.append(f"{period}: é«˜ç¢ºç‡ã§ç¶™ç¶š ({prob:.1%})")
            elif prob >= 0.5:
                reasoning_parts.append(f"{period}: ç¶™ç¶šã®å¯èƒ½æ€§ã‚ã‚Š ({prob:.1%})")
            else:
                reasoning_parts.append(f"{period}: é›¢è„±ã®å¯èƒ½æ€§ ({prob:.1%})")

        reasoning = "; ".join(reasoning_parts)

        return {
            'continuation_probabilities': continuation_probs,
            'predictions': predictions,
            'reasoning': reasoning,
            'reward': float(predicted_reward.item())
        }
```

### 5. è©•ä¾¡é–¢æ•°ã®å¤‰æ›´

```python
def evaluate_irl_model_multi_period(
    irl_system: 'MultiPeriodRetentionIRLSystem',
    test_trajectories: List[Dict[str, Any]]
) -> Dict[str, Dict[str, float]]:
    """
    ãƒãƒ«ãƒæœŸé–“IRLãƒ¢ãƒ‡ãƒ«ã‚’è©•ä¾¡

    Returns:
        {
            'short_term': {
                'auc_roc': 0.85,
                'auc_pr': 0.90,
                'f1': 0.78,
                ...
            },
            'mid_term': {...},
            'long_term': {...}
        }
    """
    logger.info("ãƒãƒ«ãƒæœŸé–“IRLãƒ¢ãƒ‡ãƒ«ã‚’è©•ä¾¡ä¸­...")

    # å„æœŸé–“ã”ã¨ã®äºˆæ¸¬çµæœã‚’æ ¼ç´
    period_results = {
        period: {'y_true': [], 'y_pred': []}
        for period in irl_system.network.prediction_periods
    }

    for trajectory in test_trajectories:
        developer = trajectory['developer']
        activity_history = trajectory['activity_history']
        context_date = trajectory['context_date']
        true_labels = trajectory['labels']

        # äºˆæ¸¬å®Ÿè¡Œ
        prediction = irl_system.predict_continuation_multi_period(
            developer, activity_history, context_date
        )

        # å„æœŸé–“ã®çµæœã‚’è¨˜éŒ²
        for period in irl_system.network.prediction_periods:
            period_results[period]['y_true'].append(
                1 if true_labels[period] else 0
            )
            period_results[period]['y_pred'].append(
                prediction['continuation_probabilities'][period]
            )

    # å„æœŸé–“ã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’è¨ˆç®—
    evaluation_results = {}

    for period, data in period_results.items():
        y_true = data['y_true']
        y_pred = data['y_pred']
        y_pred_binary = [1 if p >= 0.5 else 0 for p in y_pred]

        metrics = {}

        # AUC-ROC
        try:
            metrics['auc_roc'] = roc_auc_score(y_true, y_pred)
        except:
            metrics['auc_roc'] = 0.0

        # AUC-PR
        try:
            precision_curve, recall_curve, _ = precision_recall_curve(y_true, y_pred)
            metrics['auc_pr'] = auc(recall_curve, precision_curve)
        except:
            metrics['auc_pr'] = 0.0

        # F1, Precision, Recall
        try:
            metrics['f1'] = f1_score(y_true, y_pred_binary, zero_division=0)
            metrics['precision'] = precision_score(y_true, y_pred_binary, zero_division=0)
            metrics['recall'] = recall_score(y_true, y_pred_binary, zero_division=0)
        except:
            metrics['f1'] = 0.0
            metrics['precision'] = 0.0
            metrics['recall'] = 0.0

        # ã‚µãƒ³ãƒ—ãƒ«æ•°
        metrics['test_samples'] = len(y_true)
        metrics['positive_samples'] = sum(y_true)
        metrics['positive_rate'] = sum(y_true) / len(y_true) if y_true else 0

        evaluation_results[period] = metrics

        logger.info(f"{period}: AUC-ROC={metrics['auc_roc']:.3f}, "
                   f"AUC-PR={metrics['auc_pr']:.3f}, F1={metrics['f1']:.3f}, "
                   f"æ­£ä¾‹ç‡={metrics['positive_rate']:.1%}")

    return evaluation_results
```

---

## ğŸ“Š å®Ÿé¨“è¨­å®šä¾‹

### è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«

```yaml
# configs/irl_multi_period_config.yaml

irl_multi_period:
  # å­¦ç¿’æœŸé–“è¨­å®š
  training_period:
    start_date: "2019-01-01"
    end_date: "2020-01-01"

  # å±¥æ­´ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ï¼ˆå„ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°æ™‚ç‚¹ã§ä½¿ç”¨ï¼‰
  history_window_months: 6

  # ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°è¨­å®š
  sampling:
    interval_months: 1 # 1ãƒ¶æœˆã”ã¨ã«ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
    min_history_events: 3 # æœ€å°ã‚¤ãƒ™ãƒ³ãƒˆæ•°

  # äºˆæ¸¬æœŸé–“å®šç¾©
  prediction_periods:
    - name: "immediate"
      start_months: 0
      end_months: 2
      description: "å³æ™‚ï¼ˆ0-2ãƒ¶æœˆå¾Œï¼‰"
      loss_weight: 1.0

    - name: "short_term"
      start_months: 3
      end_months: 5
      description: "çŸ­æœŸï¼ˆ3-5ãƒ¶æœˆå¾Œï¼‰"
      loss_weight: 1.0

    - name: "mid_term"
      start_months: 6
      end_months: 8
      description: "ä¸­æœŸï¼ˆ6-8ãƒ¶æœˆå¾Œï¼‰"
      loss_weight: 1.0

    - name: "long_term"
      start_months: 9
      end_months: 12
      description: "é•·æœŸï¼ˆ9-12ãƒ¶æœˆå¾Œï¼‰"
      loss_weight: 0.8 # é•·æœŸã¯ä¸ç¢ºå®Ÿæ€§ãŒé«˜ã„ãŸã‚é‡ã¿ã‚’ä¸‹ã’ã‚‹

  # ãƒ¢ãƒ‡ãƒ«è¨­å®š
  model:
    state_dim: 10
    action_dim: 5
    hidden_dim: 64
    lstm_hidden: 128
    sequence: true
    seq_len: 15

  # è¨“ç·´è¨­å®š
  training:
    epochs: 30
    learning_rate: 0.001
    test_split: 0.2
```

### å®Ÿè¡Œã‚³ãƒãƒ³ãƒ‰ä¾‹

```bash
# ãƒãƒ«ãƒæœŸé–“IRLè¨“ç·´ã¨è©•ä¾¡
uv run python scripts/training/irl/train_multi_period_irl.py \
  --config configs/irl_multi_period_config.yaml \
  --reviews data/review_requests_openstack_multi_5y_detail.csv \
  --output outputs/irl_multi_period
```

---

## ğŸ“ˆ æœŸå¾…ã•ã‚Œã‚‹åŠ¹æœ

### 1. ã‚ˆã‚Šç´°ã‹ã„ç²’åº¦ã®äºˆæ¸¬

**å¾“æ¥:**

- ã€Œ6 ãƒ¶æœˆå¾Œã«è²¢çŒ®ã—ã¦ã„ã‚‹ã‹ï¼Ÿã€ã®ã¿

**å¤‰æ›´å¾Œ:**

- ã€Œ0-2 ãƒ¶æœˆå¾Œã¯ç¶™ç¶š (90%ç¢ºç‡)ã€
- ã€Œ3-5 ãƒ¶æœˆå¾Œã¯å¾®å¦™ (55%ç¢ºç‡)ã€
- ã€Œ6-8 ãƒ¶æœˆå¾Œã¯é›¢è„± (30%ç¢ºç‡)ã€

â†’ **æ™‚é–“çš„ãªå¤‰åŒ–ã‚’æ‰ãˆã‚‰ã‚Œã‚‹**

### 2. å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã®åŠ¹ç‡çš„æ´»ç”¨

**å¾“æ¥:** å­¦ç¿’æœŸé–“ 12 ãƒ¶æœˆ â†’ 1 ã‚µãƒ³ãƒ—ãƒ«

**å¤‰æ›´å¾Œ:** å­¦ç¿’æœŸé–“ 12 ãƒ¶æœˆ â†’ 12 ã‚µãƒ³ãƒ—ãƒ«ï¼ˆ1 ãƒ¶æœˆé–“éš”ã®å ´åˆï¼‰

â†’ **ãƒ‡ãƒ¼ã‚¿åŠ¹ç‡ãŒå¤§å¹…ã«å‘ä¸Š**

### 3. å®Ÿç”¨çš„ãªä»‹å…¥ã‚¿ã‚¤ãƒŸãƒ³ã‚°ã®ææ¡ˆ

```python
prediction = model.predict_continuation_multi_period(developer, history, date)

if prediction['predictions']['immediate']:  # 0-2ãƒ¶æœˆã¯ç¶™ç¶š
    if not prediction['predictions']['short_term']:  # 3-5ãƒ¶æœˆã§é›¢è„±ãƒªã‚¹ã‚¯
        print("âš ï¸ 3ãƒ¶æœˆä»¥å†…ã«ä»‹å…¥ãŒå¿…è¦")
        print("æ¨å¥¨ã‚¢ã‚¯ã‚·ãƒ§ãƒ³: ãƒ¡ãƒ³ã‚¿ãƒªãƒ³ã‚°ã€é©åˆ‡ãªã‚¿ã‚¹ã‚¯ã®å‰²ã‚Šå½“ã¦")
```

### 4. æ™‚ç³»åˆ—ãƒ‘ã‚¿ãƒ¼ãƒ³ã®å­¦ç¿’

LSTM ã«ã‚ˆã‚Šã€ä»¥ä¸‹ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’å­¦ç¿’å¯èƒ½:

- å¾ã€…ã«æ´»å‹•ãŒæ¸›å°‘ â†’ é›¢è„±ãƒªã‚¹ã‚¯é«˜
- ä¸€æ™‚çš„ãªä¼‘æ­¢å¾Œã«å¾©å¸° â†’ é•·æœŸçš„ã«ã¯ç¶™ç¶š
- çŸ­æœŸçš„ã«æ´»ç™º â†’ ãã®å¾Œç‡ƒãˆå°½ã

---

## ğŸš€ å®Ÿè£…ãƒ­ãƒ¼ãƒ‰ãƒãƒƒãƒ—

### Phase 1: ãƒ‡ãƒ¼ã‚¿æŠ½å‡ºã®å®Ÿè£…ï¼ˆ1-2 æ—¥ï¼‰

- [ ] `extract_temporal_trajectories_multi_window()` ã®å®Ÿè£…
- [ ] æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã§ã®å‹•ä½œç¢ºèª
- [ ] ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°é–“éš”ãƒ»å±¥æ­´ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿èª¿æ•´

### Phase 2: ãƒ¢ãƒ‡ãƒ«æ‹¡å¼µï¼ˆ2-3 æ—¥ï¼‰

- [ ] `MultiPeriodRetentionIRLNetwork` ã®å®Ÿè£…
- [ ] ãƒãƒ«ãƒãƒ˜ãƒƒãƒ‰æ§‹é€ ã®ãƒ†ã‚¹ãƒˆ
- [ ] æ—¢å­˜ãƒ¢ãƒ‡ãƒ«ã¨ã®äº’æ›æ€§ç¢ºä¿

### Phase 3: è¨“ç·´ãƒ»è©•ä¾¡ã®å®Ÿè£…ï¼ˆ2-3 æ—¥ï¼‰

- [ ] `train_irl_multi_period()` ã®å®Ÿè£…
- [ ] æå¤±é‡ã¿ä»˜ã‘ã®èª¿æ•´æ©Ÿèƒ½
- [ ] `evaluate_irl_model_multi_period()` ã®å®Ÿè£…
- [ ] æœŸé–“ã”ã¨ã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹å¯è¦–åŒ–

### Phase 4: å®Ÿé¨“ã¨æ¤œè¨¼ï¼ˆ3-5 æ—¥ï¼‰

- [ ] ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã§ã®å‹•ä½œç¢ºèª
- [ ] OpenStack å®Ÿãƒ‡ãƒ¼ã‚¿ã§ã®è©•ä¾¡
- [ ] ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°
- [ ] å¾“æ¥æ‰‹æ³•ã¨ã®æ¯”è¼ƒå®Ÿé¨“

### Phase 5: ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ•´å‚™ï¼ˆ1-2 æ—¥ï¼‰

- [ ] å®Ÿè£…ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®æ›´æ–°
- [ ] ä½¿ç”¨ä¾‹ã®ä½œæˆ
- [ ] README ã®æ›´æ–°

**ç·æ‰€è¦æ™‚é–“: ç´„ 10-15 æ—¥**

---

## ğŸ” ä»£æ›¿æ¡ˆãƒ»æ‹¡å¼µæ¡ˆ

### æ¡ˆ 1: ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦æ•°ã®å‹•çš„èª¿æ•´

```python
# ãƒ‡ãƒ¼ã‚¿ãŒè±Šå¯ŒãªæœŸé–“ã¯ç´°ã‹ãã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
if data_density_high:
    sampling_interval_months = 1  # 1ãƒ¶æœˆã”ã¨
else:
    sampling_interval_months = 3  # 3ãƒ¶æœˆã”ã¨
```

### æ¡ˆ 2: é‡è¤‡æœŸé–“ã®å®šç¾©

```python
# é‡è¤‡ã‚’è¨±ã™ï¼ˆã‚ˆã‚Šå¯†ãªäºˆæ¸¬ï¼‰
prediction_periods = [
    {'name': '0-3m', 'start': 0, 'end': 3},
    {'name': '2-5m', 'start': 2, 'end': 5},  # é‡è¤‡
    {'name': '4-7m', 'start': 4, 'end': 7},  # é‡è¤‡
]
```

### æ¡ˆ 3: é€£ç¶šå€¤ã§ã®è²¢çŒ®åº¦äºˆæ¸¬

```python
# äºŒå€¤åˆ†é¡ã§ã¯ãªãã€æœŸé–“å†…ã®è²¢çŒ®é‡ã‚’äºˆæ¸¬
label = {
    'short_term': {
        'has_activity': True,
        'activity_count': 5,      # æœŸé–“å†…ã®ãƒ¬ãƒ“ãƒ¥ãƒ¼æ•°
        'activity_intensity': 0.7  # å¹³å‡æ´»å‹•å¼·åº¦
    }
}
```

---

## ğŸ’¡ ã¾ã¨ã‚

### ä¸»è¦ãªå¤‰æ›´ç‚¹

1. **æœŸé–“ãƒ™ãƒ¼ã‚¹ã®ãƒ©ãƒ™ãƒ«**: å˜ä¸€æ™‚ç‚¹ â†’ æœŸé–“ï¼ˆ0-2m, 3-5m ãªã©ï¼‰
2. **å‹•çš„ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°**: å›ºå®šã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆ â†’ å­¦ç¿’æœŸé–“å†…ã®è¤‡æ•°æ™‚ç‚¹
3. **ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯å­¦ç¿’**: å˜ä¸€äºˆæ¸¬ â†’ è¤‡æ•°æœŸé–“ã®åŒæ™‚äºˆæ¸¬
4. **ãƒ‡ãƒ¼ã‚¿åŠ¹ç‡**: 1 ã‚µãƒ³ãƒ—ãƒ« â†’ N ã‚µãƒ³ãƒ—ãƒ«ï¼ˆå­¦ç¿’æœŸé–“ã«æ¯”ä¾‹ï¼‰

### ãƒ¡ãƒªãƒƒãƒˆ

âœ… ã‚ˆã‚Šå®Ÿç”¨çš„ãªäºˆæ¸¬ï¼ˆã„ã¤ä»‹å…¥ã™ã¹ãã‹åˆ†ã‹ã‚‹ï¼‰  
âœ… å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã®åŠ¹ç‡çš„æ´»ç”¨  
âœ… æ™‚ç³»åˆ—ãƒ‘ã‚¿ãƒ¼ãƒ³ã®è©³ç´°ãªå­¦ç¿’  
âœ… äºˆæ¸¬ã®ä¿¡é ¼æ€§å‘ä¸Šï¼ˆè¤‡æ•°æœŸé–“ã§æ¤œè¨¼ï¼‰

### ãƒ‡ãƒ¡ãƒªãƒƒãƒˆãƒ»æ³¨æ„ç‚¹

âš ï¸ å®Ÿè£…ã®è¤‡é›‘æ€§ãŒå¢—åŠ   
âš ï¸ è¨“ç·´æ™‚é–“ã®å¢—åŠ ï¼ˆã‚µãƒ³ãƒ—ãƒ«æ•°å¢—åŠ ï¼‰  
âš ï¸ ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã®å¢—åŠ ï¼ˆãƒãƒ«ãƒãƒ˜ãƒƒãƒ‰ï¼‰  
âš ï¸ ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿èª¿æ•´ã®é›£åº¦ä¸Šæ˜‡

---

## ğŸ“š å‚è€ƒè³‡æ–™

- ç¾åœ¨ã®å®Ÿè£…: `scripts/training/irl/train_temporal_irl_sliding_window.py`
- IRL ã‚·ã‚¹ãƒ†ãƒ : `src/gerrit_retention/rl_prediction/retention_irl_system.py`
- ç¾åœ¨ã®è¨­è¨ˆ: `README_TEMPORAL_IRL.md`

---

**ä½œæˆæ—¥**: 2025-10-21  
**ãƒãƒ¼ã‚¸ãƒ§ãƒ³**: 1.0  
**ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹**: ææ¡ˆ
