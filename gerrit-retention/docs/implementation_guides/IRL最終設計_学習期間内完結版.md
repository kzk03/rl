# IRL æœ€çµ‚è¨­è¨ˆ: å­¦ç¿’æœŸé–“å†…å®Œçµç‰ˆ

## ğŸ¯ æ ¸å¿ƒçš„ãªãƒã‚¤ãƒ³ãƒˆ

### âœ… ã‚„ã‚ŠãŸã„ã“ã¨

**ç¶šã„ãŸäººã¨ç¶šã‹ãªã‹ã£ãŸäººã‚’åŒºåˆ¥ã™ã‚‹å ±é…¬é–¢æ•°ã‚’å­¦ç¿’**

- LSTM ã§éå»ã®æ™‚ç³»åˆ—ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’å­¦ç¿’
- å°†æ¥ã®è²¢çŒ®ï¼ˆ1m å¾Œãªã©ï¼‰ã‚’ãƒ©ãƒ™ãƒ«ã¨ã—ã¦ä½¿ç”¨
- å­¦ç¿’æœŸé–“å†…ã§ã™ã¹ã¦å®Œçµ
- è©•ä¾¡æœŸé–“ã¨ã¯ cutoff ã§åˆ†é›¢

---

## ğŸ“ è¨­è¨ˆã®å…¨ä½“åƒ

### ã‚¿ã‚¤ãƒ ãƒ©ã‚¤ãƒ³

```
å­¦ç¿’æœŸé–“: 2019-01-01 ï½ 2020-01-01 (12ãƒ¶æœˆ)
|------------------------------------------------|
[train_start]                          [train_end]

ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°å¯èƒ½ç¯„å›²ï¼ˆfuture_window=0-1m ã®å ´åˆï¼‰:
|-----------------------------------|
     â†‘                           â†‘
  2019-07-01                  2019-12-01
  (æœ€åˆ)                       (æœ€å¾Œ)
                                  â†“
                                  +-- å°†æ¥çª“ (1m)
                                  [2019-12-01 ï½ 2020-01-01]
                                                   â†‘
                                                   å­¦ç¿’æœŸé–“å†…ã§å®Œçµï¼

è©•ä¾¡æœŸé–“: 2020-01-01 ï½ 2021-01-01
|------------------------------------------------|
     â†‘
     cutoff ã§å®Œå…¨åˆ†é›¢
```

### é‡è¦ãªåˆ¶ç´„

```python
# ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°æ™‚ç‚¹ã®ç¯„å›²
min_sampling_date = train_start + history_window_months
max_sampling_date = train_end - future_window_end_months  # â† ã“ã‚ŒãŒé‡è¦ï¼

# ä¾‹: train_end = 2020-01-01, future_window = 1m
max_sampling_date = 2020-01-01 - 1m = 2019-12-01

# æœ€å¾Œã®ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°æ™‚ç‚¹ã§ã®å°†æ¥çª“
future_end = 2019-12-01 + 1m = 2020-01-01  # â† train_end ã«ä¸€è‡´
```

---

## ğŸ”§ å…·ä½“çš„ãªè¨­è¨ˆ

### 1. ãƒ‡ãƒ¼ã‚¿æ§‹é€ 

```python
trajectory = {
    'developer': {...},
    'activity_history': [...],  # éå»ã®æ´»å‹•å±¥æ­´ï¼ˆLSTMå…¥åŠ›ï¼‰
    'context_date': sampling_point,

    # å°†æ¥ã®è²¢çŒ®ã¯ã€Œãƒ©ãƒ™ãƒ«ã€ã¨ã—ã¦æ ¼ç´ï¼ˆçŠ¶æ…‹ç‰¹å¾´é‡ã§ã¯ãªã„ï¼‰
    'future_contribution': True,  # â† å­¦ç¿’ã®ã‚¿ãƒ¼ã‚²ãƒƒãƒˆ

    # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿
    'future_window': {
        'start_months': 0,
        'end_months': 1,
    },
}
```

**é‡è¦ãªé•ã„:**

- âŒ å°†æ¥ã®è²¢çŒ®ã‚’çŠ¶æ…‹ç‰¹å¾´é‡ã«å«ã‚ã‚‹ â†’ å› æœé–¢ä¿‚ãŒé€†è»¢
- âœ… å°†æ¥ã®è²¢çŒ®ã‚’å­¦ç¿’ã®ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã«ã™ã‚‹ â†’ æ­£ã—ã„å› æœé–¢ä¿‚

### 2. ãƒ‡ãƒ¼ã‚¿æŠ½å‡ºï¼ˆå­¦ç¿’æœŸé–“å†…ã§å®Œçµï¼‰

```python
def extract_temporal_trajectories_within_training_period(
    df: pd.DataFrame,
    train_start: pd.Timestamp,
    train_end: pd.Timestamp,
    history_window_months: int = 6,
    future_window_start_months: int = 0,  # parserã§æŒ‡å®š
    future_window_end_months: int = 1,    # parserã§æŒ‡å®š
    sampling_interval_months: int = 1
):
    """
    å­¦ç¿’æœŸé–“å†…ã§å®Œçµã™ã‚‹è»Œè·¡ã‚’æŠ½å‡º

    é‡è¦:
    - å°†æ¥ã®è²¢çŒ®ã¯ã€Œãƒ©ãƒ™ãƒ«ã€ã¨ã—ã¦ä½¿ç”¨ï¼ˆçŠ¶æ…‹ã«ã¯å«ã‚ãªã„ï¼‰
    - ã™ã¹ã¦ã®ãƒ‡ãƒ¼ã‚¿ãŒå­¦ç¿’æœŸé–“å†…ã§å®Œçµ
    """
    trajectories = []

    # ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°å¯èƒ½ç¯„å›²ã‚’è¨ˆç®—
    min_sampling_date = train_start + pd.DateOffset(months=history_window_months)
    max_sampling_date = train_end - pd.DateOffset(months=future_window_end_months)
    # â†‘ ã“ã‚Œã«ã‚ˆã‚Šå­¦ç¿’æœŸé–“å†…ã§å®Œçµ

    # ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°æ™‚ç‚¹ã‚’ç”Ÿæˆ
    sampling_points = []
    current = min_sampling_date

    while current <= max_sampling_date:
        sampling_points.append(current)
        current += pd.DateOffset(months=sampling_interval_months)

    logger.info(f"ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°å¯èƒ½ç¯„å›²: {min_sampling_date} ï½ {max_sampling_date}")
    logger.info(f"ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°æ™‚ç‚¹æ•°: {len(sampling_points)}")

    for sampling_point in sampling_points:
        for reviewer in df['reviewer_email'].unique():
            # éå»ã®å±¥æ­´ï¼ˆLSTMå…¥åŠ›ï¼‰
            history_start = sampling_point - pd.DateOffset(months=history_window_months)
            history_end = sampling_point

            history_df = df[
                (df['reviewer_email'] == reviewer) &
                (df['request_time'] >= history_start) &
                (df['request_time'] < history_end)
            ]

            if len(history_df) < 3:  # æœ€å°ã‚¤ãƒ™ãƒ³ãƒˆæ•°
                continue

            # æ´»å‹•å±¥æ­´ã‚’æ§‹ç¯‰
            activity_history = []
            for _, row in history_df.iterrows():
                activity = {
                    'timestamp': row['request_time'],
                    'action_type': 'review',
                    'project': row.get('project', 'unknown'),
                }
                activity_history.append(activity)

            # å°†æ¥ã®è²¢çŒ®ã‚’è¨ˆç®—ï¼ˆãƒ©ãƒ™ãƒ«ã¨ã—ã¦ä½¿ç”¨ï¼‰
            future_start = sampling_point + pd.DateOffset(months=future_window_start_months)
            future_end = sampling_point + pd.DateOffset(months=future_window_end_months)

            # å­¦ç¿’æœŸé–“å†…ã§å®Œçµã—ã¦ã„ã‚‹ã“ã¨ã‚’ç¢ºèª
            assert future_end <= train_end, \
                f"å°†æ¥çª“ãŒå­¦ç¿’æœŸé–“ã‚’è¶…ãˆã¦ã„ã¾ã™: {future_end} > {train_end}"

            future_df = df[
                (df['reviewer_email'] == reviewer) &
                (df['request_time'] >= future_start) &
                (df['request_time'] < future_end)
            ]

            future_contribution = len(future_df) > 0

            # é–‹ç™ºè€…æƒ…å ±
            developer_info = {
                'developer_id': reviewer,
                'first_seen': history_df['request_time'].min(),
                'changes_reviewed': len(history_df),
            }

            # è»Œè·¡ã‚’ä½œæˆ
            trajectory = {
                'developer': developer_info,
                'activity_history': activity_history,
                'context_date': sampling_point,

                # å°†æ¥ã®è²¢çŒ®ã¯ãƒ©ãƒ™ãƒ«ã¨ã—ã¦æ ¼ç´
                'future_contribution': future_contribution,

                # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿
                'future_window': {
                    'start_months': future_window_start_months,
                    'end_months': future_window_end_months,
                    'start_date': future_start,
                    'end_date': future_end,
                },
                'history_window': {
                    'start_date': history_start,
                    'end_date': history_end,
                },
            }

            trajectories.append(trajectory)

    # çµ±è¨ˆæƒ…å ±
    positive_count = sum(1 for t in trajectories if t['future_contribution'])
    positive_rate = positive_count / len(trajectories) if trajectories else 0

    logger.info(f"è»Œè·¡æŠ½å‡ºå®Œäº†: {len(trajectories)}ã‚µãƒ³ãƒ—ãƒ«")
    logger.info(f"  ç¶™ç¶šç‡: {positive_rate:.1%} ({positive_count}/{len(trajectories)})")
    logger.info(f"  ã™ã¹ã¦ã®ãƒ‡ãƒ¼ã‚¿ãŒå­¦ç¿’æœŸé–“å†…ã§å®Œçµ: âœ…")

    return trajectories
```

### 3. çŠ¶æ…‹æŠ½å‡ºï¼ˆå°†æ¥ã®æƒ…å ±ã‚’å«ã‚ãªã„ï¼‰

```python
def extract_developer_state(self, trajectory):
    """
    çŠ¶æ…‹ãƒ™ã‚¯ãƒˆãƒ«ã‚’æŠ½å‡ºï¼ˆå°†æ¥ã®æƒ…å ±ã¯å«ã‚ãªã„ï¼‰

    é‡è¦: å°†æ¥ã®è²¢çŒ®ã¯çŠ¶æ…‹ç‰¹å¾´é‡ã«ã¯å«ã‚ãšã€
    å­¦ç¿’ã®ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã¨ã—ã¦ä½¿ç”¨ã™ã‚‹
    """
    # æ—¢å­˜ã®æ‹¡å¼µIRLå®Ÿè£…ã§éå»ãƒ™ãƒ¼ã‚¹ã®ç‰¹å¾´é‡ã‚’å–å¾—
    from gerrit_retention.rl_prediction.enhanced_retention_irl_system import (
        EnhancedRetentionIRLSystem
    )

    developer = trajectory['developer']
    activity_history = trajectory['activity_history']
    context_date = trajectory['context_date']

    # æ—¢å­˜å®Ÿè£…ã‚’ä½¿ç”¨ï¼ˆéå»ã®æƒ…å ±ã®ã¿ï¼‰
    state_features = EnhancedRetentionIRLSystem.extract_developer_state(
        developer, activity_history, context_date
    )  # Næ¬¡å…ƒï¼ˆæ—¢å­˜ã®æ‹¡å¼µIRLã®æ¬¡å…ƒï¼‰

    # ã“ã“ã§ future_contribution ã¯è¿½åŠ ã—ãªã„ï¼
    # â†’ å°†æ¥ã®æƒ…å ±ã‚’çŠ¶æ…‹ã«å«ã‚ãªã„

    return state_features
```

### 4. è¨“ç·´ï¼ˆç¶šã„ãŸäººã¨ç¶šã‹ãªã‹ã£ãŸäººã‚’åŒºåˆ¥ï¼‰

```python
def train_irl_to_distinguish_continued(
    self,
    trajectories: List[Dict[str, Any]],
    epochs: int = 30
) -> Dict[str, Any]:
    """
    ç¶šã„ãŸäººã¨ç¶šã‹ãªã‹ã£ãŸäººã‚’åŒºåˆ¥ã™ã‚‹å ±é…¬é–¢æ•°ã‚’å­¦ç¿’

    é‡è¦:
    - çŠ¶æ…‹ã«ã¯å°†æ¥ã®æƒ…å ±ã‚’å«ã‚ãªã„
    - å°†æ¥ã®è²¢çŒ®ã‚’ã‚¿ãƒ¼ã‚²ãƒƒãƒˆï¼ˆãƒ©ãƒ™ãƒ«ï¼‰ã¨ã—ã¦ä½¿ç”¨
    - LSTMã§æ™‚ç³»åˆ—ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’å­¦ç¿’
    """
    logger.info(f"IRLè¨“ç·´é–‹å§‹: {len(trajectories)}è»Œè·¡, {epochs}ã‚¨ãƒãƒƒã‚¯")
    logger.info("ç›®æ¨™: ç¶šã„ãŸäººã¨ç¶šã‹ãªã‹ã£ãŸäººã‚’åŒºåˆ¥ã™ã‚‹å ±é…¬é–¢æ•°ã‚’å­¦ç¿’")

    training_losses = []

    # ç¶™ç¶šç‡ã‚’ç¢ºèª
    positive_count = sum(1 for t in trajectories if t['future_contribution'])
    positive_rate = positive_count / len(trajectories)
    logger.info(f"ç¶™ç¶šç‡: {positive_rate:.1%}")

    for epoch in range(epochs):
        epoch_loss = 0.0
        batch_count = 0

        for trajectory in trajectories:
            try:
                # çŠ¶æ…‹ã‚’æŠ½å‡ºï¼ˆå°†æ¥ã®æƒ…å ±ã¯å«ã¾ãªã„ï¼‰
                state = self.extract_developer_state(trajectory)
                actions = self.extract_developer_actions(
                    trajectory['activity_history'],
                    trajectory['context_date']
                )

                if not actions:
                    continue

                # æ™‚ç³»åˆ—å‡¦ç†ï¼ˆLSTMï¼‰
                if self.sequence:
                    # ã‚·ãƒ¼ã‚±ãƒ³ã‚¹æ§‹ç¯‰
                    if len(actions) < self.seq_len:
                        padded_actions = [actions[0]] * (self.seq_len - len(actions)) + actions
                    else:
                        padded_actions = actions[-self.seq_len:]

                    state_seq = torch.stack([
                        self.state_to_tensor(state) for _ in range(self.seq_len)
                    ]).unsqueeze(0)
                    action_seq = torch.stack([
                        self.action_to_tensor(action) for action in padded_actions
                    ]).unsqueeze(0)

                    # LSTMã§æ™‚ç³»åˆ—ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’å­¦ç¿’
                    predicted_reward = self.network(state_seq, action_seq)
                else:
                    state_tensor = self.state_to_tensor(state).unsqueeze(0)
                    action_tensor = self.action_to_tensor(actions[-1]).unsqueeze(0)
                    predicted_reward = self.network(state_tensor, action_tensor)

                # å°†æ¥ã®è²¢çŒ®ã‹ã‚‰ç›®æ¨™å ±é…¬ã‚’ç”Ÿæˆ
                future_contribution = trajectory['future_contribution']

                # ç¶šã„ãŸäºº â†’ å ±é…¬ 1.0
                # ç¶šã‹ãªã‹ã£ãŸäºº â†’ å ±é…¬ 0.0
                target_reward = torch.tensor(
                    [[1.0 if future_contribution else 0.0]],
                    device=self.device
                )

                # æå¤±è¨ˆç®—
                loss = F.mse_loss(predicted_reward, target_reward)

                # é€†ä¼æ’­
                self.optimizer.zero_grad()
                loss.backward()
                self.optimizer.step()

                epoch_loss += loss.item()
                batch_count += 1

            except Exception as e:
                logger.warning(f"è»Œè·¡å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
                continue

        if batch_count > 0:
            epoch_loss /= batch_count
            training_losses.append(epoch_loss)

            if (epoch + 1) % 10 == 0:
                logger.info(f"Epoch {epoch+1}/{epochs}: æå¤±={epoch_loss:.4f}")

    return {
        'losses': training_losses,
        'final_loss': training_losses[-1] if training_losses else 0
    }
```

### 5. è©•ä¾¡ï¼ˆæ™‚ç³»åˆ— Cutoff ã§åˆ†é›¢ï¼‰

```python
# è¨“ç·´ãƒ‡ãƒ¼ã‚¿ï¼ˆå­¦ç¿’æœŸé–“å†…ã§å®Œçµï¼‰
train_trajectories = extract_temporal_trajectories_within_training_period(
    df=df,
    train_start=pd.Timestamp('2019-01-01'),
    train_end=pd.Timestamp('2020-01-01'),
    history_window_months=6,
    future_window_start_months=0,
    future_window_end_months=1,
)

# è©•ä¾¡ãƒ‡ãƒ¼ã‚¿ï¼ˆè©•ä¾¡æœŸé–“ã§æŠ½å‡ºï¼‰
eval_trajectories = extract_temporal_trajectories_within_training_period(
    df=df,
    train_start=pd.Timestamp('2020-01-01'),  # cutoff
    train_end=pd.Timestamp('2021-01-01'),
    history_window_months=6,
    future_window_start_months=0,
    future_window_end_months=1,
)

# ãƒ‡ãƒ¼ã‚¿ãƒªãƒ¼ã‚¯ãªã— âœ…
```

---

## ğŸ“ é‡è¦ãªç†è«–çš„ãƒã‚¤ãƒ³ãƒˆ

### 1. LSTM ã§æ™‚ç³»åˆ—ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’å­¦ç¿’

```python
# éå»ã®æ´»å‹•å±¥æ­´ï¼ˆæ™‚ç³»åˆ—ï¼‰
activity_history = [
    action_t0,   # 6ãƒ¶æœˆå‰
    action_t1,   # 5.5ãƒ¶æœˆå‰
    ...
    action_t15,  # ç¾åœ¨
]

# LSTMã§å‡¦ç†
state_seq = LSTM(activity_history)
# â†‘ æ™‚ç³»åˆ—çš„ãªå¤‰åŒ–ã‚’æ‰ãˆã‚‰ã‚Œã‚‹

# ç¶šã„ãŸäººã¨ç¶šã‹ãªã‹ã£ãŸäººã®é•ã„ã‚’å­¦ç¿’
# - ç¶šã„ãŸäºº: æ´»å‹•ãŒå¢—åŠ å‚¾å‘ã€è¦å‰‡çš„ãªãƒ‘ã‚¿ãƒ¼ãƒ³
# - ç¶šã‹ãªã‹ã£ãŸäºº: æ´»å‹•ãŒæ¸›å°‘å‚¾å‘ã€ä¸è¦å‰‡ãªãƒ‘ã‚¿ãƒ¼ãƒ³
```

### 2. å› æœé–¢ä¿‚ãŒæ­£ã—ã„

```python
# âŒ é–“é•ã„: å°†æ¥ã‚’è¦‹ã¦éå»ã‚’è©•ä¾¡
state = [*past_features, future_contribution]  # å°†æ¥ã®æƒ…å ±ã‚’å«ã‚€
reward = model(state)

# âœ… æ­£ã—ã„: éå»ã‹ã‚‰å°†æ¥ã‚’äºˆæ¸¬
state = [*past_features]  # éå»ã®æƒ…å ±ã®ã¿
reward = model(state)
target = future_contribution  # å°†æ¥ã®è²¢çŒ®ã‚’ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã«
loss = MSE(reward, target)
```

### 3. å­¦ç¿’æœŸé–“å†…ã§å®Œçµ

```python
# ã™ã¹ã¦ã®ã‚µãƒ³ãƒ—ãƒ«ã«ã¤ã„ã¦
for trajectory in trajectories:
    history_start >= train_start  # âœ…
    history_end <= train_end      # âœ…
    future_start >= train_start   # âœ…
    future_end <= train_end       # âœ… å­¦ç¿’æœŸé–“å†…
```

### 4. ç¶šã„ãŸäººã¨ç¶šã‹ãªã‹ã£ãŸäººã‚’åŒºåˆ¥

```python
# ç¶šã„ãŸäººã®è»Œè·¡ â†’ é«˜ã„å ±é…¬ã‚’å­¦ç¿’
if future_contribution:
    target_reward = 1.0
    # ãƒ¢ãƒ‡ãƒ«ã¯ã€Œç¶šãäººã®éå»ãƒ‘ã‚¿ãƒ¼ãƒ³ã€ã‚’é«˜å ±é…¬ã¨å­¦ç¿’

# ç¶šã‹ãªã‹ã£ãŸäººã®è»Œè·¡ â†’ ä½ã„å ±é…¬ã‚’å­¦ç¿’
else:
    target_reward = 0.0
    # ãƒ¢ãƒ‡ãƒ«ã¯ã€Œç¶šã‹ãªã„äººã®éå»ãƒ‘ã‚¿ãƒ¼ãƒ³ã€ã‚’ä½å ±é…¬ã¨å­¦ç¿’
```

---

## ğŸ“ è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ä¾‹

```yaml
# configs/irl_within_training_period.yaml

irl_training:
  # å­¦ç¿’æœŸé–“
  training_period:
    start_date: "2019-01-01"
    end_date: "2020-01-01"

  # è©•ä¾¡æœŸé–“ï¼ˆcutoffã§åˆ†é›¢ï¼‰
  evaluation_period:
    start_date: "2020-01-01"
    end_date: "2021-01-01"

  # ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°è¨­å®š
  sampling:
    interval_months: 1
    history_window_months: 6
    min_history_events: 3

  # å°†æ¥çª“ï¼ˆparserã§æŒ‡å®šå¯èƒ½ï¼‰
  future_window:
    start_months: 0 # 0ãƒ¶æœˆå¾Œã‹ã‚‰
    end_months: 1 # 1ãƒ¶æœˆå¾Œã¾ã§
    description: "å°†æ¥ã®è²¢çŒ®ã‚’è¦‹ã‚‹æœŸé–“ï¼ˆå­¦ç¿’ã®ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã€çŠ¶æ…‹ã«ã¯å«ã‚ãªã„ï¼‰"

  # ãƒ¢ãƒ‡ãƒ«è¨­å®š
  model:
    use_enhanced_irl_features: true
    state_dim: auto # æ‹¡å¼µIRLæ¬¡å…ƒï¼ˆå°†æ¥ã®æƒ…å ±ã¯å«ã¾ãªã„ï¼‰
    action_dim: 5
    hidden_dim: 64
    lstm_hidden: 128
    sequence: true
    seq_len: 15

  # è¨“ç·´è¨­å®š
  training:
    epochs: 30
    learning_rate: 0.001
```

---

## ğŸš€ å®Ÿè¡Œä¾‹

### ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å¼•æ•°

```bash
# åŸºæœ¬çš„ãªå®Ÿè¡Œ
python scripts/training/irl/train_irl_within_training_period.py \
  --reviews data/review_requests_openstack_multi_5y_detail.csv \
  --train-start 2019-01-01 \
  --train-end 2020-01-01 \
  --eval-start 2020-01-01 \
  --eval-end 2021-01-01 \
  --future-window-start 0 \
  --future-window-end 1 \
  --history-window 6 \
  --epochs 30 \
  --output outputs/irl_0_1m

# å®Ÿé¨“: å°†æ¥çª“ã‚’å¤‰æ›´
python scripts/training/irl/train_irl_within_training_period.py \
  --reviews data/review_requests_openstack_multi_5y_detail.csv \
  --train-start 2019-01-01 \
  --train-end 2020-01-01 \
  --eval-start 2020-01-01 \
  --eval-end 2021-01-01 \
  --future-window-start 1 \
  --future-window-end 3 \
  --history-window 6 \
  --epochs 30 \
  --output outputs/irl_1_3m
```

### æœŸå¾…ã•ã‚Œã‚‹å‡ºåŠ›

```
INFO: ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°å¯èƒ½ç¯„å›²: 2019-07-01 ï½ 2019-12-01
INFO: ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°æ™‚ç‚¹æ•°: 6
INFO: è»Œè·¡æŠ½å‡ºå®Œäº†: 5432ã‚µãƒ³ãƒ—ãƒ«
INFO:   ç¶™ç¶šç‡: 15.3% (831/5432)
INFO:   ã™ã¹ã¦ã®ãƒ‡ãƒ¼ã‚¿ãŒå­¦ç¿’æœŸé–“å†…ã§å®Œçµ: âœ…
INFO: IRLè¨“ç·´é–‹å§‹: 5432è»Œè·¡, 30ã‚¨ãƒãƒƒã‚¯
INFO: ç›®æ¨™: ç¶šã„ãŸäººã¨ç¶šã‹ãªã‹ã£ãŸäººã‚’åŒºåˆ¥ã™ã‚‹å ±é…¬é–¢æ•°ã‚’å­¦ç¿’
INFO: Epoch 10/30: æå¤±=0.1234
INFO: Epoch 20/30: æå¤±=0.0987
INFO: Epoch 30/30: æå¤±=0.0823
INFO: è¨“ç·´å®Œäº†
INFO: è©•ä¾¡é–‹å§‹...
INFO: AUC-ROC: 0.852
INFO: AUC-PR: 0.789
INFO: F1: 0.654
```

---

## âœ… ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ

å®Ÿè£…ã™ã‚‹éš›ã®ç¢ºèªäº‹é …ï¼š

- [ ] å°†æ¥ã®è²¢çŒ®ã‚’çŠ¶æ…‹ç‰¹å¾´é‡ã«å«ã‚ã¦ã„ãªã„
- [ ] å°†æ¥ã®è²¢çŒ®ã‚’å­¦ç¿’ã®ã‚¿ãƒ¼ã‚²ãƒƒãƒˆï¼ˆãƒ©ãƒ™ãƒ«ï¼‰ã¨ã—ã¦ä½¿ç”¨
- [ ] ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°æ™‚ç‚¹ã®ç¯„å›²ãŒæ­£ã—ãåˆ¶é™ã•ã‚Œã¦ã„ã‚‹
- [ ] ã™ã¹ã¦ã®ãƒ‡ãƒ¼ã‚¿ãŒå­¦ç¿’æœŸé–“å†…ã§å®Œçµ
- [ ] è¨“ç·´æœŸé–“ã¨è©•ä¾¡æœŸé–“ãŒ cutoff ã§åˆ†é›¢
- [ ] LSTM ã§æ™‚ç³»åˆ—ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’å‡¦ç†
- [ ] ç¶šã„ãŸäºº â†’ é«˜å ±é…¬ã€ç¶šã‹ãªã‹ã£ãŸäºº â†’ ä½å ±é…¬ ã‚’å­¦ç¿’

---

## ğŸ¯ ã¾ã¨ã‚

### ã“ã®è¨­è¨ˆãŒã§ãã‚‹ã“ã¨

âœ… **ç¶šã„ãŸäººã¨ç¶šã‹ãªã‹ã£ãŸäººã®éå»ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’åŒºåˆ¥**

- LSTM ã§æ™‚ç³»åˆ—çš„ãªå¤‰åŒ–ã‚’å­¦ç¿’
- ç¶šã„ãŸäºº: é«˜ã„å ±é…¬ã‚’å­¦ç¿’
- ç¶šã‹ãªã‹ã£ãŸäºº: ä½ã„å ±é…¬ã‚’å­¦ç¿’

âœ… **å­¦ç¿’æœŸé–“å†…ã§å®Œçµ**

- ã™ã¹ã¦ã®ãƒ‡ãƒ¼ã‚¿ãŒå­¦ç¿’æœŸé–“å†…
- è©•ä¾¡æœŸé–“ã¨ã¯ cutoff ã§å®Œå…¨åˆ†é›¢
- ãƒ‡ãƒ¼ã‚¿ãƒªãƒ¼ã‚¯ãªã—

âœ… **å› æœé–¢ä¿‚ãŒæ­£ã—ã„**

- éå»ã®æƒ…å ±ã®ã¿ã‹ã‚‰å°†æ¥ã‚’äºˆæ¸¬
- å°†æ¥ã®æƒ…å ±ã¯çŠ¶æ…‹ã«å«ã‚ãªã„

âœ… **å®Ÿé¨“ã®è‡ªç”±åº¦**

- parser ã§å°†æ¥çª“ã‚’æŒ‡å®š
- çŸ­æœŸãƒ»ä¸­æœŸãƒ»é•·æœŸã®äºˆæ¸¬ã‚’æ¯”è¼ƒå¯èƒ½

### ã§ããªã„ã“ã¨

âŒ è¤‡æ•°ã®å°†æ¥çª“ã‚’åŒæ™‚ã«å­¦ç¿’

- ç¾åœ¨ã¯ 1 ã¤ã®å°†æ¥çª“ã®ã¿
- è¤‡æ•°ã®æœŸé–“ã‚’è¦‹ãŸã„å ´åˆã¯åˆ¥ã€…ã«å®Ÿé¨“

âŒ æ¨è«–æ™‚ã«å°†æ¥ã®è²¢çŒ®ã‚’äºˆæ¸¬

- è¨“ç·´æ™‚ã¯å°†æ¥ã®è²¢çŒ®ã‚’ãƒ©ãƒ™ãƒ«ã¨ã—ã¦ä½¿ç”¨
- æ¨è«–æ™‚ã¯éå»ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã‹ã‚‰å ±é…¬ã‚’è¨ˆç®—ã®ã¿

---

**ä½œæˆæ—¥:** 2025-10-21  
**ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹:** æœ€çµ‚ç‰ˆ  
**é‡è¦ãªå¤‰æ›´:** å°†æ¥ã®è²¢çŒ®ã‚’çŠ¶æ…‹ç‰¹å¾´é‡ã§ã¯ãªãã€å­¦ç¿’ã®ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã¨ã—ã¦ä½¿ç”¨
