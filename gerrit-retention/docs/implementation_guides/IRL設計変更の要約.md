# IRL è¨­è¨ˆå¤‰æ›´ã®è¦ç´„

## ğŸ¯ æ ¸å¿ƒçš„ãªå¤‰æ›´ç‚¹

### å¾“æ¥ã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒï¼ˆæ•™å¸«ã‚ã‚Šå­¦ç¿’çš„ IRLï¼‰

```python
# ãƒ‡ãƒ¼ã‚¿æ§‹é€ 
trajectory = {
    'developer': {...},
    'activity_history': [...],  # éå»ã®å±¥æ­´ï¼ˆç‰¹å¾´é‡ï¼‰
    'continued': True,  # â† æ­£è§£ãƒ©ãƒ™ãƒ«ï¼ˆæ•™å¸«ä¿¡å·ï¼‰
}

# è¨“ç·´
predicted_prob = model(state, action)
loss = BCE(predicted_prob, trajectory['continued'])  # æ­£è§£ãƒ©ãƒ™ãƒ«ã‚’ä½¿ç”¨
```

**å•é¡Œç‚¹:**

- æ­£è§£ãƒ©ãƒ™ãƒ«ã‚’æ˜ç¤ºçš„ã«ä½¿ç”¨ â†’ æ•™å¸«ã‚ã‚Šå­¦ç¿’ã«è¿‘ã„
- å­¦ç¿’æœŸé–“å†…ã®æ™‚é–“çš„å¤‰åŒ–ã‚’æ´»ç”¨ã§ããªã„
- ãƒ‡ãƒ¼ã‚¿åŠ¹ç‡ãŒæ‚ªã„ï¼ˆå­¦ç¿’æœŸé–“ 12 ãƒ¶æœˆ â†’ 1 ã‚µãƒ³ãƒ—ãƒ«ï¼‰

---

### æ–°ã—ã„ã‚¢ãƒ—ãƒ­ãƒ¼ãƒï¼ˆç‰¹å¾´é‡ãƒ™ãƒ¼ã‚¹ IRLï¼‰

```python
# ãƒ‡ãƒ¼ã‚¿æ§‹é€ 
trajectory = {
    'developer': {...},
    'activity_history': [...],  # éå»ã®å±¥æ­´

    # å°†æ¥ã®è²¢çŒ®ã‚’ã€ŒçŠ¶æ…‹ç‰¹å¾´é‡ã€ã¨ã—ã¦å«ã‚ã‚‹ï¼ˆ1æ¬¡å…ƒã®ã¿ï¼‰
    'future_contribution': True,   # â† ç‰¹å¾´é‡ï¼ˆæ­£è§£ãƒ©ãƒ™ãƒ«ã§ã¯ãªã„ï¼‰

    # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ï¼ˆã©ã®æœŸé–“ã‚’è¦‹ã¦ã„ã‚‹ã‹ï¼‰
    'future_window': {
        'start_months': 0,  # 0ãƒ¶æœˆå¾Œã‹ã‚‰
        'end_months': 1,    # 1ãƒ¶æœˆå¾Œã¾ã§
    },
    # æ­£è§£ãƒ©ãƒ™ãƒ«ã¯å«ã¾ãªã„
}

# è¨“ç·´
# çŠ¶æ…‹ã®ç‰¹å¾´é‡ã«å°†æ¥ã®è²¢çŒ®ãŒå«ã¾ã‚Œã¦ã„ã‚‹ï¼ˆ1æ¬¡å…ƒã®ã¿ï¼‰
state_features = [
    *past_features,  # éå»ã®å±¥æ­´ã‹ã‚‰è¨ˆç®—ï¼ˆæ‹¡å¼µIRLç‰¹å¾´ï¼‰
    1.0,  # future_contribution (ç‰¹å¾´é‡ã¨ã—ã¦çµ„ã¿è¾¼ã¿ã€1æ¬¡å…ƒã®ã¿)
]

reward = model(state_features, action)

# å°†æ¥ã®è²¢çŒ®ã‹ã‚‰ç›®æ¨™å ±é…¬ã‚’è‡ªå‹•ç”Ÿæˆï¼ˆ1æ¬¡å…ƒãªã®ã§ã‚·ãƒ³ãƒ—ãƒ«ï¼‰
future_contribution = trajectory['future_contribution']
target_reward = 1.0 if future_contribution else 0.0

loss = MSE(reward, target_reward)  # æ­£è§£ãƒ©ãƒ™ãƒ«ã¯ä½¿ã‚ãªã„
```

**æ”¹å–„ç‚¹:**

- âœ… æ­£è§£ãƒ©ãƒ™ãƒ«ã‚’ä½¿ã‚ãªã„ â†’ ã‚ˆã‚Šç´”ç²‹ãª IRL
- âœ… å°†æ¥ã®è²¢çŒ®ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’çŠ¶æ…‹ç‰¹å¾´é‡ã«çµ„ã¿è¾¼ã‚€
- âœ… å­¦ç¿’æœŸé–“å†…ã®è¤‡æ•°æ™‚ç‚¹ã‹ã‚‰ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚° â†’ ãƒ‡ãƒ¼ã‚¿åŠ¹ç‡å‘ä¸Š
- âœ… æŸ”è»Ÿãªå­¦ç¿’æ–¹æ³•ï¼ˆäºŒå€¤åŒ–ã€é€£ç¶šå€¤ã€ã‚³ãƒ³ãƒˆãƒ©ã‚¹ãƒˆå­¦ç¿’ãªã©ï¼‰

---

## ğŸ“Š å…·ä½“ä¾‹ã§ç†è§£ã™ã‚‹

### ã‚¿ã‚¤ãƒ ãƒ©ã‚¤ãƒ³

```
å­¦ç¿’æœŸé–“: 2019-01-01 ï½ 2020-01-01 (12ãƒ¶æœˆ)

ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°æ™‚ç‚¹:
  t1 (2019-01-01)  t2 (2019-02-01)  ...  t12 (2019-12-01)
  |                |                      |
  +-- éå»6ãƒ¶æœˆã®å±¥æ­´                      +-- éå»6ãƒ¶æœˆã®å±¥æ­´
  +-- å°†æ¥ã®è²¢çŒ®:                           +-- å°†æ¥ã®è²¢çŒ®:
      1må¾Œ: âœ“ (ç‰¹å¾´é‡)                         1må¾Œ: âœ— (ç‰¹å¾´é‡)
      2må¾Œ: âœ“ (ç‰¹å¾´é‡)                         2må¾Œ: âœ“ (ç‰¹å¾´é‡)
      3må¾Œ: âœ— (ç‰¹å¾´é‡)                         3må¾Œ: âœ— (ç‰¹å¾´é‡)
```

### ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆã®é•ã„

**å¾“æ¥:**

```python
# ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆæ—¥1ç‚¹ã®ã¿
snapshot_date = 2020-01-01

trajectory = {
    'activity_history': [éå»6ãƒ¶æœˆã®å±¥æ­´],
    'continued': has_activity_at(2020-07-01),  # 6ãƒ¶æœˆå¾Œã®è²¢çŒ®ï¼ˆæ­£è§£ãƒ©ãƒ™ãƒ«ï¼‰
}

# â†’ 12ãƒ¶æœˆã®å­¦ç¿’æœŸé–“ã‹ã‚‰1ã‚µãƒ³ãƒ—ãƒ«ã®ã¿
```

**æ–°è¨­è¨ˆ:**

```python
# å­¦ç¿’æœŸé–“å†…ã‚’1ãƒ¶æœˆé–“éš”ã§ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
trajectories = []

for t in [2019-01-01, 2019-02-01, ..., 2019-12-01]:  # 12æ™‚ç‚¹
    trajectory = {
        'activity_history': [t-6ãƒ¶æœˆ ï½ t ã®å±¥æ­´],
        'temporal_contribution_features': {
            'contrib_1m': has_activity(t, t+1m),  # ç‰¹å¾´é‡
            'contrib_2m': has_activity(t+1m, t+2m),  # ç‰¹å¾´é‡
            'contrib_3m': has_activity(t+2m, t+3m),  # ç‰¹å¾´é‡
        },
    }
    trajectories.append(trajectory)

# â†’ 12ãƒ¶æœˆã®å­¦ç¿’æœŸé–“ã‹ã‚‰12ã‚µãƒ³ãƒ—ãƒ«ç”Ÿæˆ
```

---

## ğŸ”§ å®Ÿè£…ã®ä¸»è¦å¤‰æ›´ç®‡æ‰€

### 1. ãƒ‡ãƒ¼ã‚¿æŠ½å‡ºé–¢æ•°

```python
def extract_temporal_trajectories_with_future_features(
    df, train_start, train_end,
    history_window_months=6,
    future_window_start_months=0,  # â† parserã§æŒ‡å®š
    future_window_end_months=1,    # â† parserã§æŒ‡å®š
    sampling_interval_months=1
):
    """
    é‡è¦: å°†æ¥ã®è²¢çŒ®ã‚’ã€Œæ­£è§£ãƒ©ãƒ™ãƒ«ã€ã§ã¯ãªãã€ŒçŠ¶æ…‹ç‰¹å¾´é‡ã€ã¨ã—ã¦æŠ½å‡º

    Args:
        future_window_start_months: å°†æ¥çª“ã®é–‹å§‹ï¼ˆ0ãªã‚‰ç¾æ™‚ç‚¹ã‹ã‚‰ï¼‰
        future_window_end_months: å°†æ¥çª“ã®çµ‚äº†ï¼ˆ1ãªã‚‰1ãƒ¶æœˆå¾Œã¾ã§ï¼‰
    """
    # å­¦ç¿’æœŸé–“å†…ã‚’1ãƒ¶æœˆé–“éš”ã§ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
    for sampling_point in sampling_points:
        for reviewer in reviewers:
            # éå»ã®å±¥æ­´
            history = get_history(reviewer, sampling_point - 6m, sampling_point)

            # å°†æ¥ã®è²¢çŒ®ï¼ˆ1æ¬¡å…ƒã®ã¿ï¼‰
            future_start = sampling_point + pd.DateOffset(months=future_window_start_months)
            future_end = sampling_point + pd.DateOffset(months=future_window_end_months)
            future_contribution = has_contribution(reviewer, future_start, future_end)

            trajectory = {
                'activity_history': history,
                'future_contribution': future_contribution,  # â† 1æ¬¡å…ƒã®ç‰¹å¾´é‡
                'future_window': {
                    'start_months': future_window_start_months,
                    'end_months': future_window_end_months,
                },
                # 'continued': ... â† å‰Šé™¤ï¼ˆæ­£è§£ãƒ©ãƒ™ãƒ«ã¯æ¸¡ã•ãªã„ï¼‰
            }
```

### 2. çŠ¶æ…‹æŠ½å‡ºé–¢æ•°ï¼ˆæ—¢å­˜ã®æ‹¡å¼µ IRL ã‚’æ´»ç”¨ï¼‰

```python
def extract_developer_state_with_future(self, trajectory):
    """
    çŠ¶æ…‹ãƒ™ã‚¯ãƒˆãƒ«ã«å°†æ¥ã®è²¢çŒ®ã‚’å«ã‚ã‚‹ï¼ˆ1æ¬¡å…ƒã®ã¿è¿½åŠ ï¼‰

    æ—¢å­˜ã® EnhancedRetentionIRLSystem ã®çŠ¶æ…‹æŠ½å‡ºã‚’æ´»ç”¨
    """
    # æ—¢å­˜ã®æ‹¡å¼µIRLå®Ÿè£…ã§éå»ãƒ™ãƒ¼ã‚¹ã®ç‰¹å¾´é‡ã‚’å–å¾—
    from gerrit_retention.rl_prediction.enhanced_retention_irl_system import EnhancedRetentionIRLSystem

    developer = trajectory['developer']
    activity_history = trajectory['activity_history']
    context_date = trajectory['context_date']

    # æ—¢å­˜å®Ÿè£…ã‚’ä½¿ç”¨ï¼ˆã“ã®éƒ¨åˆ†ã¯å¤‰æ›´ä¸è¦ï¼‰
    past_state_features = EnhancedRetentionIRLSystem.extract_developer_state(
        developer, activity_history, context_date
    )  # Næ¬¡å…ƒï¼ˆæ—¢å­˜ã®æ‹¡å¼µIRLã®æ¬¡å…ƒï¼‰

    # å°†æ¥ã®è²¢çŒ®ã‚’ç‰¹å¾´é‡ã¨ã—ã¦è¿½åŠ ï¼ˆ1æ¬¡å…ƒã®ã¿ï¼‰
    future_contribution = trajectory['future_contribution']
    future_feature = np.array([
        1.0 if future_contribution else 0.0
    ])  # 1æ¬¡å…ƒ

    # çµåˆï¼ˆN+1æ¬¡å…ƒï¼‰
    state_vector = np.concatenate([past_state_features, future_feature])
    return state_vector
```

### 3. ãƒ¢ãƒ‡ãƒ«ï¼ˆæ—¢å­˜ã®æ‹¡å¼µ IRL ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’æ´»ç”¨ï¼‰

```python
# æ—¢å­˜ã®æ‹¡å¼µIRLãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’ãã®ã¾ã¾ä½¿ç”¨å¯èƒ½
from gerrit_retention.rl_prediction.enhanced_retention_irl_system import EnhancedRetentionIRLNetwork

class FeatureBasedIRLNetwork(nn.Module):
    """
    å ±é…¬ã®ã¿ã‚’å‡ºåŠ›ï¼ˆç¶™ç¶šç¢ºç‡ã®äºˆæ¸¬ãƒ˜ãƒƒãƒ‰ã¯å‰Šé™¤ï¼‰

    æ—¢å­˜ã® EnhancedRetentionIRLNetwork ã‚’ãƒ™ãƒ¼ã‚¹ã«ã™ã‚‹
    """

    def __init__(self, state_dim, action_dim=5):  # state_dim = æ‹¡å¼µIRLæ¬¡å…ƒ + 1
        super().__init__()
        # æ—¢å­˜ã®æ‹¡å¼µIRLå®Ÿè£…ã‚’ãƒ™ãƒ¼ã‚¹ã«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’æ§‹ç¯‰
        # state_dim ã¯è‡ªå‹•çš„ã«æ‹¡å¼µIRLæ¬¡å…ƒ + 1æ¬¡å…ƒã«ãªã‚‹

        self.state_encoder = nn.Sequential(
            nn.Linear(state_dim, 64),
            nn.ReLU(),
            nn.Linear(64, 64),
            nn.ReLU()
        )

        self.action_encoder = nn.Sequential(
            nn.Linear(action_dim, 64),
            nn.ReLU(),
            nn.Linear(64, 64),
            nn.ReLU()
        )

        # LSTMï¼ˆæ™‚ç³»åˆ—ãƒ¢ãƒ¼ãƒ‰ï¼‰
        self.lstm = nn.LSTM(input_size=64, hidden_size=128, batch_first=True)

        # å ±é…¬äºˆæ¸¬ãƒ˜ãƒƒãƒ‰ã®ã¿
        self.reward_head = nn.Sequential(
            nn.Linear(128, 64),
            nn.ReLU(),
            nn.Linear(64, 1)
        )

    def forward(self, state, action):
        # state ã«ã¯æ‹¡å¼µIRLç‰¹å¾´ + å°†æ¥ã®è²¢çŒ®ï¼ˆ1æ¬¡å…ƒï¼‰ãŒå«ã¾ã‚Œã¦ã„ã‚‹
        state_enc = self.state_encoder(state)
        action_enc = self.action_encoder(action)
        combined = state_enc + action_enc

        lstm_out, _ = self.lstm(combined)
        final = lstm_out[:, -1, :]

        reward = self.reward_head(final)
        return reward  # å ±é…¬ã®ã¿è¿”ã™
```

### 4. è¨“ç·´ãƒ«ãƒ¼ãƒ—

```python
def train_irl_feature_based(self, trajectories, epochs=100):
    """æ­£è§£ãƒ©ãƒ™ãƒ«ã‚’ä½¿ã‚ãšã«è¨“ç·´"""

    for epoch in range(epochs):
        for trajectory in trajectories:
            # çŠ¶æ…‹ã‚’æŠ½å‡ºï¼ˆå°†æ¥ã®è²¢çŒ®ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’å«ã‚€ï¼‰
            state = self.extract_developer_state(trajectory)
            actions = self.extract_developer_actions(trajectory['activity_history'])

            # å ±é…¬ã‚’äºˆæ¸¬
            predicted_reward = self.network(state, action)

            # å°†æ¥ã®è²¢çŒ®ã‹ã‚‰ç›®æ¨™å ±é…¬ã‚’ç”Ÿæˆï¼ˆ1æ¬¡å…ƒãªã®ã§ã‚·ãƒ³ãƒ—ãƒ«ï¼‰
            future_contribution = trajectory['future_contribution']
            target_reward = 1.0 if future_contribution else 0.0

            # æå¤±è¨ˆç®—ï¼ˆæ­£è§£ãƒ©ãƒ™ãƒ«ã¯ä½¿ã‚ãªã„ï¼‰
            loss = F.mse_loss(predicted_reward, torch.tensor([[target_reward]]))

            # é€†ä¼æ’­
            loss.backward()
            self.optimizer.step()
```

---

## âš ï¸ é‡è¦ãªæ³¨æ„ç‚¹

### ãƒ‡ãƒ¼ã‚¿åˆ†å‰²: ã‚·ãƒ³ãƒ—ãƒ«ãªæ™‚ç³»åˆ— Cutoff ã§ OK

**æ–¹é‡:** è¨“ç·´ã¨è©•ä¾¡ã¯æ™‚ç³»åˆ—ã§ cutoff ã™ã‚‹ã ã‘ã§ååˆ†

```python
# ã‚·ãƒ³ãƒ—ãƒ«ãªæ™‚ç³»åˆ—åˆ†å‰²
train_period = [2019-01-01, 2020-01-01]
eval_period = [2020-01-01, 2021-01-01]  # cutoff ã§åˆ†é›¢

# ã“ã‚Œã§ååˆ†ï¼ˆè¤‡é›‘ãªå¯¾ç­–ã¯ä¸è¦ï¼‰
```

### çŠ¶æ…‹ç‰¹å¾´é‡: æ—¢å­˜ã®æ‹¡å¼µ IRL ã‚’æ´»ç”¨

**æ–¹é‡:** çŠ¶æ…‹ç‰¹å¾´é‡ã®æŠ½å‡ºã¯æ—¢å­˜ã® `EnhancedRetentionIRLSystem` ã‚’æ´»ç”¨

```python
# æ—¢å­˜ã®æ‹¡å¼µIRLå®Ÿè£…ã‚’ä½¿ç”¨
from gerrit_retention.rl_prediction.enhanced_retention_irl_system import EnhancedRetentionIRLSystem

# çŠ¶æ…‹æŠ½å‡ºã¯æ—¢å­˜å®Ÿè£…ã‚’ä½¿ã†
state_features = enhanced_irl.extract_developer_state(developer, activity_history, context_date)

# å°†æ¥ã®è²¢çŒ®ã‚’è¿½åŠ ï¼ˆ1æ¬¡å…ƒã®ã¿ï¼‰
future_feature = [
    1.0 if trajectory['future_contribution'] else 0.0
]

# çµåˆ
state_vector = np.concatenate([state_features, future_feature])
```

### æ¨è«–æ™‚ã®æ‰±ã„

```python
# æ¨è«–æ™‚ã¯å°†æ¥ã®è²¢çŒ®ãŒæœªçŸ¥ â†’ ã‚¼ãƒ­åŸ‹ã‚
future_contribution = 0.0  # ã¾ãŸã¯ False
```

---

## ğŸ“ è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ä¾‹

```yaml
# configs/irl_feature_based_config.yaml

irl_feature_based:
  # å­¦ç¿’æœŸé–“
  training_period:
    start_date: "2019-01-01"
    end_date: "2020-01-01"

  # ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°è¨­å®š
  sampling:
    interval_months: 1 # 1ãƒ¶æœˆã”ã¨ã«ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
    history_window_months: 6 # å„ã‚µãƒ³ãƒ—ãƒ«ã®å±¥æ­´æœŸé–“
    min_history_events: 3

  # å°†æ¥ã®è²¢çŒ®çª“ï¼ˆparserã§æŒ‡å®šå¯èƒ½ï¼‰
  future_window:
    start_months: 0 # 0ãƒ¶æœˆå¾Œã‹ã‚‰ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼‰
    end_months: 1 # 1ãƒ¶æœˆå¾Œã¾ã§ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼‰
    description: "å°†æ¥ã®è²¢çŒ®ã‚’è¦‹ã‚‹æœŸé–“ï¼ˆçŠ¶æ…‹ç‰¹å¾´é‡ã¨ã—ã¦ä½¿ç”¨ã€1æ¬¡å…ƒã®ã¿ï¼‰"

  # ãƒ¢ãƒ‡ãƒ«è¨­å®š
  model:
    # state_dim ã¯è‡ªå‹•è¨ˆç®—: æ‹¡å¼µIRLæ¬¡å…ƒ + 1æ¬¡å…ƒ
    use_enhanced_irl_features: true # æ‹¡å¼µIRLç‰¹å¾´é‡ã‚’ä½¿ç”¨
    action_dim: 5
    hidden_dim: 64
    lstm_hidden: 128
    sequence: true
    seq_len: 15

  # è¨“ç·´è¨­å®š
  training:
    epochs: 30
    learning_rate: 0.001
    approach: "binary" # "binary", "continuous", "contrastive"

# ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å¼•æ•°ã§ã®æŒ‡å®šä¾‹
# python train_feature_based_irl.py \
#   --future-window-start 0 \   # 0ãƒ¶æœˆå¾Œã‹ã‚‰
#   --future-window-end 1 \     # 1ãƒ¶æœˆå¾Œã¾ã§
#   --epochs 30
```

---

## ğŸš€ å®Ÿè£…æ‰‹é †ï¼ˆã‚·ãƒ³ãƒ—ãƒ«ç‰ˆï¼‰

### Step 1: ãƒ‡ãƒ¼ã‚¿æŠ½å‡ºã®å¤‰æ›´

```bash
# æ–°ã—ã„ãƒ‡ãƒ¼ã‚¿æŠ½å‡ºé–¢æ•°ã‚’å®Ÿè£…
# scripts/data_processing/extract_temporal_features.py

# é‡è¦: æ—¢å­˜ã®ãƒ‡ãƒ¼ã‚¿æŠ½å‡ºã‚’ãƒ™ãƒ¼ã‚¹ã«ã€å°†æ¥ã®è²¢çŒ®ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’è¿½åŠ ã™ã‚‹ã ã‘
```

### Step 2: çŠ¶æ…‹æŠ½å‡ºã®ãƒ©ãƒƒãƒ‘ãƒ¼ä½œæˆ

```bash
# æ—¢å­˜ã® EnhancedRetentionIRLSystem ã‚’ãƒ©ãƒƒãƒ—
# src/gerrit_retention/rl_prediction/feature_based_irl_wrapper.py

# extract_developer_state_with_future() ã‚’è¿½åŠ 
# â†’ æ—¢å­˜ã®æ‹¡å¼µIRLç‰¹å¾´ + å°†æ¥3æ¬¡å…ƒã‚’çµåˆ
```

### Step 3: è¨“ç·´ã‚¹ã‚¯ãƒªãƒ—ãƒˆã®ä½œæˆ

```bash
# æ–°ã—ã„è¨“ç·´ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’å®Ÿè£…
# scripts/training/irl/train_feature_based_irl.py

# æ—¢å­˜ã® train_temporal_irl_sliding_window.py ã‚’ãƒ™ãƒ¼ã‚¹ã«
# å°†æ¥ã®è²¢çŒ®ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’çŠ¶æ…‹ç‰¹å¾´é‡ã«çµ„ã¿è¾¼ã‚€ã‚ˆã†ã«å¤‰æ›´
```

### Step 4: è©•ä¾¡ï¼ˆæ™‚ç³»åˆ— Cutoff ã§åˆ†å‰²ï¼‰

```bash
# ã‚·ãƒ³ãƒ—ãƒ«ãªæ™‚ç³»åˆ—åˆ†å‰²ã§è©•ä¾¡
# train: 2019-01-01 ~ 2020-01-01
# eval:  2020-01-01 ~ 2021-01-01
```

**å®Ÿè£…ã®ãƒã‚¤ãƒ³ãƒˆ:**

- æ—¢å­˜ã®æ‹¡å¼µ IRL å®Ÿè£…ã‚’æœ€å¤§é™æ´»ç”¨
- ãƒ‡ãƒ¼ã‚¿æŠ½å‡ºã¨çŠ¶æ…‹çµåˆã®éƒ¨åˆ†ã ã‘ã‚’å¤‰æ›´
- è¤‡é›‘ãªå¯¾ç­–ã¯ä¸è¦ï¼ˆcutoff ã§ååˆ†ï¼‰

---

## ğŸ“ ç†è«–çš„èƒŒæ™¯

### ãªãœã“ã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒãŒæœ‰åŠ¹ã‹ï¼Ÿ

1. **çŠ¶æ…‹ã®è±Šã‹ãªè¡¨ç¾:**
   - éå»ã®å±¥æ­´ã ã‘ã§ãªãã€å°†æ¥ã®çµæœã‚‚çŠ¶æ…‹ã®ä¸€éƒ¨ã¨ã—ã¦æ‰±ã†ã“ã¨ã§ã€ã‚ˆã‚Šè±Šã‹ãªçŠ¶æ…‹è¡¨ç¾ã‚’å­¦ç¿’
2. **è‡ªå·±æ•™å¸«ã‚ã‚Šå­¦ç¿’:**
   - æ˜ç¤ºçš„ãªæ•™å¸«ãƒ©ãƒ™ãƒ«ã§ã¯ãªãã€ãƒ‡ãƒ¼ã‚¿å†…ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆå°†æ¥ã®è²¢çŒ®ï¼‰ã‹ã‚‰å­¦ç¿’
3. **IRL ã®æœ¬è³ªã«è¿‘ã„:**
   - å°‚é–€å®¶ï¼ˆç¶™ç¶šã™ã‚‹é–‹ç™ºè€…ï¼‰ã®è»Œè·¡ã‹ã‚‰å ±é…¬é–¢æ•°ã‚’å­¦ç¿’ã™ã‚‹ã¨ã„ã†ã€IRL ã®æœ¬æ¥ã®ç›®çš„ã«è¿‘ã„

### å¾“æ¥ã®æ•™å¸«ã‚ã‚Šå­¦ç¿’ã¨ã®é•ã„

| ç‰¹å¾´             | æ•™å¸«ã‚ã‚Šå­¦ç¿’       | ç‰¹å¾´é‡ãƒ™ãƒ¼ã‚¹ IRL             |
| ---------------- | ------------------ | ---------------------------- |
| **ãƒ©ãƒ™ãƒ«ã®æ‰±ã„** | æ˜ç¤ºçš„ãªæ­£è§£ãƒ©ãƒ™ãƒ« | ç‰¹å¾´é‡ã®ä¸€éƒ¨                 |
| **å­¦ç¿’ç›®æ¨™**     | ãƒ©ãƒ™ãƒ«ã‚’æ­£ã—ãäºˆæ¸¬ | å ±é…¬é–¢æ•°ã‚’å­¦ç¿’               |
| **æŸ”è»Ÿæ€§**       | å›ºå®šçš„             | è¤‡æ•°ã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒå¯èƒ½         |
| **ãƒ‡ãƒ¼ã‚¿åŠ¹ç‡**   | ä½ã„               | é«˜ã„ï¼ˆè¤‡æ•°æ™‚ç‚¹ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¼‰ |

---

## ğŸ“š å‚è€ƒ

- è©³ç´°ãªå®Ÿè£…: `docs/irl_temporal_window_redesign.md`
- ç¾åœ¨ã® IRL å®Ÿè£…: `src/gerrit_retention/rl_prediction/retention_irl_system.py`
- æ™‚ç³»åˆ— IRL: `README_TEMPORAL_IRL.md`

---

---

## ğŸ”¬ å®Ÿé¨“ä¾‹ï¼ˆå°†æ¥çª“ã‚’å¤‰ãˆã¦ãƒ¢ãƒ‡ãƒ«ã®å¤‰åŒ–ã‚’è¦³å¯Ÿï¼‰

### å®Ÿé¨“è¨­è¨ˆ

```bash
# å®Ÿé¨“1: çŸ­æœŸäºˆæ¸¬ï¼ˆ0-1ãƒ¶æœˆå¾Œï¼‰
python scripts/training/irl/train_feature_based_irl.py \
  --reviews data/review_requests_openstack_multi_5y_detail.csv \
  --train-start 2019-01-01 \
  --train-end 2020-01-01 \
  --future-window-start 0 \
  --future-window-end 1 \
  --output outputs/irl_future_0_1m

# å®Ÿé¨“2: ä¸­æœŸäºˆæ¸¬ï¼ˆ1-3ãƒ¶æœˆå¾Œï¼‰
python scripts/training/irl/train_feature_based_irl.py \
  --reviews data/review_requests_openstack_multi_5y_detail.csv \
  --train-start 2019-01-01 \
  --train-end 2020-01-01 \
  --future-window-start 1 \
  --future-window-end 3 \
  --output outputs/irl_future_1_3m

# å®Ÿé¨“3: é•·æœŸäºˆæ¸¬ï¼ˆ3-6ãƒ¶æœˆå¾Œï¼‰
python scripts/training/irl/train_feature_based_irl.py \
  --reviews data/review_requests_openstack_multi_5y_detail.csv \
  --train-start 2019-01-01 \
  --train-end 2020-01-01 \
  --future-window-start 3 \
  --future-window-end 6 \
  --output outputs/irl_future_3_6m
```

### æœŸå¾…ã•ã‚Œã‚‹çµæœ

| å®Ÿé¨“       | å°†æ¥çª“ | æœŸå¾…ã•ã‚Œã‚‹ç‰¹å¾´                               |
| ---------- | ------ | -------------------------------------------- |
| **å®Ÿé¨“ 1** | 0-1m   | çŸ­æœŸçš„ãªãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’å­¦ç¿’ã€å³åº§ã®é›¢è„±ã‚’äºˆæ¸¬     |
| **å®Ÿé¨“ 2** | 1-3m   | ä¸­æœŸçš„ãªãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’å­¦ç¿’ã€ãƒãƒ©ãƒ³ã‚¹å‹           |
| **å®Ÿé¨“ 3** | 3-6m   | é•·æœŸçš„ãªãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’å­¦ç¿’ã€æŒç¶šçš„ãªè²¢çŒ®è€…ã‚’è­˜åˆ¥ |

### æ¯”è¼ƒåˆ†æ

```bash
# çµæœã‚’æ¯”è¼ƒ
python scripts/analysis/compare_future_window_results.py \
  --results outputs/irl_future_*/evaluation_results.json \
  --output outputs/future_window_comparison.png
```

**åˆ†æè¦³ç‚¹:**

- ã©ã®å°†æ¥çª“ãŒæœ€ã‚‚äºˆæ¸¬ç²¾åº¦ãŒé«˜ã„ã‹ï¼Ÿ
- çŸ­æœŸ vs é•·æœŸã§å­¦ç¿’ã•ã‚Œã‚‹ç‰¹å¾´ã¯ã©ã†é•ã†ã‹ï¼Ÿ
- å®Ÿç”¨çš„ã«ã¯ã©ã®çª“ãŒæœ€é©ã‹ï¼Ÿ

---

**ä½œæˆæ—¥:** 2025-10-21  
**ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹:** ææ¡ˆ
