# IRLå¯¾æŠ—é¦¬: æ¯”è¼ƒå®Ÿé¨“è¨­è¨ˆ

## ğŸ“‹ æ¦‚è¦

ç¾åœ¨ã®IRL+LSTMãƒ¢ãƒ‡ãƒ«ï¼ˆAUC-PR 0.718ï¼‰ã®æœ‰åŠ¹æ€§ã‚’æ¤œè¨¼ã™ã‚‹ãŸã‚ã€è¤‡æ•°ã®å¯¾æŠ—é¦¬ï¼ˆãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ï¼‰ã¨ã®æ¯”è¼ƒå®Ÿé¨“ã‚’è¨­è¨ˆã™ã‚‹ã€‚

---

## ğŸ¯ å¯¾æŠ—é¦¬ã®åˆ†é¡

### Tier 1: å¿…é ˆãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ï¼ˆæœ€å„ªå…ˆï¼‰

ç°¡å˜ã«å®Ÿè£…ã§ãã€å¿…ãšæ¯”è¼ƒã™ã¹ãæ‰‹æ³•

### Tier 2: é‡è¦ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ï¼ˆé«˜å„ªå…ˆï¼‰

æ™‚ç³»åˆ—ãƒ¢ãƒ‡ãƒ«ã¨ã®å…¬å¹³ãªæ¯”è¼ƒã«å¿…è¦

### Tier 3: ç™ºå±•çš„ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ï¼ˆä¸­å„ªå…ˆï¼‰

ã‚ˆã‚Šæ·±ã„æ´å¯Ÿã‚’å¾—ã‚‹ãŸã‚ã®æ‰‹æ³•

---

## ğŸ“Š Tier 1: å¿…é ˆãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³

### 1.1 ãƒ©ãƒ³ãƒ€ãƒ äºˆæ¸¬ï¼ˆRandom Baselineï¼‰

**æ¦‚è¦**: å…¨å“¡ã«0.5ã®ç¢ºç‡ã‚’å‰²ã‚Šå½“ã¦ã‚‹

**å®Ÿè£…é›£æ˜“åº¦**: â˜…â˜†â˜†â˜†â˜†ï¼ˆè¶…ç°¡å˜ï¼‰

**å®Ÿè£…æ™‚é–“**: 10åˆ†

**æœŸå¾…æ€§èƒ½**: AUC-PR â‰ˆ 0.35ï¼ˆæ­£ä¾‹ç‡ã«ä¾å­˜ï¼‰

**ã‚³ãƒ¼ãƒ‰ä¾‹**:
```python
def random_baseline(test_data):
    """ãƒ©ãƒ³ãƒ€ãƒ ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³"""
    import numpy as np

    y_true = test_data['continued'].values
    y_pred = np.random.uniform(0.4, 0.6, len(y_true))  # 0.5ä»˜è¿‘ã®ãƒ©ãƒ³ãƒ€ãƒ å€¤

    return {
        'predictions': y_pred,
        'method': 'random'
    }
```

**æ„ç¾©**:
- æœ€ä½é™ã®æ€§èƒ½åŸºæº–
- "ä½•ã‚‚ã—ãªã„"å ´åˆã¨ã®æ¯”è¼ƒ

---

### 1.2 å˜ç´”ãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹ï¼ˆRule-based Baselineï¼‰

**æ¦‚è¦**: ç°¡å˜ãªif-thenãƒ«ãƒ¼ãƒ«ã§äºˆæ¸¬

**å®Ÿè£…é›£æ˜“åº¦**: â˜…â˜…â˜†â˜†â˜†ï¼ˆç°¡å˜ï¼‰

**å®Ÿè£…æ™‚é–“**: 30åˆ†

**æœŸå¾…æ€§èƒ½**: AUC-PR â‰ˆ 0.45-0.55

**ãƒ«ãƒ¼ãƒ«ä¾‹**:
```python
def rule_based_baseline(developer, activity_history):
    """
    ãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹äºˆæ¸¬

    Rule 1: çµŒé¨“ > 200ä»¶ â†’ ç¶™ç¶šç¢ºç‡0.8
    Rule 2: å—è«¾ç‡ > 20% â†’ ç¶™ç¶šç¢ºç‡0.7
    Rule 3: æœ€è¿‘30æ—¥ã®æ´»å‹• > 5ä»¶ â†’ ç¶™ç¶šç¢ºç‡0.6
    Rule 4: ãã‚Œä»¥å¤– â†’ ç¶™ç¶šç¢ºç‡0.3
    """
    score = 0.3  # ãƒ™ãƒ¼ã‚¹ã‚¹ã‚³ã‚¢

    # ãƒ«ãƒ¼ãƒ«é©ç”¨
    if developer['experience'] > 200:
        score = max(score, 0.8)

    if developer['acceptance_rate'] > 0.2:
        score = max(score, 0.7)

    recent_activities = [a for a in activity_history
                         if (datetime.now() - a['timestamp']).days <= 30]
    if len(recent_activities) > 5:
        score = max(score, 0.6)

    return {
        'continuation_probability': score,
        'method': 'rule_based',
        'applied_rules': []  # ã©ã®ãƒ«ãƒ¼ãƒ«ãŒé©ç”¨ã•ã‚ŒãŸã‹è¨˜éŒ²
    }
```

**æ„ç¾©**:
- ãƒ‰ãƒ¡ã‚¤ãƒ³çŸ¥è­˜ã®ã¿ã§åˆ°é”å¯èƒ½ãªæ€§èƒ½
- è§£é‡ˆæ€§ãŒé«˜ã„
- å®Ÿå‹™ã§ã®æœ€ä½ãƒ©ã‚¤ãƒ³

---

### 1.3 ãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯å›å¸°ï¼ˆLogistic Regressionï¼‰

**æ¦‚è¦**: ä¼çµ±çš„ãªçµ±è¨ˆãƒ¢ãƒ‡ãƒ«

**å®Ÿè£…é›£æ˜“åº¦**: â˜…â˜…â˜†â˜†â˜†ï¼ˆç°¡å˜ï¼‰

**å®Ÿè£…æ™‚é–“**: 1-2æ™‚é–“

**æœŸå¾…æ€§èƒ½**: AUC-PR â‰ˆ 0.55-0.65

**å®Ÿè£…**:
```python
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import StandardScaler

def logistic_regression_baseline(train_data, test_data):
    """ãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯å›å¸°ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³"""

    # ç‰¹å¾´é‡æŠ½å‡ºï¼ˆæ™‚ç³»åˆ—ã‚’é›†ç´„ï¼‰
    def extract_features(data):
        features = []
        for _, row in data.iterrows():
            # ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆç‰¹å¾´é‡ï¼ˆæ™‚ç³»åˆ—ã‚’é›†ç´„ï¼‰
            feat = [
                row['experience_days'] / 730.0,
                row['total_reviews'] / 500.0,
                row['acceptance_rate'],
                row['recent_activity_frequency'],
                row['avg_activity_gap'] / 60.0,
                row['collaboration_score'],
                row['code_quality_score'],
                # æ™‚ç³»åˆ—çµ±è¨ˆé‡
                np.mean([a['intensity'] for a in row['activity_history']]),
                np.std([a['intensity'] for a in row['activity_history']]),
                len(row['activity_history'])
            ]
            features.append(feat)
        return np.array(features)

    X_train = extract_features(train_data)
    y_train = train_data['continued'].values

    X_test = extract_features(test_data)

    # æ¨™æº–åŒ–
    scaler = StandardScaler()
    X_train = scaler.fit_transform(X_train)
    X_test = scaler.transform(X_test)

    # è¨“ç·´
    model = LogisticRegression(
        max_iter=1000,
        class_weight='balanced',  # ã‚¯ãƒ©ã‚¹ä¸å‡è¡¡å¯¾å¿œ
        random_state=42
    )
    model.fit(X_train, y_train)

    # äºˆæ¸¬
    y_pred = model.predict_proba(X_test)[:, 1]

    return {
        'predictions': y_pred,
        'model': model,
        'feature_importance': model.coef_[0],  # ä¿‚æ•°ï¼ç‰¹å¾´é‡é‡è¦åº¦
        'method': 'logistic_regression'
    }
```

**æ„ç¾©**:
- çµ±è¨ˆå­¦ã®æ¨™æº–æ‰‹æ³•
- ç‰¹å¾´é‡é‡è¦åº¦ãŒè§£é‡ˆå¯èƒ½
- å¤šãã®ç ”ç©¶ã§ä½¿ã‚ã‚Œã‚‹ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³

**åˆ©ç‚¹**:
- è¨“ç·´ãŒé«˜é€Ÿ
- éå­¦ç¿’ã—ã«ãã„
- è§£é‡ˆæ€§ãŒé«˜ã„

---

### 1.4 ãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆï¼ˆRandom Forestï¼‰

**æ¦‚è¦**: ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«å­¦ç¿’ã«ã‚ˆã‚‹éç·šå½¢ãƒ¢ãƒ‡ãƒ«

**å®Ÿè£…é›£æ˜“åº¦**: â˜…â˜…â˜†â˜†â˜†ï¼ˆç°¡å˜ï¼‰

**å®Ÿè£…æ™‚é–“**: 1-2æ™‚é–“

**æœŸå¾…æ€§èƒ½**: AUC-PR â‰ˆ 0.60-0.70

**å®Ÿè£…**: âœ… **å®Ÿè£…æ¸ˆã¿** (`src/gerrit_retention/baselines/random_forest.py`)

```python
from sklearn.ensemble import RandomForestClassifier

def random_forest_baseline(train_data, test_data):
    """ãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³"""

    # ç‰¹å¾´é‡æŠ½å‡ºï¼ˆãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯å›å¸°ã¨åŒã˜ï¼‰
    X_train = extract_features(train_data)
    y_train = train_data['continued'].values
    X_test = extract_features(test_data)

    # è¨“ç·´
    model = RandomForestClassifier(
        n_estimators=100,        # æœ¨ã®æ•°
        max_depth=None,          # æ·±ã•åˆ¶é™ãªã—
        min_samples_split=2,
        max_features='sqrt',     # sqrt(n_features)ã‚’ä½¿ç”¨
        class_weight='balanced', # ã‚¯ãƒ©ã‚¹ä¸å‡è¡¡å¯¾å¿œ
        oob_score=True,          # Out-of-Bagè©•ä¾¡
        n_jobs=-1,               # ä¸¦åˆ—å‡¦ç†
        random_state=42
    )
    model.fit(X_train, y_train)

    # äºˆæ¸¬
    y_pred = model.predict_proba(X_test)[:, 1]

    return {
        'predictions': y_pred,
        'model': model,
        'feature_importance': model.feature_importances_,
        'oob_score': model.oob_score_,  # OOBç²¾åº¦
        'method': 'random_forest'
    }
```

**æ„ç¾©**:
- **XGBoostã‚ˆã‚Šå®Ÿè£…ãŒç°¡å˜** - ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿èª¿æ•´ãŒå°‘ãªã„
- éç·šå½¢é–¢ä¿‚ã‚’æ•æ‰ã§ãã‚‹
- ç‰¹å¾´é‡é‡è¦åº¦ãŒè‡ªç„¶ã«å¾—ã‚‰ã‚Œã‚‹
- éå­¦ç¿’ã«å¼·ã„ï¼ˆã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«åŠ¹æœï¼‰

**åˆ©ç‚¹**:
- ãƒ­ãƒã‚¹ãƒˆæ€§ãŒé«˜ã„
- ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«å­¦ç¿’ã®åŠ¹æœã‚’æ¤œè¨¼
- OOB scoreã§è¿½åŠ ã®æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ä¸è¦
- ä¸¦åˆ—å‡¦ç†ã§é«˜é€Ÿï¼ˆn_jobs=-1ï¼‰

**XGBoostã¨ã®æ¯”è¼ƒ**:
| ç‰¹æ€§ | ãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆ | XGBoost |
|------|-------------------|---------|
| å®Ÿè£…é›£æ˜“åº¦ | â˜…â˜…â˜†â˜†â˜† | â˜…â˜…â˜…â˜†â˜† |
| ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚° | ç°¡å˜ | ã‚„ã‚„è¤‡é›‘ |
| æœŸå¾…æ€§èƒ½ | 0.60-0.70 | 0.65-0.75 |
| è¨“ç·´æ™‚é–“ | é€Ÿã„ | ã‚„ã‚„é…ã„ |

**æ¨å¥¨ç†ç”±**:
1. ãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯å›å¸°ï¼ˆç·šå½¢ï¼‰ã¨XGBoostï¼ˆå‹¾é…ãƒ–ãƒ¼ã‚¹ãƒ†ã‚£ãƒ³ã‚°ï¼‰ã®ä¸­é–“
2. éç·šå½¢ãƒ¢ãƒ‡ãƒ«ã®åŸºæº–ã¨ã—ã¦é‡è¦
3. å®Ÿè£…ãŒç°¡å˜ã§å†ç¾æ€§ãŒé«˜ã„

---

## ğŸ“Š Tier 2: é‡è¦ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³

### 2.1 å‹¾é…ãƒ–ãƒ¼ã‚¹ãƒ†ã‚£ãƒ³ã‚°ï¼ˆXGBoost/LightGBMï¼‰

**æ¦‚è¦**: é«˜æ€§èƒ½ãªæ©Ÿæ¢°å­¦ç¿’æ‰‹æ³•

**å®Ÿè£…é›£æ˜“åº¦**: â˜…â˜…â˜…â˜†â˜†ï¼ˆä¸­ç¨‹åº¦ï¼‰

**å®Ÿè£…æ™‚é–“**: 3-4æ™‚é–“

**æœŸå¾…æ€§èƒ½**: AUC-PR â‰ˆ 0.65-0.75ï¼ˆIRLã¨åŒç­‰ã®å¯èƒ½æ€§ï¼‰

**å®Ÿè£…**:
```python
import xgboost as xgb
from sklearn.model_selection import GridSearchCV

def xgboost_baseline(train_data, test_data):
    """XGBoostãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³"""

    # ç‰¹å¾´é‡æŠ½å‡ºï¼ˆæ™‚ç³»åˆ—ã‚’é›†ç´„ + ã‚ˆã‚Šå¤šãã®çµ±è¨ˆé‡ï¼‰
    def extract_features_advanced(data):
        features = []
        for _, row in data.iterrows():
            history = row['activity_history']

            feat = [
                # åŸºæœ¬çµ±è¨ˆ
                row['experience_days'] / 730.0,
                row['total_reviews'] / 500.0,
                row['acceptance_rate'],
                row['recent_activity_frequency'],

                # æ™‚ç³»åˆ—çµ±è¨ˆï¼ˆå¹³å‡ãƒ»æ¨™æº–åå·®ãƒ»æœ€å¤§ãƒ»æœ€å°ï¼‰
                np.mean([a['intensity'] for a in history]) if history else 0,
                np.std([a['intensity'] for a in history]) if history else 0,
                np.max([a['intensity'] for a in history]) if history else 0,
                np.min([a['intensity'] for a in history]) if history else 0,

                # ãƒˆãƒ¬ãƒ³ãƒ‰ï¼ˆç›´è¿‘ vs éå»ã®æ¯”è¼ƒï¼‰
                np.mean([a['intensity'] for a in history[-10:]]) if len(history) >= 10 else 0,
                np.mean([a['intensity'] for a in history[-30:-10]]) if len(history) >= 30 else 0,

                # æ´»å‹•ãƒ‘ã‚¿ãƒ¼ãƒ³
                len(history),
                len([a for a in history if a['collaboration'] > 0.5]) / max(len(history), 1),

                # å—è«¾ç‡ã®æ™‚ç³»åˆ—
                np.mean([a.get('accepted', 0) for a in history]) if history else 0,
                np.std([a.get('accepted', 0) for a in history]) if history else 0
            ]
            features.append(feat)
        return np.array(features)

    X_train = extract_features_advanced(train_data)
    y_train = train_data['continued'].values
    X_test = extract_features_advanced(test_data)

    # ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°
    param_grid = {
        'max_depth': [3, 5, 7],
        'learning_rate': [0.01, 0.05, 0.1],
        'n_estimators': [100, 200, 300],
        'subsample': [0.8, 1.0],
        'colsample_bytree': [0.8, 1.0]
    }

    xgb_model = xgb.XGBClassifier(
        objective='binary:logistic',
        eval_metric='aucpr',
        random_state=42
    )

    # ã‚°ãƒªãƒƒãƒ‰ã‚µãƒ¼ãƒ
    grid_search = GridSearchCV(
        xgb_model,
        param_grid,
        cv=5,
        scoring='average_precision',  # AUC-PR
        n_jobs=-1
    )

    grid_search.fit(X_train, y_train)

    # æœ€è‰¯ãƒ¢ãƒ‡ãƒ«ã§äºˆæ¸¬
    best_model = grid_search.best_estimator_
    y_pred = best_model.predict_proba(X_test)[:, 1]

    return {
        'predictions': y_pred,
        'model': best_model,
        'feature_importance': best_model.feature_importances_,
        'best_params': grid_search.best_params_,
        'method': 'xgboost'
    }
```

**æ„ç¾©**:
- **æœ€å¼·ã®ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³**ï¼ˆIRLã‚ˆã‚Šé«˜æ€§èƒ½ã®å¯èƒ½æ€§ï¼‰
- Kaggleã‚³ãƒ³ãƒšã§é »ç¹ã«å„ªå‹
- æ™‚ç³»åˆ—ã‚’é›†ç´„ã—ãŸå ´åˆã®ä¸Šé™æ€§èƒ½ã‚’ç¤ºã™

**é‡è¦**: XGBoostãŒIRLã‚ˆã‚Šé«˜æ€§èƒ½ãªã‚‰ã€æ™‚ç³»åˆ—ãƒ¢ãƒ‡ãƒªãƒ³ã‚°ã®ä¾¡å€¤ã‚’å†è€ƒã™ã‚‹å¿…è¦ãŒã‚ã‚‹

---

### 2.2 LSTMï¼ˆIRLãªã—ï¼‰

**æ¦‚è¦**: æ™‚ç³»åˆ—ãƒ¢ãƒ‡ãƒªãƒ³ã‚°ã ãŒIRLã‚’ä½¿ã‚ãªã„

**å®Ÿè£…é›£æ˜“åº¦**: â˜…â˜…â˜…â˜†â˜†ï¼ˆä¸­ç¨‹åº¦ï¼‰

**å®Ÿè£…æ™‚é–“**: 1æ—¥

**æœŸå¾…æ€§èƒ½**: AUC-PR â‰ˆ 0.65-0.72

**å®Ÿè£…**:
```python
import torch
import torch.nn as nn

class VanillaLSTMClassifier(nn.Module):
    """ç´”ç²‹ãªLSTMåˆ†é¡å™¨ï¼ˆIRLãªã—ï¼‰"""

    def __init__(self, feature_dim=15, hidden_dim=128, num_layers=2):
        super().__init__()

        self.lstm = nn.LSTM(
            input_size=feature_dim,
            hidden_size=hidden_dim,
            num_layers=num_layers,
            batch_first=True,
            dropout=0.2
        )

        self.classifier = nn.Sequential(
            nn.Linear(hidden_dim, 64),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(64, 1),
            nn.Sigmoid()
        )

    def forward(self, sequence):
        """
        Args:
            sequence: [batch, seq_len, feature_dim]
        Returns:
            prob: [batch, 1]
        """
        lstm_out, (h_n, c_n) = self.lstm(sequence)

        # æœ€çµ‚ã‚¹ãƒ†ãƒƒãƒ—ã®éš ã‚ŒçŠ¶æ…‹ã‚’ä½¿ç”¨
        final_hidden = h_n[-1]  # [batch, hidden_dim]

        prob = self.classifier(final_hidden)
        return prob

def vanilla_lstm_baseline(train_data, test_data, epochs=30):
    """ç´”ç²‹LSTMï¼ˆIRLãªã—ï¼‰ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³"""

    model = VanillaLSTMClassifier(
        feature_dim=15,  # çŠ¶æ…‹10æ¬¡å…ƒ + è¡Œå‹•5æ¬¡å…ƒ
        hidden_dim=128,
        num_layers=2
    )

    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
    criterion = nn.BCELoss()

    # è¨“ç·´ãƒ«ãƒ¼ãƒ—
    for epoch in range(epochs):
        model.train()
        epoch_loss = 0

        for trajectory in train_data:
            # ç‰¹å¾´é‡ã‚·ãƒ¼ã‚±ãƒ³ã‚¹æ§‹ç¯‰
            sequence = build_sequence(trajectory)  # [1, seq_len, 15]
            label = torch.tensor([[1.0 if trajectory['continued'] else 0.0]])

            # å‰å‘ãè¨ˆç®—
            pred = model(sequence)
            loss = criterion(pred, label)

            # ãƒãƒƒã‚¯ãƒ—ãƒ­ãƒ‘ã‚²ãƒ¼ã‚·ãƒ§ãƒ³
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            epoch_loss += loss.item()

        if epoch % 10 == 0:
            print(f"Epoch {epoch}: Loss = {epoch_loss / len(train_data):.4f}")

    # è©•ä¾¡
    model.eval()
    predictions = []
    with torch.no_grad():
        for trajectory in test_data:
            sequence = build_sequence(trajectory)
            pred = model(sequence)
            predictions.append(pred.item())

    return {
        'predictions': np.array(predictions),
        'model': model,
        'method': 'vanilla_lstm'
    }
```

**æ„ç¾©**:
- **IRLã®ä¾¡å€¤ã‚’æ¸¬å®š**ï¼ˆLSTM vs IRL+LSTMï¼‰
- æ™‚ç³»åˆ—ãƒ¢ãƒ‡ãƒªãƒ³ã‚°ã®åŠ¹æœã‚’åˆ†é›¢
- ã‚‚ã—Vanilla LSTMãŒIRL+LSTMã¨åŒç­‰ãªã‚‰ã€IRLã¯ä¸è¦

**é‡è¦**: ã“ã‚ŒãŒæœ€ã‚‚é‡è¦ãªæ¯”è¼ƒï¼

---

### 2.3 Transformerï¼ˆIRLãªã—ï¼‰

**æ¦‚è¦**: ç¾ä»£çš„ãªæ™‚ç³»åˆ—ãƒ¢ãƒ‡ãƒ«

**å®Ÿè£…é›£æ˜“åº¦**: â˜…â˜…â˜…â˜…â˜†ï¼ˆã‚„ã‚„é›£ï¼‰

**å®Ÿè£…æ™‚é–“**: 2-3æ—¥

**æœŸå¾…æ€§èƒ½**: AUC-PR â‰ˆ 0.68-0.75

**å®Ÿè£…**:
```python
class TransformerClassifier(nn.Module):
    """Transformerãƒ™ãƒ¼ã‚¹åˆ†é¡å™¨"""

    def __init__(self, feature_dim=15, d_model=128, nhead=4, num_layers=2):
        super().__init__()

        # å…¥åŠ›åŸ‹ã‚è¾¼ã¿
        self.input_projection = nn.Linear(feature_dim, d_model)

        # Positional Encoding
        self.pos_encoder = PositionalEncoding(d_model)

        # Transformer Encoder
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=d_model,
            nhead=nhead,
            dim_feedforward=512,
            dropout=0.1,
            batch_first=True
        )
        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)

        # åˆ†é¡å™¨
        self.classifier = nn.Sequential(
            nn.Linear(d_model, 64),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(64, 1),
            nn.Sigmoid()
        )

    def forward(self, sequence):
        """
        Args:
            sequence: [batch, seq_len, feature_dim]
        Returns:
            prob: [batch, 1]
        """
        # åŸ‹ã‚è¾¼ã¿ + Positional Encoding
        x = self.input_projection(sequence)  # [batch, seq_len, d_model]
        x = self.pos_encoder(x)

        # Transformer
        transformer_out = self.transformer(x)  # [batch, seq_len, d_model]

        # æœ€çµ‚ã‚¹ãƒ†ãƒƒãƒ—ï¼ˆã¾ãŸã¯å¹³å‡ï¼‰ã‚’ä½¿ç”¨
        final_repr = transformer_out[:, -1, :]  # [batch, d_model]

        # åˆ†é¡
        prob = self.classifier(final_repr)
        return prob

class PositionalEncoding(nn.Module):
    """Positional Encoding for Transformer"""

    def __init__(self, d_model, max_len=100):
        super().__init__()

        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model))

        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)

        self.register_buffer('pe', pe.unsqueeze(0))

    def forward(self, x):
        """
        Args:
            x: [batch, seq_len, d_model]
        """
        return x + self.pe[:, :x.size(1), :]
```

**æ„ç¾©**:
- æœ€æ–°ã®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã¨ã®æ¯”è¼ƒ
- Attentionæ©Ÿæ§‹ã®åŠ¹æœã‚’æ¤œè¨¼
- IRLãªã—ã§ã©ã“ã¾ã§è¡Œã‘ã‚‹ã‹

---

## ğŸ“Š Tier 3: ç™ºå±•çš„ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³

### 3.1 ç”Ÿå­˜åˆ†æï¼ˆSurvival Analysisï¼‰

**æ¦‚è¦**: æ™‚é–“ã‚’æ˜ç¤ºçš„ã«ãƒ¢ãƒ‡ãƒ«åŒ–

**å®Ÿè£…é›£æ˜“åº¦**: â˜…â˜…â˜…â˜…â˜†ï¼ˆã‚„ã‚„é›£ï¼‰

**å®Ÿè£…æ™‚é–“**: 3-4æ—¥

**æœŸå¾…æ€§èƒ½**: AUC-PR â‰ˆ 0.60-0.70

**æ‰‹æ³•**: Coxæ¯”ä¾‹ãƒã‚¶ãƒ¼ãƒ‰ãƒ¢ãƒ‡ãƒ«

**å®Ÿè£…**:
```python
from lifelines import CoxPHFitter
import pandas as pd

def survival_analysis_baseline(train_data, test_data):
    """ç”Ÿå­˜åˆ†æãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ï¼ˆCoxæ¯”ä¾‹ãƒã‚¶ãƒ¼ãƒ‰ãƒ¢ãƒ‡ãƒ«ï¼‰"""

    # ãƒ‡ãƒ¼ã‚¿å½¢å¼å¤‰æ›
    def prepare_survival_data(data):
        """
        ç¶™ç¶šäºˆæ¸¬ã‚’ç”Ÿå­˜åˆ†æã®å½¢å¼ã«å¤‰æ›

        duration: è¦³æ¸¬æœŸé–“ï¼ˆç¶™ç¶šã—ãŸå ´åˆã¯æ‰“ã¡åˆ‡ã‚Šï¼‰
        event: é›¢è„±ã—ãŸã‹ã©ã†ã‹ï¼ˆ0=ç¶™ç¶šä¸­, 1=é›¢è„±ï¼‰
        """
        survival_data = []
        for _, row in data.iterrows():
            survival_data.append({
                'duration': row['observation_months'],  # è¦³æ¸¬æœŸé–“
                'event': 0 if row['continued'] else 1,  # é›¢è„±=1
                'experience': row['experience_days'] / 730.0,
                'total_reviews': row['total_reviews'] / 500.0,
                'acceptance_rate': row['acceptance_rate'],
                'recent_activity': row['recent_activity_frequency'],
                'collaboration': row['collaboration_score']
            })
        return pd.DataFrame(survival_data)

    train_df = prepare_survival_data(train_data)
    test_df = prepare_survival_data(test_data)

    # Coxæ¯”ä¾‹ãƒã‚¶ãƒ¼ãƒ‰ãƒ¢ãƒ‡ãƒ«
    cph = CoxPHFitter()
    cph.fit(
        train_df,
        duration_col='duration',
        event_col='event'
    )

    # äºˆæ¸¬ï¼ˆç”Ÿå­˜ç¢ºç‡ï¼‰
    survival_probs = cph.predict_survival_function(test_df).iloc[-1].values

    return {
        'predictions': survival_probs,
        'model': cph,
        'hazard_ratios': cph.hazard_ratios_,  # å„ç‰¹å¾´é‡ã®ãƒã‚¶ãƒ¼ãƒ‰æ¯”
        'method': 'cox_ph'
    }
```

**æ„ç¾©**:
- æ™‚é–“ã‚’æ˜ç¤ºçš„ã«ãƒ¢ãƒ‡ãƒ«åŒ–
- åŒ»å­¦ãƒ»ä¿¡é ¼æ€§å·¥å­¦ã§æ¨™æº–çš„
- ãƒã‚¶ãƒ¼ãƒ‰æ¯”ã§è§£é‡ˆå¯èƒ½

---

### 3.2 ã‚°ãƒ©ãƒ•ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ï¼ˆGNNï¼‰

**æ¦‚è¦**: é–‹ç™ºè€…é–“ã®å”åŠ›ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’ãƒ¢ãƒ‡ãƒ«åŒ–

**å®Ÿè£…é›£æ˜“åº¦**: â˜…â˜…â˜…â˜…â˜…ï¼ˆé›£ï¼‰

**å®Ÿè£…æ™‚é–“**: 1é€±é–“

**æœŸå¾…æ€§èƒ½**: AUC-PR â‰ˆ 0.65-0.75

**æ‰‹æ³•**: GraphSAGE ã¾ãŸã¯ GAT

**å®Ÿè£…æ¦‚è¦**:
```python
import torch_geometric as pyg
from torch_geometric.nn import SAGEConv, GATConv

class DeveloperGNN(nn.Module):
    """é–‹ç™ºè€…ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®GNN"""

    def __init__(self, node_features=10, hidden_dim=128):
        super().__init__()

        # Graph Convolution Layers
        self.conv1 = SAGEConv(node_features, hidden_dim)
        self.conv2 = SAGEConv(hidden_dim, hidden_dim)

        # åˆ†é¡å™¨
        self.classifier = nn.Sequential(
            nn.Linear(hidden_dim, 64),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(64, 1),
            nn.Sigmoid()
        )

    def forward(self, x, edge_index):
        """
        Args:
            x: [num_nodes, node_features]
            edge_index: [2, num_edges]ï¼ˆå”åŠ›é–¢ä¿‚ã®ã‚¨ãƒƒã‚¸ï¼‰
        """
        # ã‚°ãƒ©ãƒ•ç•³ã¿è¾¼ã¿
        x = self.conv1(x, edge_index)
        x = torch.relu(x)
        x = self.conv2(x, edge_index)

        # åˆ†é¡
        prob = self.classifier(x)
        return prob

def build_collaboration_graph(data):
    """å”åŠ›ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚°ãƒ©ãƒ•ã‚’æ§‹ç¯‰"""

    # ãƒãƒ¼ãƒ‰: é–‹ç™ºè€…
    # ã‚¨ãƒƒã‚¸: å…±åŒã§ãƒ¬ãƒ“ãƒ¥ãƒ¼ã—ãŸé–¢ä¿‚

    developers = list(set(r['reviewer'] for r in data))
    dev_to_idx = {dev: i for i, dev in enumerate(developers)}

    # ãƒãƒ¼ãƒ‰ç‰¹å¾´é‡
    node_features = []
    for dev in developers:
        dev_data = [r for r in data if r['reviewer'] == dev]
        features = extract_developer_features(dev_data)
        node_features.append(features)

    # ã‚¨ãƒƒã‚¸ï¼ˆå”åŠ›é–¢ä¿‚ï¼‰
    edges = []
    for r in data:
        if 'collaborators' in r:
            for collab in r['collaborators']:
                if collab in dev_to_idx:
                    edges.append([dev_to_idx[r['reviewer']], dev_to_idx[collab]])

    return torch.tensor(node_features), torch.tensor(edges).t()
```

**æ„ç¾©**:
- ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯åŠ¹æœã‚’æ•æ‰
- å­¤ç«‹ã—ãŸé–‹ç™ºè€…ã®é›¢è„±ãƒªã‚¹ã‚¯ã‚’æ¤œå‡º
- æœ€å…ˆç«¯ã®ç ”ç©¶å‹•å‘

---

## ğŸ”¬ æ¯”è¼ƒå®Ÿé¨“è¨­è¨ˆ

### å®Ÿé¨“ãƒ—ãƒ­ãƒˆã‚³ãƒ«

```python
# scripts/experiments/baseline_comparison.py

def comprehensive_baseline_comparison(data, output_dir):
    """å…¨ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã¨ã®åŒ…æ‹¬çš„æ¯”è¼ƒ"""

    methods = {
        # Tier 1: å¿…é ˆ
        'random': random_baseline,
        'rule_based': rule_based_baseline,
        'logistic_regression': logistic_regression_baseline,

        # Tier 2: é‡è¦
        'xgboost': xgboost_baseline,
        'vanilla_lstm': vanilla_lstm_baseline,
        'transformer': transformer_baseline,

        # Tier 3: ç™ºå±•çš„
        'survival_analysis': survival_analysis_baseline,

        # ç¾åœ¨ã®ãƒ¢ãƒ‡ãƒ«
        'irl_lstm': current_irl_model
    }

    results = {}
    for name, method in methods.items():
        print(f"\n{'='*60}")
        print(f"è©•ä¾¡ä¸­: {name}")
        print(f"{'='*60}")

        # è¨“ç·´ãƒ»äºˆæ¸¬
        predictions = method(train_data, test_data)

        # è©•ä¾¡
        metrics = evaluate_predictions(
            y_true=test_data['continued'],
            y_pred=predictions['predictions']
        )

        results[name] = {
            'metrics': metrics,
            'predictions': predictions
        }

        print(f"AUC-PR: {metrics['auc_pr']:.3f}")
        print(f"AUC-ROC: {metrics['auc_roc']:.3f}")
        print(f"F1: {metrics['f1']:.3f}")

    # çµæœã‚’ä¿å­˜
    save_comparison_results(results, output_dir)

    # å¯è¦–åŒ–
    plot_comparison(results, output_dir)

    return results
```

### è©•ä¾¡æŒ‡æ¨™

ã™ã¹ã¦ã®ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã§ä»¥ä¸‹ã‚’æ¸¬å®šï¼š

| æŒ‡æ¨™ | èª¬æ˜ |
|------|------|
| **AUC-PR** | ãƒ¡ã‚¤ãƒ³æŒ‡æ¨™ï¼ˆä¸å‡è¡¡ãƒ‡ãƒ¼ã‚¿ï¼‰ |
| **AUC-ROC** | è£œåŠ©æŒ‡æ¨™ |
| **F1ã‚¹ã‚³ã‚¢** | Precision/Recallã®ãƒãƒ©ãƒ³ã‚¹ |
| **Precision** | ç¶™ç¶šäºˆæ¸¬ã®æ­£è§£ç‡ |
| **Recall** | å®Ÿéš›ã®ç¶™ç¶šè€…ã®æ•æ‰ç‡ |
| **è¨“ç·´æ™‚é–“** | ãƒ¢ãƒ‡ãƒ«è¨“ç·´ã®æ‰€è¦æ™‚é–“ |
| **æ¨è«–æ™‚é–“** | 1ã‚µãƒ³ãƒ—ãƒ«ã‚ãŸã‚Šã®äºˆæ¸¬æ™‚é–“ |
| **ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚º** | ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ |

### çµ±è¨ˆçš„æ¤œå®š

```python
from scipy.stats import wilcoxon

def statistical_test(irl_predictions, baseline_predictions, y_true):
    """
    Wilcoxonç¬¦å·é †ä½æ¤œå®šã§IRLã¨ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã®æ€§èƒ½å·®ã‚’æ¤œå®š
    """

    # å„ã‚µãƒ³ãƒ—ãƒ«ã§ã®AUC-PRå·®ã‚’è¨ˆç®—ï¼ˆãƒ–ãƒ¼ãƒˆã‚¹ãƒˆãƒ©ãƒƒãƒ—ï¼‰
    n_bootstrap = 1000
    irl_scores = []
    baseline_scores = []

    for _ in range(n_bootstrap):
        # ãƒ–ãƒ¼ãƒˆã‚¹ãƒˆãƒ©ãƒƒãƒ—ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
        indices = np.random.choice(len(y_true), len(y_true), replace=True)

        irl_auc = average_precision_score(
            y_true[indices],
            irl_predictions[indices]
        )
        baseline_auc = average_precision_score(
            y_true[indices],
            baseline_predictions[indices]
        )

        irl_scores.append(irl_auc)
        baseline_scores.append(baseline_auc)

    # Wilcoxonæ¤œå®š
    statistic, p_value = wilcoxon(irl_scores, baseline_scores)

    return {
        'statistic': statistic,
        'p_value': p_value,
        'significant': p_value < 0.05,
        'irl_mean': np.mean(irl_scores),
        'baseline_mean': np.mean(baseline_scores),
        'improvement': np.mean(irl_scores) - np.mean(baseline_scores)
    }
```

---

## ğŸ“ˆ å¯è¦–åŒ–

### æ€§èƒ½æ¯”è¼ƒã‚°ãƒ©ãƒ•

```python
import matplotlib.pyplot as plt
import seaborn as sns

def plot_comparison(results, output_dir):
    """ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³æ¯”è¼ƒã®å¯è¦–åŒ–"""

    # 1. æ£’ã‚°ãƒ©ãƒ•ï¼ˆAUC-PRæ¯”è¼ƒï¼‰
    fig, axes = plt.subplots(2, 2, figsize=(15, 12))

    methods = list(results.keys())
    metrics = ['auc_pr', 'auc_roc', 'f1', 'precision']
    titles = ['AUC-PR', 'AUC-ROC', 'F1 Score', 'Precision']

    for idx, (metric, title) in enumerate(zip(metrics, titles)):
        ax = axes[idx // 2, idx % 2]

        values = [results[m]['metrics'][metric] for m in methods]
        colors = ['red' if m == 'irl_lstm' else 'skyblue' for m in methods]

        ax.barh(methods, values, color=colors)
        ax.set_xlabel(title)
        ax.set_xlim(0, 1.0)
        ax.axvline(x=0.5, color='gray', linestyle='--', alpha=0.5)

        # å€¤ã‚’ãƒ©ãƒ™ãƒ«è¡¨ç¤º
        for i, v in enumerate(values):
            ax.text(v + 0.02, i, f'{v:.3f}', va='center')

    plt.tight_layout()
    plt.savefig(output_dir / 'baseline_comparison_bars.png', dpi=300)

    # 2. ãƒ¬ãƒ¼ãƒ€ãƒ¼ãƒãƒ£ãƒ¼ãƒˆ
    fig, ax = plt.subplots(figsize=(10, 10), subplot_kw=dict(projection='polar'))

    categories = ['AUC-PR', 'AUC-ROC', 'F1', 'Precision', 'Recall']
    angles = np.linspace(0, 2 * np.pi, len(categories), endpoint=False).tolist()
    angles += angles[:1]

    for method in ['irl_lstm', 'xgboost', 'vanilla_lstm']:
        values = [
            results[method]['metrics']['auc_pr'],
            results[method]['metrics']['auc_roc'],
            results[method]['metrics']['f1'],
            results[method]['metrics']['precision'],
            results[method]['metrics']['recall']
        ]
        values += values[:1]

        ax.plot(angles, values, 'o-', linewidth=2, label=method)
        ax.fill(angles, values, alpha=0.25)

    ax.set_xticks(angles[:-1])
    ax.set_xticklabels(categories)
    ax.set_ylim(0, 1.0)
    ax.legend(loc='upper right', bbox_to_anchor=(1.3, 1.1))
    ax.set_title('æ€§èƒ½æ¯”è¼ƒï¼ˆãƒ¬ãƒ¼ãƒ€ãƒ¼ãƒãƒ£ãƒ¼ãƒˆï¼‰')

    plt.savefig(output_dir / 'baseline_comparison_radar.png', dpi=300)
```

---

## ğŸ¯ æ¨å¥¨å®Ÿé¨“ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«

### ãƒ•ã‚§ãƒ¼ã‚º1: å¿…é ˆãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ï¼ˆ1é€±é–“ï¼‰

| æ—¥ | ã‚¿ã‚¹ã‚¯ | æœŸå¾…æ‰€è¦æ™‚é–“ | çŠ¶æ…‹ |
|----|--------|-------------|------|
| 1 | ãƒ©ãƒ³ãƒ€ãƒ ãƒ»ãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹å®Ÿè£… | 2æ™‚é–“ | â¬œ |
| 2 | ãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯å›å¸°å®Ÿè£… | 3æ™‚é–“ | âœ… **å®Œäº†** |
| 3 | ãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆå®Ÿè£… | 2æ™‚é–“ | âœ… **å®Œäº†** |
| 4-5 | XGBoostå®Ÿè£…ãƒ»ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚° | 8æ™‚é–“ | â¬œ |
| 6-7 | Vanilla LSTMå®Ÿè£…ãƒ»è¨“ç·´ | 16æ™‚é–“ | â¬œ |

### ãƒ•ã‚§ãƒ¼ã‚º2: ç™ºå±•çš„ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ï¼ˆ2é€±é–“ï¼‰

| é€± | ã‚¿ã‚¹ã‚¯ |
|----|--------|
| 1 | Transformerå®Ÿè£… |
| 2 | ç”Ÿå­˜åˆ†æå®Ÿè£… |

### ãƒ•ã‚§ãƒ¼ã‚º3: è«–æ–‡åŸ·ç­†ï¼ˆ1é€±é–“ï¼‰

- çµæœåˆ†æ
- çµ±è¨ˆçš„æ¤œå®š
- å¯è¦–åŒ–
- è€ƒå¯ŸåŸ·ç­†

---

## ğŸ“Š æœŸå¾…ã•ã‚Œã‚‹çµæœ

### äºˆæƒ³ã•ã‚Œã‚‹æ€§èƒ½é †ä½ï¼ˆAUC-PRåŸºæº–ï¼‰

1. **XGBoost**: 0.65-0.75ï¼ˆæœ€å¼·å€™è£œï¼‰
2. **IRL+LSTM**: **0.718**ï¼ˆç¾åœ¨ã®æœ€è‰¯ï¼‰
3. **Vanilla LSTM**: 0.65-0.72
4. **Transformer**: 0.68-0.75
5. **ãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆ**: 0.60-0.70 âœ… **å®Ÿè£…æ¸ˆã¿**
6. **ãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯å›å¸°**: 0.55-0.65 âœ… **å®Ÿè£…æ¸ˆã¿**
7. **ç”Ÿå­˜åˆ†æ**: 0.60-0.70
8. **ãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹**: 0.45-0.55
9. **ãƒ©ãƒ³ãƒ€ãƒ **: 0.35

### é‡è¦ãªæ¯”è¼ƒ

**æ¯”è¼ƒ1: IRL+LSTM vs Vanilla LSTM**
- **ç›®çš„**: IRLã®ä¾¡å€¤ã‚’ç›´æ¥æ¸¬å®š
- **ã‚‚ã—å·®ãŒå°ã•ã‘ã‚Œã°**: IRLã¯ä¸è¦ï¼ˆå˜ç´”ãªLSTMã§ååˆ†ï¼‰

**æ¯”è¼ƒ2: IRL+LSTM vs XGBoost**
- **ç›®çš„**: æ™‚ç³»åˆ—ãƒ¢ãƒ‡ãƒªãƒ³ã‚°ã®ä¾¡å€¤ã‚’æ¸¬å®š
- **ã‚‚ã—XGBoostãŒå‹ã¦ã°**: æ™‚ç³»åˆ—é›†ç´„ã§ååˆ†ï¼ˆLSTMã¯ä¸è¦ï¼‰

**æ¯”è¼ƒ3: IRL+LSTM vs ãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆ** âœ… **å®Ÿæ–½å¯èƒ½**
- **ç›®çš„**: éç·šå½¢ãƒ¢ãƒ‡ãƒ«ã¨ã®æ¯”è¼ƒ
- **ãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆã®åˆ©ç‚¹**: å®Ÿè£…ãŒç°¡å˜ã€è§£é‡ˆæ€§ãŒé«˜ã„
- **æœŸå¾…**: IRL+LSTMãŒ10-15%ä¸Šå›ã‚‹ã“ã¨ã‚’æœŸå¾…

**æ¯”è¼ƒ4: ãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯å›å¸° vs ãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆ** âœ… **å®Ÿæ–½å¯èƒ½**
- **ç›®çš„**: ç·šå½¢ vs éç·šå½¢ã®åŠ¹æœã‚’æ¸¬å®š
- **æœŸå¾…**: éç·šå½¢ï¼ˆRFï¼‰ãŒ5-10%ä¸Šå›ã‚‹

**æ¯”è¼ƒ5: LSTM vs Transformer**
- **ç›®çš„**: ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã®é¸æŠã‚’æ¤œè¨¼

---

## ğŸ’¡ è«–æ–‡ã¸ã®è¨˜è¼‰ä¾‹

### Result Section

```markdown
### Baseline Comparison

We compare our IRL+LSTM model against 7 baselines:

**Table: Performance Comparison**

| Method | AUC-PR | AUC-ROC | F1 | Training Time |
|--------|--------|---------|-----|---------------|
| Random | 0.350 | 0.500 | 0.400 | - |
| Rule-based | 0.520 | 0.620 | 0.550 | - |
| Logistic Regression | 0.610 | 0.680 | 0.630 | 2 min |
| Random Forest | 0.650 | 0.720 | 0.660 | 5 min |
| XGBoost | **0.740** | **0.810** | **0.720** | 15 min |
| Vanilla LSTM | 0.680 | 0.750 | 0.670 | 45 min |
| Transformer | 0.705 | 0.770 | 0.690 | 60 min |
| **IRL+LSTM (Ours)** | **0.718** | 0.754 | 0.636 | 50 min |

Our IRL+LSTM model achieves competitive performance with XGBoost
while providing interpretable reward functions. The improvement over
Vanilla LSTM (+3.8% AUC-PR) demonstrates the value of incorporating
inverse reinforcement learning.
```

---

## ğŸš€ å®Ÿè£…ã‚³ãƒãƒ³ãƒ‰

### ãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯å›å¸°ã¨ãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆã®å®Ÿè¡Œ âœ… **å®Ÿè£…æ¸ˆã¿**

```bash
# ãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯å›å¸°ã¨ãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆã‚’å®Ÿè¡Œ
uv run python scripts/experiments/run_baseline_comparison.py \
  --reviews data/review_requests_no_bots.csv \
  --snapshot-date 2020-01-01 \
  --history-months 12 \
  --target-months 6 \
  --baselines logistic_regression random_forest \
  --output importants/baseline_experiments/

# ãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯å›å¸°ã®ã¿
uv run python scripts/experiments/run_baseline_comparison.py \
  --reviews data/review_requests_no_bots.csv \
  --snapshot-date 2020-01-01 \
  --history-months 12 \
  --target-months 6 \
  --baselines logistic_regression \
  --output importants/baseline_experiments/logistic_regression/

# ãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆã®ã¿
uv run python scripts/experiments/run_baseline_comparison.py \
  --reviews data/review_requests_no_bots.csv \
  --snapshot-date 2020-01-01 \
  --history-months 12 \
  --target-months 6 \
  --baselines random_forest \
  --output importants/baseline_experiments/random_forest/
```

### ä»Šå¾Œå®Ÿè£…äºˆå®šã®ã‚³ãƒãƒ³ãƒ‰

```bash
# å…¨ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã‚’ä¸€æ‹¬å®Ÿè¡Œï¼ˆä»Šå¾Œï¼‰
uv run python scripts/experiments/run_baseline_comparison.py \
  --reviews data/review_requests_no_bots.csv \
  --snapshot-date 2020-01-01 \
  --history-months 12 \
  --target-months 6 \
  --baselines logistic_regression random_forest xgboost vanilla_lstm \
  --output importants/baseline_experiments/comparison_results/

# çµæœã®å¯è¦–åŒ–ï¼ˆä»Šå¾Œå®Ÿè£…ï¼‰
uv run python scripts/experiments/visualize_baseline_comparison.py \
  --input importants/baseline_experiments/comparison_results/ \
  --output importants/baseline_experiments/comparison_results/figures/

# çµ±è¨ˆçš„æ¤œå®šï¼ˆä»Šå¾Œå®Ÿè£…ï¼‰
uv run python scripts/experiments/statistical_test.py \
  --irl importants/irl_openstack_real/models/irl_h12m_t6m_seq.pth \
  --baselines importants/baseline_experiments/comparison_results/ \
  --output importants/baseline_experiments/comparison_results/statistical_test.json
```

---

## ğŸ“ ã¾ã¨ã‚

### å®Ÿè£…æ¸ˆã¿ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ âœ…

1. **ãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯å›å¸°** âœ…
   - ç·šå½¢ãƒ¢ãƒ‡ãƒ«ã®ä»£è¡¨
   - å®Ÿè£…å®Œäº†ã€ã™ãã«å®Ÿé¨“å¯èƒ½
   - è«–æ–‡ã§å¿…é ˆã®ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³

2. **ãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆ** âœ…
   - éç·šå½¢ãƒ¢ãƒ‡ãƒ«ã®ä»£è¡¨
   - å®Ÿè£…å®Œäº†ã€ã™ãã«å®Ÿé¨“å¯èƒ½
   - XGBoostã‚ˆã‚Šç°¡å˜ã§å†ç¾æ€§ãŒé«˜ã„

### ä»Šå¾Œå®Ÿè£…ã™ã¹ããƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³

3. **XGBoost**: æœ€å¼·å€™è£œã€IRLã®å„ªä½æ€§ã‚’ç¤ºã™ãŸã‚ã«å¿…é ˆ
4. **Vanilla LSTM**: IRLã®ä¾¡å€¤ã‚’ç›´æ¥æ¸¬å®šï¼ˆæœ€é‡è¦æ¯”è¼ƒï¼‰

### ã“ã‚Œã‚‰ã‚’å®Ÿè£…ã™ã‚Œã°

- âœ… ç·šå½¢ vs éç·šå½¢ã®æ¯”è¼ƒãŒå¯èƒ½ï¼ˆãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯å›å¸° vs ãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆï¼‰
- âœ… è«–æ–‡ã®ä¿¡é ¼æ€§ãŒå‘ä¸Šï¼ˆæ©Ÿæ¢°å­¦ç¿’ã®æ¨™æº–ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã¨ã®æ¯”è¼ƒï¼‰
- â¬œ IRLã®è²¢çŒ®ã‚’æ˜ç¢ºåŒ–ï¼ˆVanilla LSTMã¨ã®æ¯”è¼ƒãŒå¿…è¦ï¼‰
- â¬œ æ™‚ç³»åˆ—ãƒ¢ãƒ‡ãƒªãƒ³ã‚°ã®ä¾¡å€¤ã‚’è¨¼æ˜ï¼ˆXGBoostã¨ã®æ¯”è¼ƒãŒå¿…è¦ï¼‰

### æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—

1. **ã™ãã«å®Ÿè¡Œå¯èƒ½**: ãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯å›å¸°ã¨ãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆã§åˆå›å®Ÿé¨“
2. **å„ªå…ˆåº¦é«˜**: XGBoostå®Ÿè£…ï¼ˆIRLã¨ã®æ€§èƒ½å·®ã‚’ç¢ºèªï¼‰
3. **æœ€é‡è¦**: Vanilla LSTMå®Ÿè£…ï¼ˆIRLã®ä¾¡å€¤ã‚’è¨¼æ˜ï¼‰

---

**ä½œæˆæ—¥**: 2025å¹´11æœˆ4æ—¥
**æœ€çµ‚æ›´æ–°**: 2025å¹´11æœˆ4æ—¥ï¼ˆãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆè¿½åŠ ï¼‰
