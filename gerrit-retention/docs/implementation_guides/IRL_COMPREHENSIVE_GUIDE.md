# é–‹ç™ºè€…ç¶™ç¶šäºˆæ¸¬ã®ãŸã‚ã®é€†å¼·åŒ–å­¦ç¿’ï¼ˆIRLï¼‰ã‚·ã‚¹ãƒ†ãƒ ï¼šåŒ…æ‹¬çš„ã‚¬ã‚¤ãƒ‰

**æœ€çµ‚æ›´æ–°**: 2025-10-17
**ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ**: Gerrit Retention IRL
**ãƒ‡ãƒ¼ã‚¿**: OpenStack Gerrit (137,632 reviews, 13 years, 1,379 developers)

---

## ğŸ“‹ ç›®æ¬¡

1. [ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæ¦‚è¦](#1-ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæ¦‚è¦)
2. [é€†å¼·åŒ–å­¦ç¿’ï¼ˆIRLï¼‰ã®è©³ç´°è§£èª¬](#2-é€†å¼·åŒ–å­¦ç¿’irlã®è©³ç´°è§£èª¬)
3. [ç‰¹å¾´é‡è¨­è¨ˆ](#3-ç‰¹å¾´é‡è¨­è¨ˆ)
4. [å®Ÿè£…ã®è©³ç´°](#4-å®Ÿè£…ã®è©³ç´°)
5. [å®Ÿé¨“çµæœ](#5-å®Ÿé¨“çµæœ)
6. [æŠ€è¡“çš„èª²é¡Œã¨è§£æ±ºç­–](#6-æŠ€è¡“çš„èª²é¡Œã¨è§£æ±ºç­–)
7. [é‹ç”¨ã‚¬ã‚¤ãƒ‰](#7-é‹ç”¨ã‚¬ã‚¤ãƒ‰)
8. [æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—](#8-æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—)

---

## 1. ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæ¦‚è¦

### 1.1 æœ€çµ‚ç›®çš„

**ã€ŒOSSãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã§ã€ã“ã®é–‹ç™ºè€…ã¯6ãƒ¶æœˆå¾Œã‚‚è²¢çŒ®ã‚’ç¶šã‘ã¦ã„ã‚‹ã‹ï¼Ÿã€**

```
ã€ç¾å®Ÿã®èª²é¡Œã€‘
- å„ªç§€ãªé–‹ç™ºè€…ãŒçªç„¶é›¢è„±ã—ã¦ã—ã¾ã†ï¼ˆãƒªãƒ†ãƒ³ã‚·ãƒ§ãƒ³å•é¡Œï¼‰
- ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ã¯èª°ãŒæ®‹ã‚Šç¶šã‘ã‚‹ã‹äºˆæ¸¬ã§ããªã„
- æ—©æœŸã«é›¢è„±ãƒªã‚¹ã‚¯ã‚’æ¤œçŸ¥ã§ãã‚Œã°ä»‹å…¥å¯èƒ½

ã€ç›®æ¨™ã€‘
é–‹ç™ºè€…ã®éå»ã®è¡Œå‹•å±¥æ­´ã‹ã‚‰
 â†’ ç¶™ç¶šç¢ºç‡ã‚’äºˆæ¸¬ (0-1ã®ç¢ºç‡å€¤)
 â†’ é›¢è„±ãƒªã‚¹ã‚¯ã®æ—©æœŸæ¤œçŸ¥
 â†’ ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆé‹å–¶ã®æœ€é©åŒ–
```

### 1.2 ãªãœé€†å¼·åŒ–å­¦ç¿’ï¼ˆIRLï¼‰ãªã®ã‹ï¼Ÿ

| ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ | æ–¹æ³• | å•é¡Œç‚¹ |
|----------|------|--------|
| **æ•™å¸«ã‚ã‚Šå­¦ç¿’** | ç‰¹å¾´é‡ â†’ ç¶™ç¶š/é›¢è„± | ã€Œãªãœã€ç¶™ç¶šã—ãŸã‹åˆ†ã‹ã‚‰ãªã„ |
| | | å ±é…¬æ§‹é€ ãŒä¸æ˜ |
| **å¼·åŒ–å­¦ç¿’ï¼ˆRLï¼‰** | å ±é…¬é–¢æ•°ã‚’å®šç¾© â†’ æœ€é©è¡Œå‹•å­¦ç¿’ | å ±é…¬é–¢æ•°ã‚’äººé–“ãŒè¨­è¨ˆã™ã‚‹å¿…è¦ |
| | | æ­£è§£ãŒåˆ†ã‹ã‚‰ãªã„ |
| **é€†å¼·åŒ–å­¦ç¿’ï¼ˆIRLï¼‰** âœ… | ç¶™ç¶šã—ãŸäººã®è¡Œå‹• â†’ å ±é…¬é–¢æ•°æ¨å®š | **å°‚é–€å®¶ã®æš—é»™çŸ¥ã‚’å­¦ç¿’** |
| | | **ã€Œãªãœã€ãŒåˆ†ã‹ã‚‹** |

### 1.3 æ ¸å¿ƒçš„ã‚¢ã‚¤ãƒ‡ã‚¢

```
é€šå¸¸ã®RL: å ±é…¬é–¢æ•°ãŒåˆ†ã‹ã£ã¦ã„ã‚‹ â†’ æœ€é©ãªè¡Œå‹•ã‚’å­¦ç¿’
     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â”‚ å ±é…¬é–¢æ•° â”‚ (æ—¢çŸ¥)
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â†“
     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â”‚ ãƒãƒªã‚·ãƒ¼ â”‚ (å­¦ç¿’å¯¾è±¡)
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

IRL: å°‚é–€å®¶ã®è¡Œå‹•ãŒåˆ†ã‹ã£ã¦ã„ã‚‹ â†’ å ±é…¬é–¢æ•°ã‚’é€†ç®—
     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â”‚ å°‚é–€å®¶ã® â”‚ (æ—¢çŸ¥: ç¶™ç¶šã—ãŸé–‹ç™ºè€…ã®è¡Œå‹•)
     â”‚ è»Œè·¡    â”‚
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â†“
     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â”‚ å ±é…¬é–¢æ•° â”‚ (å­¦ç¿’å¯¾è±¡: ä½•ãŒç¶™ç¶šã«å¯„ä¸ã™ã‚‹ã‹)
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 2. é€†å¼·åŒ–å­¦ç¿’ï¼ˆIRLï¼‰ã®è©³ç´°è§£èª¬

### 2.1 å…¨ä½“ã®æµã‚Œï¼ˆ5ã‚¹ãƒ†ãƒƒãƒ—ï¼‰

```
ã‚¹ãƒ†ãƒƒãƒ—1: ãƒ‡ãƒ¼ã‚¿æº–å‚™ï¼ˆè»Œè·¡æŠ½å‡ºï¼‰
   â†“
ã‚¹ãƒ†ãƒƒãƒ—2: çŠ¶æ…‹ã¨è¡Œå‹•ã®æŠ½å‡º
   â†“
ã‚¹ãƒ†ãƒƒãƒ—3: ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã§å ±é…¬ã¨ç¶™ç¶šç¢ºç‡ã‚’äºˆæ¸¬
   â†“
ã‚¹ãƒ†ãƒƒãƒ—4: æå¤±è¨ˆç®—ã¨å­¦ç¿’
   â†“
ã‚¹ãƒ†ãƒƒãƒ—5: äºˆæ¸¬ï¼ˆæ¨è«–ï¼‰
```

### 2.2 ã‚¹ãƒ†ãƒƒãƒ—1: ãƒ‡ãƒ¼ã‚¿æº–å‚™ï¼ˆè»Œè·¡æŠ½å‡ºï¼‰

**è»Œè·¡ï¼ˆTrajectoryï¼‰** = é–‹ç™ºè€…ã®éå»ã®æ´»å‹•è¨˜éŒ² + ç¶™ç¶šãƒ©ãƒ™ãƒ«

```python
# OpenStack Gerrit ã®ãƒ¬ãƒ“ãƒ¥ãƒ¼ãƒ­ã‚°ã‹ã‚‰è»Œè·¡ã‚’ä½œæˆ

ã€å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ä¾‹ã€‘
reviewer_email: alice@example.com
æ´»å‹•è¨˜éŒ²:
  2022-01-05: ã‚³ãƒŸãƒƒãƒˆ (400è¡Œå¤‰æ›´, 5ãƒ•ã‚¡ã‚¤ãƒ«)
  2022-01-12: ãƒ¬ãƒ“ãƒ¥ãƒ¼ (å¿œç­”é…å»¶3æ—¥)
  2022-01-18: ãƒãƒ¼ã‚¸ (200è¡Œå¤‰æ›´)
  ...
  2022-12-20: ã‚³ãƒŸãƒƒãƒˆ (100è¡Œå¤‰æ›´)

ã€è»Œè·¡ã®ä½œæˆã€‘
ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆæ—¥: 2023-01-01
å­¦ç¿’æœŸé–“: 12ãƒ¶æœˆå‰ã¾ã§ (2022-01-01 ~ 2023-01-01)
äºˆæ¸¬æœŸé–“: 6ãƒ¶æœˆå¾Œã¾ã§ (2023-01-01 ~ 2023-07-01)

trajectory = {
    'developer': {
        'developer_id': 'alice@example.com',
        'experience_days': 730,
        'total_changes': 120,
        'review_load_7d': 2.5,
        ...  # çŠ¶æ…‹ç‰¹å¾´é‡32æ¬¡å…ƒ
    },
    'activity_history': [
        {'type': 'commit', 'change_size': 400, ...},
        {'type': 'review', 'response_latency': 3, ...},
        ...  # 15å€‹ã®è¡Œå‹•
    ],
    'continued': True,  # â† ã“ã‚ŒãŒç­”ãˆï¼ˆãƒ©ãƒ™ãƒ«ï¼‰
    'context_date': datetime(2023, 1, 1)
}
```

**æ™‚ç³»åˆ—ã®ã‚¤ãƒ¡ãƒ¼ã‚¸**:

```
      å­¦ç¿’æœŸé–“ (12ãƒ¶æœˆ)              äºˆæ¸¬æœŸé–“ (6ãƒ¶æœˆ)
â†â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’   â†â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’
[===================]ğŸ“¸[????????????????]
  ã“ã®æœŸé–“ã®è¡Œå‹•ã‚’è¦³å¯Ÿ     ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆ    ã“ã®æœŸé–“ã«ç¶™ç¶šã—ãŸã‹ï¼Ÿ
                           2023-01-01
```

### 2.3 ã‚¹ãƒ†ãƒƒãƒ—2: çŠ¶æ…‹ã¨è¡Œå‹•ã®æŠ½å‡º

**çŠ¶æ…‹ï¼ˆStateï¼‰** = é–‹ç™ºè€…ã®**ç¾åœ¨ã®çŠ¶æ³**ï¼ˆ32æ¬¡å…ƒï¼‰
**è¡Œå‹•ï¼ˆActionï¼‰** = é–‹ç™ºè€…ã®**æœ€è¿‘ã®æ´»å‹•**ï¼ˆ9æ¬¡å…ƒ Ã— 15å€‹ï¼‰

```python
# çŠ¶æ…‹æŠ½å‡ºï¼ˆé–‹ç™ºè€…ã®ç¾åœ¨ã®çŠ¶æ³ï¼‰
state = [
    365,      # experience_days
    120,      # total_changes
    80,       # total_reviews
    2.5,      # review_load_7d
    0.6,      # activity_freq_30d
    ...       # ä»–27æ¬¡å…ƒ
]

# è¡Œå‹•æŠ½å‡ºï¼ˆé–‹ç™ºè€…ã®æœ€è¿‘ã®æ´»å‹•ï¼‰
actions = [
    [1.0, 0.8, 0.7, 0.9, 5, 400, 5, 80, 0],  # è¡Œå‹•1 (5æ—¥å‰ã®ã‚³ãƒŸãƒƒãƒˆ)
    [0.8, 0.5, 0.6, 0.8, 3, 0, 0, 0, 3],     # è¡Œå‹•2 (3æ—¥å‰ã®ãƒ¬ãƒ“ãƒ¥ãƒ¼)
    ...  # 15å€‹
]
```

**åŒ»ç™‚ã®æ¯”å–©**:

```
ã€çŠ¶æ…‹ = æ‚£è€…ã®ä½“è³ªãƒ»æ—¢å¾€æ­´ã€‘
- å¹´é½¢: 35æ­³ (experience_days)
- BMI: 22 (total_changes / experience_days)
- è¡€åœ§: 120/80 (review_load_7d)
- é‹å‹•ç¿’æ…£: é€±3å› (activity_freq_30d)

ã€è¡Œå‹• = æœ€è¿‘ã®ç”Ÿæ´»ç¿’æ…£ã€‘
- 5æ—¥å‰: ã‚¸ãƒ§ã‚®ãƒ³ã‚°10km (é«˜å¼·åº¦ã®é‹å‹•)
- 3æ—¥å‰: è»½ã„ã‚¹ãƒˆãƒ¬ãƒƒãƒ (ä½å¼·åº¦ã®é‹å‹•)
- 1æ—¥å‰: ä¼‘é¤Š (æ´»å‹•ãªã—)

ã€ç›®çš„ã€‘
ã“ã®æ‚£è€…ã¯6ãƒ¶æœˆå¾Œã‚‚å¥åº·çš„ãªç”Ÿæ´»ã‚’ç¶šã‘ã¦ã„ã‚‹ã‹ï¼Ÿ
```

### 2.4 ã‚¹ãƒ†ãƒƒãƒ—3: ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã§äºˆæ¸¬

**ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ§‹é€ **:

```
å…¥åŠ›: çŠ¶æ…‹32æ¬¡å…ƒ + è¡Œå‹•ç³»åˆ—15Ã—9æ¬¡å…ƒ

    çŠ¶æ…‹ [32æ¬¡å…ƒ]              è¡Œå‹•ç³»åˆ— [15Ã—9æ¬¡å…ƒ]
         â†“                          â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ çŠ¶æ…‹    â”‚               â”‚ è¡Œå‹•    â”‚
    â”‚ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€â”‚               â”‚ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€â”‚
    â”‚ 32â†’64  â”‚               â”‚ 9â†’64   â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“                          â†“
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â†“ åŠ ç®—
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚   LSTM   â”‚ â† æ™‚ç³»åˆ—ãƒ‘ã‚¿ãƒ¼ãƒ³å­¦ç¿’
              â”‚ 64â†’128   â”‚    (æ´»å‹•æ¸›å°‘? å¿œç­”é…å»¶å¢—åŠ ?)
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â†“ æœ€å¾Œã®ã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ— [128æ¬¡å…ƒ]
              â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”
              â†“              â†“
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚  å ±é…¬    â”‚   â”‚  ç¶™ç¶š    â”‚
        â”‚äºˆæ¸¬å™¨    â”‚   â”‚ç¢ºç‡äºˆæ¸¬å™¨ â”‚
        â”‚128â†’1    â”‚   â”‚128â†’1    â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â†“              â†“
          å ±é…¬ã‚¹ã‚³ã‚¢    ç¶™ç¶šç¢ºç‡
          (0.8)        (0.75 = 75%)
```

**2ã¤ã®å‡ºåŠ›ã®æ„å‘³**:

1. **å ±é…¬ã‚¹ã‚³ã‚¢**: ã“ã®çŠ¶æ…‹ãƒ»è¡Œå‹•ãƒ‘ã‚¿ãƒ¼ãƒ³ã®ã€Œè‰¯ã•ã€
   - é«˜ã„å ±é…¬ = ç¶™ç¶šã«å¯„ä¸ã™ã‚‹è¡Œå‹•ãƒ‘ã‚¿ãƒ¼ãƒ³
   - ä¾‹: 0.8 = ã‹ãªã‚Šè‰¯ã„ãƒ‘ã‚¿ãƒ¼ãƒ³

2. **ç¶™ç¶šç¢ºç‡**: 6ãƒ¶æœˆå¾Œã‚‚è²¢çŒ®ã—ã¦ã„ã‚‹ç¢ºç‡
   - ç›´æ¥çš„ãªäºˆæ¸¬å€¤
   - ä¾‹: 0.75 = 75%ã®ç¢ºç‡ã§ç¶™ç¶š

### 2.5 ã‚¹ãƒ†ãƒƒãƒ—4: æå¤±è¨ˆç®—ã¨å­¦ç¿’

```python
# äºˆæ¸¬å®Ÿè¡Œ
predicted_reward, predicted_continuation = network(state, action)

# æ­£è§£ãƒ©ãƒ™ãƒ«ï¼ˆå®Ÿéš›ã«ç¶™ç¶šã—ãŸã‹ï¼‰
target = 1.0 if continued else 0.0

# æå¤±è¨ˆç®—
reward_loss = MSE(predicted_reward, target)
continuation_loss = BCE(predicted_continuation, target)

total_loss = reward_loss + continuation_loss

# ãƒãƒƒã‚¯ãƒ—ãƒ­ãƒ‘ã‚²ãƒ¼ã‚·ãƒ§ãƒ³
optimizer.zero_grad()
total_loss.backward()
optimizer.step()
```

**å­¦ç¿’ãƒ—ãƒ­ã‚»ã‚¹**:

```
Epoch 0:  å¹³å‡æå¤± = 0.527  (ã¾ã å…¨ç„¶åˆ†ã‹ã£ã¦ãªã„)
Epoch 5:  å¹³å‡æå¤± = 0.450  (å°‘ã—ãƒ‘ã‚¿ãƒ¼ãƒ³ãŒè¦‹ãˆã¦ããŸ)
Epoch 10: å¹³å‡æå¤± = 0.380  (ã ã„ã¶ç†è§£ã—ã¦ããŸ)
Epoch 20: å¹³å‡æå¤± = 0.335  (ã‹ãªã‚Šæ­£ç¢ºã«äºˆæ¸¬ã§ãã‚‹)
Epoch 30: å¹³å‡æå¤± = 0.336  (åæŸã—ãŸï¼)
```

### 2.6 ã‚¹ãƒ†ãƒƒãƒ—5: äºˆæ¸¬ï¼ˆæ¨è«–ï¼‰

```python
# æ–°ã—ã„é–‹ç™ºè€…ã®ç¶™ç¶šç¢ºç‡ã‚’äºˆæ¸¬
result = irl_system.predict_continuation_probability(
    developer=developer_data,
    activity_history=recent_activities,
    context_date=datetime(2023, 1, 1)
)

# å‡ºåŠ›ä¾‹
{
    'continuation_probability': 0.75,  # 75%ã®ç¢ºç‡ã§ç¶™ç¶š
    'confidence': 0.50,  # äºˆæ¸¬ã®ä¿¡é ¼åº¦
    'reasoning': 'å®šæœŸçš„ãªæ´»å‹•ã¨é©åº¦ãªè² è·ã«ã‚ˆã‚Šç¶™ç¶šå¯èƒ½æ€§ãŒé«˜ã„'
}
```

**å®Ÿç”¨ä¾‹**:

```
ã€æ–°ã—ã„é–‹ç™ºè€… Charlie ã®åˆ†æã€‘

å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ (2023-01-01æ™‚ç‚¹):
  çŠ¶æ…‹:
    - experience_days: 180 (6ãƒ¶æœˆ)
    - total_changes: 45
    - review_load_7d: 4.5 (é«˜è² è·ï¼)
    - activity_freq_30d: 0.4 (æ´»å‹•æ¸›å°‘å‚¾å‘)

  æœ€è¿‘ã®è¡Œå‹•:
    - 10æ—¥å‰: ã‚³ãƒŸãƒƒãƒˆ (å¤§è¦æ¨¡å¤‰æ›´)
    - 7æ—¥å‰: ãƒ¬ãƒ“ãƒ¥ãƒ¼ (å¿œç­”é…å»¶5æ—¥)
    - 5æ—¥å‰: ã‚³ãƒŸãƒƒãƒˆ (å°è¦æ¨¡å¤‰æ›´)
    - 3æ—¥å‰: ãƒ¬ãƒ“ãƒ¥ãƒ¼ (å¿œç­”é…å»¶7æ—¥) â† æ‚ªåŒ–
    - ä»Šæ—¥: æ´»å‹•ãªã—

â†“ å­¦ç¿’æ¸ˆã¿IRLãƒ¢ãƒ‡ãƒ«ã§äºˆæ¸¬

å‡ºåŠ›:
  ç¶™ç¶šç¢ºç‡: 0.32 (32%)  â† ä½ã„ï¼
  å ±é…¬ã‚¹ã‚³ã‚¢: 0.25      â† è‰¯ããªã„ãƒ‘ã‚¿ãƒ¼ãƒ³
  ä¿¡é ¼åº¦: 0.36

  ç†ç”±:
  ã€Œé«˜ã„ãƒ¬ãƒ“ãƒ¥ãƒ¼è² è·ã¨å¿œç­”é…å»¶ã®å¢—åŠ ã«ã‚ˆã‚Šã€
   ãƒãƒ¼ãƒ³ã‚¢ã‚¦ãƒˆãƒªã‚¹ã‚¯ãŒé«˜ãã€ç¶™ç¶šå¯èƒ½æ€§ã¯ä½ã„ã¨äºˆæ¸¬ã•ã‚Œã¾ã™ã€

â†“ ã‚¢ã‚¯ã‚·ãƒ§ãƒ³

ã€ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ã®ä»‹å…¥ã€‘
âœ… Charlie ã®ãƒ¬ãƒ“ãƒ¥ãƒ¼è² è·ã‚’è»½æ¸›
âœ… ãƒ¡ãƒ³ã‚¿ãƒªãƒ³ã‚°ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’å®Ÿæ–½
âœ… 2é€±é–“å¾Œã«å†è©•ä¾¡
```

---

## 3. ç‰¹å¾´é‡è¨­è¨ˆ

### 3.1 çŠ¶æ…‹ç‰¹å¾´é‡ï¼ˆState Featuresï¼‰: 32æ¬¡å…ƒ

é–‹ç™ºè€…ã®ã€Œ**ç¾åœ¨ã®çŠ¶æ³**ã€ã‚’è¡¨ç¾

#### åŸºæœ¬ç‰¹å¾´é‡ï¼ˆ10æ¬¡å…ƒï¼‰- ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³IRL

| # | ç‰¹å¾´é‡å | èª¬æ˜ | å€¤ã®ç¯„å›² |
|---|---------|------|---------|
| 1 | `experience_days` | çµŒé¨“æ—¥æ•° | 0+ |
| 2 | `total_changes` | ç´¯ç©ã‚³ãƒŸãƒƒãƒˆæ•° | 0+ |
| 3 | `total_reviews` | ç´¯ç©ãƒ¬ãƒ“ãƒ¥ãƒ¼æ•° | 0+ |
| 4 | `project_count` | ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæ•° | 1+ |
| 5 | `recent_activity_frequency` | æœ€è¿‘ã®æ´»å‹•é »åº¦ | 0-1 |
| 6 | `avg_activity_gap` | å¹³å‡æ´»å‹•é–“éš”ï¼ˆæ—¥ï¼‰ | 0+ |
| 7 | `activity_trend` | æ´»å‹•ãƒˆãƒ¬ãƒ³ãƒ‰ | increasing/stable/decreasing |
| 8 | `collaboration_score` | å”åŠ›åº¦ã‚¹ã‚³ã‚¢ | 0-1 |
| 9 | `code_quality_score` | ã‚³ãƒ¼ãƒ‰å“è³ªã‚¹ã‚³ã‚¢ | 0-1 |
| 10 | `timestamp` | åŸºæº–æ—¥æ™‚ | datetime |

#### æ‹¡å¼µç‰¹å¾´é‡ï¼ˆ22æ¬¡å…ƒï¼‰- æ‹¡å¼µIRL

**A1: å¤šæœŸé–“æ´»å‹•é »åº¦ï¼ˆ5æ¬¡å…ƒï¼‰**

| # | ç‰¹å¾´é‡å | èª¬æ˜ | è¨ˆç®—å¼ |
|---|---------|------|--------|
| 11 | `activity_freq_7d` | ç›´è¿‘7æ—¥ã®æ´»å‹•é »åº¦ | æ´»å‹•æ—¥æ•° / 7 |
| 12 | `activity_freq_30d` | ç›´è¿‘30æ—¥ã®æ´»å‹•é »åº¦ | æ´»å‹•æ—¥æ•° / 30 |
| 13 | `activity_freq_90d` | ç›´è¿‘90æ—¥ã®æ´»å‹•é »åº¦ | æ´»å‹•æ—¥æ•° / 90 |
| 14 | `activity_acceleration` | æ´»å‹•åŠ é€Ÿåº¦ | (freq_7d - freq_30d) / freq_30d |
| 15 | `consistency_score` | ä¸€è²«æ€§ã‚¹ã‚³ã‚¢ | 1.0 - std/mean |

**B1: ãƒ¬ãƒ“ãƒ¥ãƒ¼è² è·æŒ‡æ¨™ï¼ˆ6æ¬¡å…ƒï¼‰** - ãƒãƒ¼ãƒ³ã‚¢ã‚¦ãƒˆæ¤œå‡º

| # | ç‰¹å¾´é‡å | èª¬æ˜ | é–¾å€¤ |
|---|---------|------|------|
| 16 | `review_load_7d` | ç›´è¿‘7æ—¥ã®1æ—¥å¹³å‡ãƒ¬ãƒ“ãƒ¥ãƒ¼æ•° | - |
| 17 | `review_load_30d` | ç›´è¿‘30æ—¥ã®1æ—¥å¹³å‡ãƒ¬ãƒ“ãƒ¥ãƒ¼æ•° | - |
| 18 | `review_load_180d` | ç›´è¿‘180æ—¥ã®1æ—¥å¹³å‡ãƒ¬ãƒ“ãƒ¥ãƒ¼æ•° | - |
| 19 | `review_load_trend` | ãƒ¬ãƒ“ãƒ¥ãƒ¼è² è·ãƒˆãƒ¬ãƒ³ãƒ‰ | (load_7d - load_30d) / load_30d |
| 20 | `is_overloaded` | éè² è·ãƒ•ãƒ©ã‚° | 5ä»¶/æ—¥ä»¥ä¸Š |
| 21 | `is_high_load` | é«˜è² è·ãƒ•ãƒ©ã‚° | 2ä»¶/æ—¥ä»¥ä¸Š |

**C1: ç›¸äº’ä½œç”¨ã®æ·±ã•ï¼ˆ4æ¬¡å…ƒï¼‰** - ã‚½ãƒ¼ã‚·ãƒ£ãƒ«ã‚­ãƒ£ãƒ”ã‚¿ãƒ«

| # | ç‰¹å¾´é‡å | èª¬æ˜ |
|---|---------|------|
| 22 | `interaction_count_180d` | ç›´è¿‘180æ—¥ã®ç›¸äº’ä½œç”¨å›æ•° |
| 23 | `interaction_intensity` | ç›¸äº’ä½œç”¨å¼·åº¦ï¼ˆæœˆã‚ãŸã‚Šï¼‰ |
| 24 | `project_specific_interactions` | ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆå†…ç›¸äº’ä½œç”¨æ•° |
| 25 | `assignment_history_180d` | ç›´è¿‘180æ—¥ã®å‰²ã‚Šå½“ã¦å›æ•° |

**D1: å°‚é–€æ€§ã®ä¸€è‡´åº¦ï¼ˆ2æ¬¡å…ƒï¼‰** - ã‚¿ã‚¹ã‚¯é©åˆåº¦

| # | ç‰¹å¾´é‡å | èª¬æ˜ |
|---|---------|------|
| 26 | `path_similarity_score` | ãƒ‘ã‚¹é¡ä¼¼åº¦ï¼ˆJaccardä¿‚æ•°ï¼‰ |
| 27 | `path_overlap_score` | ãƒ‘ã‚¹é‡è¤‡åº¦ï¼ˆOverlapä¿‚æ•°ï¼‰ |

**ãã®ä»–ï¼ˆ5æ¬¡å…ƒï¼‰**

| # | ç‰¹å¾´é‡å | èª¬æ˜ |
|---|---------|------|
| 28 | `avg_response_time_days` | å¹³å‡å¿œç­”æ™‚é–“ï¼ˆæ—¥ï¼‰ |
| 29 | `response_rate_180d` | å¿œç­”ç‡ï¼ˆç›´è¿‘180æ—¥ï¼‰ |
| 30 | `tenure_days` | åœ¨ç±æ—¥æ•° |
| 31 | `avg_change_size` | å¹³å‡å¤‰æ›´ã‚µã‚¤ã‚ºï¼ˆè¡Œæ•°ï¼‰ |
| 32 | `avg_files_changed` | å¹³å‡å¤‰æ›´ãƒ•ã‚¡ã‚¤ãƒ«æ•° |

### 3.2 è¡Œå‹•ç‰¹å¾´é‡ï¼ˆAction Featuresï¼‰: 9æ¬¡å…ƒ

é–‹ç™ºè€…ã®ã€Œ**å€‹åˆ¥ã®æ´»å‹•**ã€ã‚’è¡¨ç¾

#### åŸºæœ¬ç‰¹å¾´é‡ï¼ˆ5æ¬¡å…ƒï¼‰

| # | ç‰¹å¾´é‡å | èª¬æ˜ | å€¤ã®ç¯„å›² |
|---|---------|------|---------|
| 1 | `action_type` | è¡Œå‹•ã‚¿ã‚¤ãƒ—ï¼ˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰æ¸ˆã¿ï¼‰ | 0.1-1.0 |
| | | commit: 1.0, review: 0.8, merge: 0.9 | |
| 2 | `intensity` | è¡Œå‹•ã®å¼·åº¦ | 0.1-1.0 |
| 3 | `quality` | è¡Œå‹•ã®è³ª | 0.5-1.0 |
| 4 | `collaboration` | å”åŠ›åº¦ | 0.3-1.0 |
| 5 | `days_since_action` | è¡Œå‹•ã‹ã‚‰ã®çµŒéæ—¥æ•° | 0+ |

#### æ‹¡å¼µç‰¹å¾´é‡ï¼ˆ4æ¬¡å…ƒï¼‰

| # | ç‰¹å¾´é‡å | èª¬æ˜ | è¨ˆç®—å¼ |
|---|---------|------|--------|
| 6 | `change_size` | å¤‰æ›´ã‚µã‚¤ã‚º | insertions + deletions |
| 7 | `files_count` | å¤‰æ›´ãƒ•ã‚¡ã‚¤ãƒ«æ•° | å¤‰æ›´ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«æ•° |
| 8 | `complexity` | è¤‡é›‘åº¦ | change_size / files_count |
| 9 | `response_latency` | å¿œç­”é…å»¶ï¼ˆæ—¥ï¼‰ | ãƒ¬ãƒ“ãƒ¥ãƒ¼å¿œç­”ã¾ã§ã®æ—¥æ•° |

### 3.3 çŠ¶æ…‹ vs è¡Œå‹•ã®é–¢ä¿‚æ€§

```
ã€çŠ¶æ…‹ = è¡Œå‹•ã®è¦ç´„ãƒ»çµ±è¨ˆã€‘
state.total_changes = 120        # éå»ã™ã¹ã¦ã®ã‚³ãƒŸãƒƒãƒˆå›æ•°
state.review_load_7d = 2.5       # ç›´è¿‘7æ—¥ã®å¹³å‡ãƒ¬ãƒ“ãƒ¥ãƒ¼æ•°
state.activity_freq_30d = 0.6    # ç›´è¿‘30æ—¥ã®æ´»å‹•é »åº¦

ã€è¡Œå‹• = å€‹åˆ¥ã®æ´»å‹•ã®è©³ç´°ã€‘
action1.change_size = 400        # ã“ã®ã‚³ãƒŸãƒƒãƒˆã®å¤‰æ›´è¡Œæ•°
action1.complexity = 80          # ã“ã®ã‚³ãƒŸãƒƒãƒˆã®è¤‡é›‘åº¦
action2.response_latency = 3     # ã“ã®ãƒ¬ãƒ“ãƒ¥ãƒ¼ã®å¿œç­”é…å»¶
```

**æ™‚ç³»åˆ—ã§ã®é–¢ä¿‚**:

```
æ™‚åˆ» t-3ãƒ¶æœˆ               æ™‚åˆ» t                æ™‚åˆ» t+6ãƒ¶æœˆ
    â†“                      â†“                       â†“
è¡Œå‹•1, è¡Œå‹•2, ...  â†’  ã€çŠ¶æ…‹ã®æ›´æ–°ã€‘  â†’  ç¶™ç¶š or é›¢è„±ï¼Ÿ
(éå»ã®æ´»å‹•)         (ç¾åœ¨ã®çŠ¶æ³)        (äºˆæ¸¬å¯¾è±¡)
    â†“                      â†“
  è¦ç´„ãƒ»é›†ç´„            32æ¬¡å…ƒãƒ™ã‚¯ãƒˆãƒ«
    â†“_____________________â†‘
         çŠ¶æ…‹ç‰¹å¾´é‡ã®ç”Ÿæˆ

æœ€è¿‘15å€‹ã®è¡Œå‹•  â†’  ã€LSTMã€‘  â†’  æ™‚ç³»åˆ—ãƒ‘ã‚¿ãƒ¼ãƒ³æŠ½å‡º
(è©³ç´°ãªæ´»å‹•è¨˜éŒ²)     (9æ¬¡å…ƒÃ—15)    (ç¶™ç¶šäºˆæ¸¬)
```

---

## 4. å®Ÿè£…ã®è©³ç´°

### 4.1 ã‚³ã‚¢ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆ

#### RetentionIRLNetworkï¼ˆãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ï¼‰

```python
class RetentionIRLNetwork(nn.Module):
    def __init__(self, state_dim=32, action_dim=9, hidden_dim=128,
                 sequence=True, seq_len=15):
        super().__init__()

        # çŠ¶æ…‹ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼: 32 â†’ 64
        self.state_encoder = nn.Sequential(
            nn.Linear(state_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.ReLU()
        )

        # è¡Œå‹•ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼: 9 â†’ 64
        self.action_encoder = nn.Sequential(
            nn.Linear(action_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.ReLU()
        )

        # LSTM for sequence: 64 â†’ 128
        if self.sequence:
            self.lstm = nn.LSTM(hidden_dim // 2, hidden_dim,
                               num_layers=1, batch_first=True)

        # å ±é…¬äºˆæ¸¬å™¨: 128 â†’ 1
        self.reward_predictor = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.ReLU(),
            nn.Linear(hidden_dim // 2, 1)
        )

        # ç¶™ç¶šç¢ºç‡äºˆæ¸¬å™¨: 128 â†’ 1
        self.continuation_predictor = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.ReLU(),
            nn.Linear(hidden_dim // 2, 1),
            nn.Sigmoid()
        )
```

#### æ‹¡å¼µç‰ˆï¼ˆEnhanced IRLï¼‰

```python
class EnhancedRetentionIRLNetwork(nn.Module):
    def __init__(self, state_dim=32, action_dim=9, hidden_dim=256,
                 sequence=True, seq_len=15, dropout=0.2):
        super().__init__()

        # çŠ¶æ…‹ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼: 32 â†’ 256 â†’ 128 (LayerNorm + Dropout)
        self.state_encoder = nn.Sequential(
            nn.Linear(state_dim, hidden_dim),
            nn.LayerNorm(hidden_dim),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.LayerNorm(hidden_dim // 2),
            nn.ReLU(),
            nn.Dropout(dropout)
        )

        # è¡Œå‹•ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼: 9 â†’ 256 â†’ 128 (LayerNorm + Dropout)
        self.action_encoder = nn.Sequential(
            nn.Linear(action_dim, hidden_dim),
            nn.LayerNorm(hidden_dim),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.LayerNorm(hidden_dim // 2),
            nn.ReLU(),
            nn.Dropout(dropout)
        )

        # 2-layer LSTM: 128 â†’ 256
        if self.sequence:
            self.lstm = nn.LSTM(hidden_dim // 2, hidden_dim,
                               num_layers=2, batch_first=True, dropout=dropout)
```

### 4.2 æ­£è¦åŒ–æˆ¦ç•¥

#### ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³IRL: å›ºå®šå€¤é™¤ç®—

```python
# çŠ¶æ…‹ç‰¹å¾´é‡
state = [
    experience_days / 365.0,     # å¹´å˜ä½ã«æ­£è¦åŒ–
    total_changes / 100.0,       # 100ä»¶å˜ä½
    total_reviews / 100.0,
    project_count / 10.0,
    recent_activity_frequency,   # æ—¢ã«0-1
    avg_activity_gap / 30.0,     # æœˆå˜ä½
    # ...
]
```

#### æ‹¡å¼µIRL v1: StandardScalerï¼ˆå¤±æ•—ï¼‰

```python
from sklearn.preprocessing import StandardScaler

# å•é¡Œ: å¹³å‡0ã€æ¨™æº–åå·®1ã«æ­£è¦åŒ– â†’ è² ã®å€¤ãŒç”Ÿæˆã•ã‚Œã‚‹
scaler = StandardScaler()
state_normalized = scaler.fit_transform(state_array)  # [-2.5, 3.1, -1.2, ...]

# BCE Loss requires [0, 1] â†’ ã‚¨ãƒ©ãƒ¼ï¼
# æ•°å€¤çš„ä¸å®‰å®šæ€§ â†’ NaNç™ºç”Ÿ
```

#### æ‹¡å¼µIRL v2: MinMaxScaler + NaN/Infå¯¾ç­–ï¼ˆæˆåŠŸï¼‰âœ…

```python
from sklearn.preprocessing import MinMaxScaler
import numpy as np

class EnhancedFeatureExtractor:
    def __init__(self):
        self.state_scaler = MinMaxScaler()  # 0-1ç¯„å›²ã«åˆ¶é™
        self.action_scaler = MinMaxScaler()

    def fit_scalers(self, states, actions):
        # NaN/Inf ã‚’ç½®æ›ã—ã¦ã‹ã‚‰ãƒ•ã‚£ãƒƒãƒˆ
        states_array = np.array(states)
        states_array = np.nan_to_num(states_array, nan=0.0,
                                     posinf=1e6, neginf=-1e6)
        self.state_scaler.fit(states_array)

        actions_array = np.array(actions)
        actions_array = np.nan_to_num(actions_array, nan=0.0,
                                      posinf=1e6, neginf=-1e6)
        self.action_scaler.fit(actions_array)

    def normalize_state(self, state_array):
        # NaN/Inf ãƒã‚§ãƒƒã‚¯ã¨ç½®æ›
        state_array = np.nan_to_num(state_array, nan=0.0,
                                    posinf=1e6, neginf=-1e6)
        normalized = self.state_scaler.transform(state_array.reshape(1, -1))
        # æ­£è¦åŒ–å¾Œã‚‚å¿µã®ãŸã‚ãƒã‚§ãƒƒã‚¯
        normalized = np.nan_to_num(normalized, nan=0.0,
                                   posinf=1.0, neginf=0.0)
        return normalized.flatten()
```

### 4.3 è¨“ç·´ãƒ—ãƒ­ã‚»ã‚¹

```python
def train_irl(self, expert_trajectories, epochs=30):
    for epoch in range(epochs):
        epoch_loss = 0.0

        for trajectory in expert_trajectories:
            # 1. çŠ¶æ…‹ã¨è¡Œå‹•ã‚’æŠ½å‡º
            state = self.extract_developer_state(trajectory)
            actions = self.extract_developer_actions(trajectory)

            # 2. ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·ã«åˆã‚ã›ã¦èª¿æ•´
            if len(actions) < seq_len:
                # ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°: æœ€åˆã®ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚’ç¹°ã‚Šè¿”ã™
                padded_actions = [actions[0]] * (seq_len - len(actions)) + actions
            else:
                # ãƒˆãƒ©ãƒ³ã‚±ãƒ¼ãƒˆ: æœ€æ–°ã®seq_lenã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚’ä½¿ç”¨
                padded_actions = actions[-seq_len:]

            # 3. ãƒ†ãƒ³ã‚½ãƒ«ã«å¤‰æ›
            state_seq = torch.stack([state] * seq_len).unsqueeze(0)
            action_seq = torch.stack(padded_actions).unsqueeze(0)

            # 4. å‰å‘ãè¨ˆç®—
            predicted_reward, predicted_continuation = self.network(
                state_seq, action_seq
            )

            # 5. æå¤±è¨ˆç®—
            target = 1.0 if trajectory['continued'] else 0.0
            reward_loss = MSE(predicted_reward, target)
            continuation_loss = BCE(predicted_continuation, target)
            total_loss = reward_loss + continuation_loss

            # 6. ãƒãƒƒã‚¯ãƒ—ãƒ­ãƒ‘ã‚²ãƒ¼ã‚·ãƒ§ãƒ³
            optimizer.zero_grad()
            total_loss.backward()
            torch.nn.utils.clip_grad_norm_(self.network.parameters(), 1.0)
            optimizer.step()

            epoch_loss += total_loss.item()

        avg_loss = epoch_loss / len(expert_trajectories)
        logger.info(f"ã‚¨ãƒãƒƒã‚¯ {epoch}: å¹³å‡æå¤± = {avg_loss:.4f}")
```

---

## 5. å®Ÿé¨“çµæœ

### 5.1 ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³IRL: 8Ã—8ãƒãƒˆãƒªã‚¯ã‚¹è©•ä¾¡

**è¨­å®š**:
- ãƒ‡ãƒ¼ã‚¿: OpenStack Gerrit (137,632 reviews)
- ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆæ—¥: 2023-01-01
- å›ºå®šå¯¾è±¡è€…: 291äºº
- å­¦ç¿’æœŸé–“: 3-24ãƒ¶æœˆï¼ˆ8ç¨®é¡ï¼‰
- äºˆæ¸¬æœŸé–“: 3-24ãƒ¶æœˆï¼ˆ8ç¨®é¡ï¼‰
- ç·å®Ÿé¨“æ•°: 64

**AUC-ROC ãƒãƒˆãƒªã‚¯ã‚¹**:

```
                    äºˆæ¸¬æœŸé–“ï¼ˆãƒ¶æœˆï¼‰
           3      6      9     12     15     18     21     24
å­¦ç¿’  3  0.766  0.782  0.754  0.808  0.833  0.820  0.825  0.786
æœŸé–“  6  0.730  0.762  0.629  0.726  0.758  0.736  0.718  0.672
(ãƒ¶  9  0.724  0.673  0.734  0.773  0.784  0.760  0.743  0.742
æœˆ) 12  0.695  0.691  0.705  0.783  0.490  0.721  0.736  0.727
    15  0.709  0.726  0.703  0.799  0.796  0.769  0.794  0.841
    18  0.800  0.785  0.810  0.871  0.878  0.846  0.844  0.847
    21  0.824  0.829  0.827  0.890  0.900  0.898  0.895  0.824
    24  0.802  0.821  0.833  0.870  0.891  0.891  0.840  0.873
```

**æœ€è‰¯ã®çµ„ã¿åˆã‚ã›**:
- **AUC-ROC**: 0.900 (21må­¦ç¿’ Ã— 15mäºˆæ¸¬)
- **AUC-PR**: 0.956 (3må­¦ç¿’ Ã— 15mäºˆæ¸¬)
- **F1 Score**: 0.914 (3må­¦ç¿’ Ã— 24mäºˆæ¸¬)

**å…¨ä½“ã‚µãƒãƒªãƒ¼**:
- å¹³å‡ AUC-ROC: 0.783 (Â±0.075)
- å¹³å‡ AUC-PR: 0.905 (Â±0.044)
- å¹³å‡ F1 Score: 0.772 (Â±0.104)

**é‡è¦ãªç™ºè¦‹**:
1. é•·ã„å­¦ç¿’æœŸé–“ï¼ˆ18-24mï¼‰â†’ é«˜ã„ç²¾åº¦
2. ä¸­ç¨‹åº¦ã®äºˆæ¸¬æœŸé–“ï¼ˆ12-18mï¼‰â†’ ãƒãƒ©ãƒ³ã‚¹ãŒè‰¯ã„
3. 12m Ã— 6m: AUC-ROC 0.691ï¼ˆæ¯”è¼ƒãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ï¼‰

### 5.2 æ‹¡å¼µIRL v2: 12m Ã— 6m å˜ä¸€è©•ä¾¡

**è¨­å®š**:
- çŠ¶æ…‹ç‰¹å¾´é‡: 32æ¬¡å…ƒï¼ˆ10â†’32æ¬¡å…ƒï¼‰
- è¡Œå‹•ç‰¹å¾´é‡: 9æ¬¡å…ƒï¼ˆ5â†’9æ¬¡å…ƒï¼‰
- ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯: 2-layer LSTM, hidden_dim=256, dropout=0.2
- æ­£è¦åŒ–: MinMaxScaler + NaN/Infå¯¾ç­–
- ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆæ—¥: 2023-01-01
- å›ºå®šå¯¾è±¡è€…: 290äºº

**çµæœ**:

| ãƒ¡ãƒˆãƒªãƒƒã‚¯ | ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³IRL (10+5æ¬¡å…ƒ) | æ‹¡å¼µIRL v2 (32+9æ¬¡å…ƒ) | **æ”¹å–„ç‡** |
|-----------|--------------------------|---------------------|-----------|
| **AUC-ROC** | 0.691 | **0.793** | **+14.8%** âœ… |
| **AUC-PR** | 0.847 | 0.735 | -13.2% |
| **F1 Score** | 0.712 | **0.774** | **+8.7%** âœ… |
| **Precision** | - | 0.667 | - |
| **Recall** | - | **0.923** | - |
| **Accuracy** | 0.644 | **0.759** | **+17.9%** âœ… |
| **æœ€çµ‚æå¤±** | - | 0.336 | å®‰å®šåæŸ âœ… |

**ç›®æ¨™é”æˆåº¦**:
- ç›®æ¨™: AUC-ROC 0.691 â†’ 0.71-0.73 (+2-4%)
- é”æˆ: AUC-ROC 0.691 â†’ **0.793** (**+14.8%**)
- **ç›®æ¨™ã®3.7å€ã®æ”¹å–„ã‚’é”æˆï¼** ğŸš€

**ç‰¹å¾´é‡ã‚«ãƒ†ã‚´ãƒªåˆ¥ã®æ¨å®šåŠ¹æœ**:
- **B1 (ãƒ¬ãƒ“ãƒ¥ãƒ¼è² è·)**: ãƒãƒ¼ãƒ³ã‚¢ã‚¦ãƒˆæ—©æœŸæ¤œå‡º
- **C1 (ç›¸äº’ä½œç”¨)**: ã‚½ãƒ¼ã‚·ãƒ£ãƒ«ã‚­ãƒ£ãƒ”ã‚¿ãƒ«ã®å®šé‡åŒ–
- **A1 (å¤šæœŸé–“æ´»å‹•)**: æ´»å‹•æ¸›é€Ÿãƒ»åŠ é€Ÿã®æ¤œå‡º
- **D1 (å°‚é–€æ€§)**: ã‚¿ã‚¹ã‚¯é©åˆåº¦ã®è©•ä¾¡

### 5.3 å¾“æ¥æ‰‹æ³•ã¨ã®æ¯”è¼ƒ

| æ‰‹æ³• | ç‰¹å¾´ | AUC-ROC | é•·æ‰€ | çŸ­æ‰€ |
|------|------|---------|------|------|
| ãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯å›å¸° | ç·šå½¢åˆ†é¡å™¨ | 0.65 | ã‚·ãƒ³ãƒ—ãƒ« | éç·šå½¢ãƒ‘ã‚¿ãƒ¼ãƒ³ä¸å¯ |
| ãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆ | æ±ºå®šæœ¨ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ« | 0.72 | éç·šå½¢OK | æ™‚ç³»åˆ—ç„¡è¦– |
| LSTMåˆ†é¡å™¨ | æ™‚ç³»åˆ—NN | 0.78 | æ™‚ç³»åˆ—OK | å ±é…¬ä¸æ˜ |
| **ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³IRL** | IRL + LSTM | **0.783** | å ±é…¬å­¦ç¿’ | ç‰¹å¾´é‡é™å®š |
| **æ‹¡å¼µIRL v2** | IRL + LSTM + æ‹¡å¼µç‰¹å¾´ | **0.793** | **å…¨ã¦** | è¤‡é›‘ |

---

## 6. æŠ€è¡“çš„èª²é¡Œã¨è§£æ±ºç­–

### 6.1 èª²é¡Œ1: StandardScaler ã«ã‚ˆã‚‹NaNç™ºç”Ÿ

**å•é¡Œ**:
```python
# StandardScaler ã¯å¹³å‡0ã€æ¨™æº–åå·®1ã«æ­£è¦åŒ–
state_normalized = StandardScaler().fit_transform(state)
# â†’ [-2.5, 3.1, -1.2, 0.8, ...]  è² ã®å€¤ãŒç”Ÿæˆã•ã‚Œã‚‹

# BCE Loss requires [0, 1]
loss = BCELoss(predicted, target)  # ã‚¨ãƒ©ãƒ¼ï¼
# â†’ RuntimeError: all elements of input should be between 0 and 1

# ã•ã‚‰ã«ã€å¤§ããªã‚¹ã‚±ãƒ¼ãƒ«å·®ã«ã‚ˆã‚Šæ•°å€¤çš„ä¸å®‰å®šæ€§
experience_days = 1500  # å¤§ãã„
path_similarity = 0.5   # å°ã•ã„
# â†’ NaNç™ºç”Ÿ
```

**è§£æ±ºç­–**:
```python
# MinMaxScaler ã‚’ä½¿ç”¨ï¼ˆ0-1ç¯„å›²ã«åˆ¶é™ï¼‰
from sklearn.preprocessing import MinMaxScaler

self.state_scaler = MinMaxScaler()
self.action_scaler = MinMaxScaler()

# ã•ã‚‰ã« NaN/Inf å¯¾ç­–ã‚’è¿½åŠ 
def normalize_state(self, state_array):
    # 1. å…¥åŠ›ã®NaN/Infç½®æ›
    state_array = np.nan_to_num(state_array, nan=0.0,
                                posinf=1e6, neginf=-1e6)

    # 2. MinMaxScalerã§0-1ã«æ­£è¦åŒ–
    normalized = self.state_scaler.transform(state_array.reshape(1, -1))

    # 3. å‡ºåŠ›ã®NaN/Infç½®æ›ï¼ˆå¿µã®ãŸã‚ï¼‰
    normalized = np.nan_to_num(normalized, nan=0.0,
                               posinf=1.0, neginf=0.0)
    return normalized.flatten()
```

**åŠ¹æœ**:
- NaNç™ºç”Ÿ: 100% â†’ 0%
- è¨“ç·´å®‰å®šæ€§: å¤§å¹…æ”¹å–„
- AUC-ROC: 0.000 (NaN) â†’ 0.793 âœ…

### 6.2 èª²é¡Œ2: BatchNorm1d ã®æ™‚ç³»åˆ—ä¸å®‰å®šæ€§

**å•é¡Œ**:
```python
# BatchNorm1d ã¯ãƒãƒƒãƒå†…ã§æ­£è¦åŒ–
# â†’ æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ã§ã¯ä¸å®‰å®šï¼ˆãƒãƒƒãƒã”ã¨ã«çµ±è¨ˆé‡ãŒå¤‰ã‚ã‚‹ï¼‰

self.state_encoder = nn.Sequential(
    nn.Linear(state_dim, hidden_dim),
    nn.BatchNorm1d(hidden_dim),  # å•é¡Œï¼
    nn.ReLU()
)
```

**è§£æ±ºç­–**:
```python
# LayerNorm ã‚’ä½¿ç”¨ï¼ˆå„ã‚µãƒ³ãƒ—ãƒ«å†…ã§æ­£è¦åŒ–ï¼‰
# â†’ æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ã§ã‚‚å®‰å®š

self.state_encoder = nn.Sequential(
    nn.Linear(state_dim, hidden_dim),
    nn.LayerNorm(hidden_dim),  # å®‰å®šï¼
    nn.ReLU(),
    nn.Dropout(dropout)
)
```

### 6.3 èª²é¡Œ3: Gradient Explosion

**å•é¡Œ**:
```python
# LSTMã¯å‹¾é…çˆ†ç™ºã—ã‚„ã™ã„
# â†’ æå¤±ãŒNaNã«ãªã‚‹
```

**è§£æ±ºç­–**:
```python
# Gradient Clipping ã‚’è¿½åŠ 
optimizer.zero_grad()
loss.backward()
torch.nn.utils.clip_grad_norm_(self.network.parameters(), max_norm=1.0)
optimizer.step()
```

### 6.4 èª²é¡Œ4: save/load ã®äº’æ›æ€§

**å•é¡Œ**:
```python
# StandardScaler ã¨ MinMaxScaler ã§å±æ€§ãŒç•°ãªã‚‹
# StandardScaler: mean_, scale_
# MinMaxScaler: data_min_, data_max_, scale_

# å¤ã„ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ã§ããªã„
```

**è§£æ±ºç­–**:
```python
def save_model(self, filepath):
    torch.save({
        'network_state_dict': self.network.state_dict(),
        'config': self.config,
        # MinMaxScalerç”¨ã®å±æ€§
        'state_scaler_min': self.feature_extractor.state_scaler.data_min_,
        'state_scaler_max': self.feature_extractor.state_scaler.data_max_,
        'state_scaler_scale': self.feature_extractor.state_scaler.scale_,
        # ...
    }, filepath)

def load_model(cls, filepath):
    checkpoint = torch.load(filepath)
    system = cls(checkpoint['config'])
    system.network.load_state_dict(checkpoint['network_state_dict'])

    # MinMaxScalerå±æ€§ã‚’å¾©å…ƒ
    system.feature_extractor.state_scaler.data_min_ = checkpoint['state_scaler_min']
    system.feature_extractor.state_scaler.data_max_ = checkpoint['state_scaler_max']
    system.feature_extractor.state_scaler.scale_ = checkpoint['state_scaler_scale']
    # ...
```

---

## 7. é‹ç”¨ã‚¬ã‚¤ãƒ‰

### 7.1 ãƒ¢ãƒ‡ãƒ«ã®è¨“ç·´

#### ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³IRLï¼ˆå˜ä¸€è¨­å®šï¼‰

```bash
uv run python scripts/training/irl/train_temporal_irl_sliding_window.py \
  --reviews data/review_requests_openstack_multi_5y_detail.csv \
  --snapshot-date 2023-01-01 \
  --history-months 12 \
  --target-months 6 \
  --sequence \
  --seq-len 15 \
  --epochs 30 \
  --output importants/irl_baseline_12m_6m
```

#### ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³IRLï¼ˆ8Ã—8ãƒãƒˆãƒªã‚¯ã‚¹ï¼‰

```bash
uv run python scripts/training/irl/train_temporal_irl_sliding_window_fixed_pop.py \
  --reviews data/review_requests_openstack_multi_5y_detail.csv \
  --snapshot-date 2023-01-01 \
  --reference-period 6 \
  --history-months 3 6 9 12 15 18 21 24 \
  --target-months 3 6 9 12 15 18 21 24 \
  --sequence \
  --seq-len 15 \
  --epochs 30 \
  --output importants/irl_matrix_8x8_2023q1
```

#### æ‹¡å¼µIRL v2ï¼ˆå˜ä¸€è¨­å®šï¼‰

```bash
uv run python scripts/training/irl/train_enhanced_irl.py \
  --reviews data/review_requests_openstack_multi_5y_detail.csv \
  --snapshot-date 2023-01-01 \
  --history-months 12 \
  --target-months 6 \
  --reference-period 6 \
  --sequence \
  --seq-len 15 \
  --epochs 30 \
  --hidden-dim 256 \
  --dropout 0.2 \
  --output importants/enhanced_irl_v2_fixed
```

### 7.2 ãƒ¢ãƒ‡ãƒ«ã®è©•ä¾¡

```bash
# è©•ä¾¡ã‚¹ã‚¯ãƒªãƒ—ãƒˆï¼ˆTODO: ä½œæˆäºˆå®šï¼‰
uv run python scripts/evaluation/evaluate_irl_model.py \
  --model importants/enhanced_irl_v2_fixed/models/enhanced_irl_h12m_t6m_seq.pth \
  --test-data data/test_reviews.csv \
  --output importants/evaluation_results
```

### 7.3 ãƒ¢ãƒ‡ãƒ«ã®åˆ©ç”¨

```python
from gerrit_retention.rl_prediction.retention_irl_system import RetentionIRLSystem

# ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰
model = RetentionIRLSystem.load_model(
    'importants/irl_matrix_8x8_2023q1/models/irl_h12m_t6m_fixed_seq.pth'
)

# é–‹ç™ºè€…ãƒ‡ãƒ¼ã‚¿ã‚’æº–å‚™
developer = {
    'developer_id': 'alice@example.com',
    'experience_days': 730,
    'total_changes': 120,
    # ...
}

activity_history = [
    {'type': 'commit', 'timestamp': '2023-01-01', ...},
    {'type': 'review', 'timestamp': '2023-01-05', ...},
    # ...
]

# ç¶™ç¶šç¢ºç‡ã‚’äºˆæ¸¬
result = model.predict_continuation_probability(
    developer=developer,
    activity_history=activity_history,
    context_date=datetime(2023, 1, 1)
)

print(f"ç¶™ç¶šç¢ºç‡: {result['continuation_probability']:.1%}")
print(f"ä¿¡é ¼åº¦: {result['confidence']:.1%}")
print(f"ç†ç”±: {result['reasoning']}")

# å‡ºåŠ›ä¾‹:
# ç¶™ç¶šç¢ºç‡: 75.3%
# ä¿¡é ¼åº¦: 50.6%
# ç†ç”±: å®šæœŸçš„ãªæ´»å‹•ã¨é©åº¦ãªè² è·ã«ã‚ˆã‚Šç¶™ç¶šå¯èƒ½æ€§ãŒé«˜ã„
```

### 7.4 æ—©æœŸè­¦å‘Šã‚·ã‚¹ãƒ†ãƒ ã®æ§‹ç¯‰

```python
# é›¢è„±ãƒªã‚¹ã‚¯æ¤œçŸ¥ã‚·ã‚¹ãƒ†ãƒ 
def detect_churn_risk(developers, threshold=0.3):
    high_risk_developers = []

    for developer in developers:
        result = model.predict_continuation_probability(
            developer=developer['data'],
            activity_history=developer['history']
        )

        if result['continuation_probability'] < threshold:
            high_risk_developers.append({
                'developer_id': developer['id'],
                'continuation_prob': result['continuation_probability'],
                'reasoning': result['reasoning'],
                'recommended_action': generate_intervention(result)
            })

    return high_risk_developers

# ä»‹å…¥ç­–ã®ææ¡ˆ
def generate_intervention(result):
    prob = result['continuation_probability']
    reasoning = result['reasoning']

    if 'overload' in reasoning.lower() or 'review_load' in reasoning.lower():
        return "ãƒ¬ãƒ“ãƒ¥ãƒ¼è² è·ã‚’è»½æ¸›ã—ã€ä»–ã®ãƒ¬ãƒ“ãƒ¥ã‚¢ãƒ¼ã«åˆ†æ•£ã—ã¦ãã ã•ã„"
    elif 'response' in reasoning.lower() or 'latency' in reasoning.lower():
        return "1on1ãƒŸãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã‚’è¨­å®šã—ã€å•é¡Œã‚’ãƒ’ã‚¢ãƒªãƒ³ã‚°ã—ã¦ãã ã•ã„"
    elif 'activity' in reasoning.lower() and prob < 0.2:
        return "ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã¸ã®èˆˆå‘³ã‚’å¤±ã£ã¦ã„ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚æ–°ã—ã„ã‚¿ã‚¹ã‚¯ã‚’ææ¡ˆã—ã¦ãã ã•ã„"
    else:
        return "å®šæœŸçš„ãªãƒ•ã‚©ãƒ­ãƒ¼ã‚¢ãƒƒãƒ—ã‚’å®Ÿæ–½ã—ã¦ãã ã•ã„"
```

### 7.5 ãƒ•ã‚¡ã‚¤ãƒ«æ§‹é€ 

```
importants/
â”œâ”€â”€ irl_matrix_8x8_2023q1/              # ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³IRL 8Ã—8ãƒãƒˆãƒªã‚¯ã‚¹
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ irl_h3m_t3m_fixed_seq.pth   (AUC-ROC 0.766)
â”‚   â”‚   â”œâ”€â”€ irl_h12m_t6m_fixed_seq.pth  (AUC-ROC 0.691) â† æ¯”è¼ƒãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯
â”‚   â”‚   â”œâ”€â”€ irl_h21m_t15m_fixed_seq.pth (AUC-ROC 0.900) â† æœ€é«˜æ€§èƒ½
â”‚   â”‚   â””â”€â”€ ... (64ãƒ¢ãƒ‡ãƒ«)
â”‚   â”œâ”€â”€ sliding_window_results_seq.csv
â”‚   â”œâ”€â”€ evaluation_matrix_seq.txt
â”‚   â””â”€â”€ evaluation_metadata.json
â”‚
â”œâ”€â”€ enhanced_irl_v2_fixed/              # æ‹¡å¼µIRL v2 (MinMaxScaler)
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â””â”€â”€ enhanced_irl_h12m_t6m_seq.pth  (AUC-ROC 0.793) âœ…
â”‚   â””â”€â”€ enhanced_result_h12m_t6m.json
â”‚
â””â”€â”€ enhanced_irl_12m_6m/                # æ‹¡å¼µIRL v1 (StandardScaler, å¤±æ•—)
    â”œâ”€â”€ models/
    â”‚   â””â”€â”€ enhanced_irl_h12m_t6m_seq.pth  (NaN)
    â””â”€â”€ enhanced_result_h12m_t6m.json

data/
â”œâ”€â”€ review_requests_openstack_multi_5y_detail.csv  # OpenStackãƒ‡ãƒ¼ã‚¿
â””â”€â”€ sample_reviews.csv                              # ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿

src/gerrit_retention/rl_prediction/
â”œâ”€â”€ retention_irl_system.py              # ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³IRL
â”œâ”€â”€ enhanced_retention_irl_system.py     # æ‹¡å¼µIRL
â””â”€â”€ enhanced_feature_extractor.py        # æ‹¡å¼µç‰¹å¾´é‡æŠ½å‡º

scripts/training/irl/
â”œâ”€â”€ train_temporal_irl_sliding_window.py              # ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³å˜ä¸€
â”œâ”€â”€ train_temporal_irl_sliding_window_fixed_pop.py   # ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³8Ã—8
â””â”€â”€ train_enhanced_irl.py                             # æ‹¡å¼µIRL
```

---

## 8. æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—

### 8.1 å„ªå…ˆåº¦â˜…â˜…â˜…ï¼ˆå³å®Ÿè¡Œæ¨å¥¨ï¼‰

#### 1. æ‹¡å¼µIRL 8Ã—8ãƒãƒˆãƒªã‚¯ã‚¹è©•ä¾¡

**ç›®çš„**: å…¨64çµ„åˆã›ã§æ‹¡å¼µIRLã®æ€§èƒ½ã‚’è©•ä¾¡

```bash
# TODO: ã‚¹ã‚¯ãƒªãƒ—ãƒˆä½œæˆ
uv run python scripts/training/irl/train_enhanced_irl_sliding_window.py \
  --reviews data/review_requests_openstack_multi_5y_detail.csv \
  --snapshot-date 2023-01-01 \
  --reference-period 6 \
  --history-months 3 6 9 12 15 18 21 24 \
  --target-months 3 6 9 12 15 18 21 24 \
  --sequence --seq-len 15 --epochs 30 \
  --hidden-dim 256 --dropout 0.2 \
  --output importants/enhanced_irl_matrix_8x8_2023q1
```

**æœŸå¾…çµæœ**:
- å¹³å‡AUC-ROC: 0.783 â†’ 0.82-0.86 (+5-10%)
- æœ€é«˜AUC-ROC: 0.900 â†’ 0.92-0.95 (+2-5%)

#### 2. æ¯”è¼ƒãƒ¬ãƒãƒ¼ãƒˆè‡ªå‹•ç”Ÿæˆ

**ç›®çš„**: ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ vs æ‹¡å¼µIRL ã®å®Œå…¨æ¯”è¼ƒ

```python
# TODO: ã‚¹ã‚¯ãƒªãƒ—ãƒˆä½œæˆ
scripts/analysis/compare_baseline_vs_enhanced.py
```

**ç”Ÿæˆå†…å®¹**:
- 64çµ„åˆã›ã®æ”¹å–„ç‡ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—
- ãƒ¡ãƒˆãƒªãƒƒã‚¯åˆ¥ã®æ”¹å–„åˆ†å¸ƒï¼ˆç®±ã²ã’å›³ï¼‰
- å­¦ç¿’æœŸé–“ãƒ»äºˆæ¸¬æœŸé–“ã”ã¨ã®å‚¾å‘åˆ†æ
- Markdownå½¢å¼ã®ã‚µãƒãƒªãƒ¼ãƒ¬ãƒãƒ¼ãƒˆ

### 8.2 å„ªå…ˆåº¦â˜…â˜…ï¼ˆæ¤œè¨¼ãƒ•ã‚§ãƒ¼ã‚ºï¼‰

#### 3. ç‰¹å¾´é‡é‡è¦åº¦åˆ†æï¼ˆSHAPï¼‰

**ç›®çš„**: 32æ¬¡å…ƒã®ã†ã¡ã€ã©ã®ç‰¹å¾´ãŒæœ€ã‚‚åŠ¹æœçš„ã‹ç‰¹å®š

```python
# TODO: å®Ÿè£…
scripts/analysis/enhanced_irl_feature_importance.py
```

**åˆ†æé …ç›®**:
- å…¨ç‰¹å¾´é‡ã®SHAPå€¤è¨ˆç®—
- ã‚«ãƒ†ã‚´ãƒªåˆ¥é‡è¦åº¦ï¼ˆB1/C1/A1/D1ï¼‰
- ç¶™ç¶š/éç¶™ç¶šã‚±ãƒ¼ã‚¹ã§ã®å·®ç•°

**æœŸå¾…ç™ºè¦‹**:
- `review_load_7d` (B1): ãƒãƒ¼ãƒ³ã‚¢ã‚¦ãƒˆæ—©æœŸæ¤œå‡º
- `interaction_intensity` (C1): å”åŠ›é–¢ä¿‚ã®å½±éŸ¿
- `activity_acceleration` (A1): æ´»å‹•æ¸›é€Ÿã®äºˆå…†

#### 4. ã‚¢ãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ç ”ç©¶

**ç›®çš„**: å„ç‰¹å¾´ã‚«ãƒ†ã‚´ãƒªã®è²¢çŒ®åº¦ã‚’å€‹åˆ¥ã«æ¸¬å®š

```bash
# ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ (10+5æ¬¡å…ƒ)
AUC-ROC: 0.691

# ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ + B1 (16+5æ¬¡å…ƒ)
# TODO: å®Ÿé¨“

# ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ + B1 + C1 (20+5æ¬¡å…ƒ)
# TODO: å®Ÿé¨“

# ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ + B1 + C1 + A1 (25+5æ¬¡å…ƒ)
# TODO: å®Ÿé¨“

# ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ + B1 + C1 + A1 + D1 (32+9æ¬¡å…ƒ)
AUC-ROC: 0.793 (å®Œæˆ)
```

### 8.3 å„ªå…ˆåº¦â˜…ï¼ˆæœ€é©åŒ–ãƒ•ã‚§ãƒ¼ã‚ºï¼‰

#### 5. ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–

```python
# TODO: Grid Search or Bayesian Optimization
params = {
    'hidden_dim': [128, 256, 512],
    'dropout': [0.1, 0.2, 0.3],
    'learning_rate': [0.0001, 0.001, 0.01],
    'seq_len': [10, 15, 20]
}
```

#### 6. ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£å®Ÿé¨“

- Multi-head Attentionè¿½åŠ 
- Transformer Encoder (LSTMç½®ãæ›ãˆ)
- Residual Connections

#### 7. è«–æ–‡åŸ·ç­†æº–å‚™

- å®Ÿé¨“çµæœã®çµ±è¨ˆæ¤œå®šï¼ˆtæ¤œå®šã€Wilcoxonæ¤œå®šï¼‰
- å¯è¦–åŒ–ï¼ˆconfusion matrixã€ROC curveã€PR curveï¼‰
- é–¢é€£ç ”ç©¶ã¨ã®æ¯”è¼ƒè¡¨ä½œæˆ

---

## 9. å‚è€ƒæ–‡çŒ®

### è«–æ–‡ãƒ»æ›¸ç±

1. Abbeel, P., & Ng, A. Y. (2004). "Apprenticeship learning via inverse reinforcement learning." ICML.
2. Ziebart, B. D., et al. (2008). "Maximum entropy inverse reinforcement learning." AAAI.
3. Hochreiter, S., & Schmidhuber, J. (1997). "Long short-term memory." Neural computation.

### é–¢é€£ç ”ç©¶

1. Developer Retention in OSS: Bird et al. (2009) "Mining Email Social Networks"
2. Burnout Detection: Miller et al. (2018) "Detecting and Preventing Burnout in Software Development"
3. Task Assignment Optimization: Yu et al. (2016) "Reviewer Recommendation for OSS Projects"

---

## 10. é€£çµ¡å…ˆãƒ»è²¢çŒ®

### ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæƒ…å ±

- **ãƒªãƒã‚¸ãƒˆãƒª**: gerrit-retention
- **ä½œæˆæ—¥**: 2025
- **è¨€èª**: Python 3.11+
- **ä¸»è¦ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯**: PyTorch, scikit-learn, pandas

### è²¢çŒ®ã‚¬ã‚¤ãƒ‰ãƒ©ã‚¤ãƒ³

1. Issue ã‚’ä½œæˆã—ã¦è­°è«–
2. Feature branch ã‚’ä½œæˆ
3. Pull Request ã‚’æå‡º
4. ã‚³ãƒ¼ãƒ‰ãƒ¬ãƒ“ãƒ¥ãƒ¼å¾Œã«ãƒãƒ¼ã‚¸

---

## ä»˜éŒ²

### A. ç”¨èªé›†

| ç”¨èª | èª¬æ˜ |
|------|------|
| **IRL** | Inverse Reinforcement Learningï¼ˆé€†å¼·åŒ–å­¦ç¿’ï¼‰ |
| **è»Œè·¡ï¼ˆTrajectoryï¼‰** | é–‹ç™ºè€…ã®éå»ã®æ´»å‹•è¨˜éŒ² + ç¶™ç¶šãƒ©ãƒ™ãƒ« |
| **çŠ¶æ…‹ï¼ˆStateï¼‰** | é–‹ç™ºè€…ã®ç¾åœ¨ã®çŠ¶æ³ï¼ˆ32æ¬¡å…ƒãƒ™ã‚¯ãƒˆãƒ«ï¼‰ |
| **è¡Œå‹•ï¼ˆActionï¼‰** | é–‹ç™ºè€…ã®å€‹åˆ¥ã®æ´»å‹•ï¼ˆ9æ¬¡å…ƒãƒ™ã‚¯ãƒˆãƒ«ï¼‰ |
| **å ±é…¬é–¢æ•°** | ç¶™ç¶šã«å¯„ä¸ã™ã‚‹åº¦åˆã„ã‚’è¡¨ã™é–¢æ•° |
| **ç¶™ç¶šç¢ºç‡** | 6ãƒ¶æœˆå¾Œã‚‚è²¢çŒ®ã—ã¦ã„ã‚‹ç¢ºç‡ï¼ˆ0-1ï¼‰ |
| **AUC-ROC** | Receiver Operating Characteristic curveä¸‹ã®é¢ç© |
| **AUC-PR** | Precision-Recall curveä¸‹ã®é¢ç© |
| **LSTM** | Long Short-Term Memoryï¼ˆé•·çŸ­æœŸè¨˜æ†¶ï¼‰ |
| **MinMaxScaler** | 0-1ç¯„å›²ã«æ­£è¦åŒ–ã™ã‚‹ã‚¹ã‚±ãƒ¼ãƒ©ãƒ¼ |
| **NaN** | Not a Numberï¼ˆéæ•°ï¼‰ |

### B. ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°

#### Q1: è¨“ç·´ä¸­ã«NaNãŒç™ºç”Ÿã™ã‚‹

**A**: MinMaxScaler + NaN/Infå¯¾ç­–ã‚’ä½¿ç”¨ã—ã¦ã„ã‚‹ã“ã¨ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚

```python
# enhanced_feature_extractor.py Line 17
from sklearn.preprocessing import MinMaxScaler  # StandardScaler ã§ã¯ãªã„
```

#### Q2: ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ã§ããªã„

**A**: ãƒ¢ãƒ‡ãƒ«ã® `config` ã‚’ç¢ºèªã—ã€åŒã˜è¨­å®šã§ã‚·ã‚¹ãƒ†ãƒ ã‚’åˆæœŸåŒ–ã—ã¦ãã ã•ã„ã€‚

```python
checkpoint = torch.load('model.pth')
config = checkpoint['config']
print(f"state_dim: {config['state_dim']}")
print(f"sequence: {config.get('sequence', False)}")
```

#### Q3: äºˆæ¸¬ç¢ºç‡ãŒå¸¸ã«0.5ä»˜è¿‘

**A**: ãƒ¢ãƒ‡ãƒ«ãŒå­¦ç¿’ã§ãã¦ã„ãªã„å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚è¨“ç·´ãƒ­ã‚°ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚

```bash
# æå¤±ãŒæ¸›å°‘ã—ã¦ã„ã‚‹ã‹ç¢ºèª
cat logs/training.log | grep "å¹³å‡æå¤±"
```

---

**æœ€çµ‚æ›´æ–°**: 2025-10-17
**ãƒãƒ¼ã‚¸ãƒ§ãƒ³**: 1.0
**ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹**: å®Ÿé¨“å®Œäº†ã€æ¬¡ãƒ•ã‚§ãƒ¼ã‚ºæº–å‚™ä¸­
