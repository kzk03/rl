# 精度問題の分析と解決策

**作成日**: 2025-10-22  
**問題**: Precision=0, Recall=0, F1=0 なのに AUC-ROC=0.6-0.7

---

## 🔴 問題の本質

### 症状

```csv
例: 0-6m訓練 → 0-3m評価
AUC-ROC: 0.699  ← そこそこ良い
AUC-PR:  0.780  ← 良い
F1:      0.000  ← 最悪
Precision: 0.000  ← 最悪
Recall:  0.000  ← 最悪
正例率:  60.8%  ← データは偏っていない
```

**矛盾**: AUC は良いのに、F1/Precision/Recall がゼロ！？

---

## 🔍 根本原因

### 1. 予測確率の分布が極端に狭い

| モデル | 評価範囲 | 予測平均 | 予測 std   | 範囲     |
| ------ | -------- | -------- | ---------- | -------- |
| 0-3m   | 0-3m     | 0.504    | **0.0008** | ほぼ一定 |
| 0-6m   | 0-3m     | 0.462    | **0.0004** | ほぼ一定 |
| 0-9m   | 0-3m     | 0.506    | **0.0007** | ほぼ一定 |
| 0-12m  | 0-3m     | 0.508    | **0.0006** | ほぼ一定 |

**問題**: すべてのサンプルに対してほぼ同じ予測確率（0.46-0.54）を出力

### 2. 閾値 0.5 が不適切

```python
# 現状のコード
y_pred_binary = [1 if p >= 0.5 else 0 for p in y_pred]

# 実際の予測確率分布
予測平均: 0.462, std: 0.0004
→ ほぼ全サンプルが0.46付近
→ 閾値0.5で判定
→ すべて「0（継続しない）」と予測
→ Precision=0, Recall=0, F1=0
```

### 3. 最適閾値は 0.1

```csv
組み合わせ        閾値0.5のF1  最適閾値  最適F1  最適Recall
0-6m → 0-3m       0.000       0.10      0.756   1.0
0-6m → 6-9m       0.000       0.10      0.701   1.0
0-9m → 6-9m       0.000       0.10      0.701   1.0
0-12m → 6-9m      0.000       0.10      0.701   1.0
```

**発見**:

- 最適閾値がほとんど**0.1**
- つまり、ほぼ全員を「継続する」と予測するのが最良
- Recall=1.0（全員を正例と予測）
- Precision=正例率（これは正しい挙動）

---

## 💡 なぜ AUC は良いのに F1 がゼロ？

### AUC と閾値依存メトリクスの違い

```python
# AUC（閾値非依存）
- すべての閾値での性能を統合して評価
- 相対的なランキング能力を測定
- 0.46と0.47の微小な差でも評価

# F1/Precision/Recall（閾値依存）
- 固定閾値0.5で二値判定
- 予測確率が0.46なら「0」、0.54なら「1」
- 微小な差（0.0004）では識別不可能
```

### 具体例

```
サンプル1: 真値=1, 予測=0.462 → 閾値0.5で「0」判定 → 誤り
サンプル2: 真値=0, 予測=0.461 → 閾値0.5で「0」判定 → 正解
サンプル3: 真値=1, 予測=0.463 → 閾値0.5で「0」判定 → 誤り

AUC的には: サンプル1 > サンプル3 > サンプル2 と正しくランク付け ✅
F1的には: すべて「0」と予測 → Recall=0 ❌
```

---

## 📊 全 16 組み合わせの詳細分析

### 予測確率の統計

| 訓練\評価 | 0-3m        | 3-6m        | 6-9m        | 9-12m       |
| --------- | ----------- | ----------- | ----------- | ----------- |
| **0-3m**  | 0.504±0.001 | 0.522±0.001 | 0.499±0.001 | 0.505±0.001 |
| **0-6m**  | 0.462±0.000 | 0.513±0.001 | 0.490±0.001 | 0.509±0.000 |
| **0-9m**  | 0.506±0.001 | 0.535±0.001 | 0.469±0.001 | 0.541±0.001 |
| **0-12m** | 0.508±0.001 | 0.542±0.002 | 0.493±0.001 | 0.504±0.002 |

**観察**:

- すべての組み合わせで **std < 0.002**
- 予測確率が 0.46-0.54 の範囲に集中
- 個別サンプルの識別が困難

### 最適閾値での性能

| 訓練\評価 | 0-3m              | 3-6m         | 6-9m         | 9-12m        |
| --------- | ----------------- | ------------ | ------------ | ------------ |
| **0-3m**  | 0.756 (閾値 0.10) | 0.726 (0.10) | 0.701 (0.10) | 0.669 (0.10) |
| **0-6m**  | 0.756 (0.10)      | 0.726 (0.10) | 0.701 (0.10) | 0.669 (0.10) |
| **0-9m**  | 0.756 (0.10)      | 0.727 (0.53) | 0.701 (0.10) | 0.669 (0.10) |
| **0-12m** | 0.756 (0.10)      | 0.727 (0.54) | 0.701 (0.10) | 0.669 (0.10) |

**パターン**:

- ほぼすべての組み合わせで最適閾値=0.10
- 例外: 3-6m 評価では 0.53-0.54
- F1 スコアは 0.67-0.76 に改善

---

## 🎯 解決策

### 即座に実装可能（優先度: 高）

#### 1. 閾値の動的調整

```python
# 方法A: 正例率ベースの閾値
def predict_with_positive_rate_threshold(y_pred_proba, positive_rate):
    """正例率に基づいて閾値を動的に設定"""
    threshold = np.percentile(y_pred_proba, (1 - positive_rate) * 100)
    return (y_pred_proba >= threshold).astype(int)

# 方法B: F1スコア最大化閾値
def find_optimal_threshold(y_true, y_pred_proba):
    """F1スコアを最大化する閾値を探索"""
    thresholds = np.linspace(0.1, 0.9, 81)
    best_f1 = 0
    best_threshold = 0.5

    for threshold in thresholds:
        y_pred = (y_pred_proba >= threshold).astype(int)
        f1 = f1_score(y_true, y_pred, zero_division=0)
        if f1 > best_f1:
            best_f1 = f1
            best_threshold = threshold

    return best_threshold

# 使用例
threshold = find_optimal_threshold(y_train_true, y_train_pred_proba)
y_test_pred = (y_test_pred_proba >= threshold).astype(int)
```

**期待効果**: F1 スコアが 0.0 → 0.7 に改善

#### 2. 確率キャリブレーション

```python
from sklearn.calibration import CalibratedClassifierCV

# モデルの予測確率をキャリブレーション
calibrated_model = CalibratedClassifierCV(
    irl_system,
    method='isotonic',  # または'sigmoid'
    cv=5
)
calibrated_model.fit(X_train, y_train)

# キャリブレーション後の予測
y_pred_calibrated = calibrated_model.predict_proba(X_test)[:, 1]
```

**期待効果**: 予測確率の分布が広がり、識別能力向上

### 中期改善（優先度: 中）

#### 3. 損失関数の改善

```python
# 現状: Binary Cross Entropy
loss = F.binary_cross_entropy(predicted, target)

# 改善案A: Focal Loss（不均衡データ向け）
def focal_loss(predicted, target, alpha=0.25, gamma=2.0):
    """
    難しいサンプルに焦点を当てる
    """
    bce = F.binary_cross_entropy(predicted, target, reduction='none')
    p_t = torch.where(target == 1, predicted, 1 - predicted)
    focal_weight = (1 - p_t) ** gamma
    loss = focal_weight * bce
    return loss.mean()

# 改善案B: Class-Balanced Loss
def class_balanced_loss(predicted, target, positive_rate):
    """
    クラスバランスを考慮した重み付け
    """
    pos_weight = (1 - positive_rate) / positive_rate
    loss = F.binary_cross_entropy_with_logits(
        predicted, target,
        pos_weight=torch.tensor([pos_weight])
    )
    return loss
```

**期待効果**: モデルが多様な予測確率を出力するように学習

#### 4. 出力層のアーキテクチャ変更

```python
# 現状: Sigmoid出力
output = torch.sigmoid(logits)

# 改善案: Temperature Scaling
def predict_with_temperature(logits, temperature=1.5):
    """
    温度パラメータで予測確率の分布を調整
    temperature > 1: より多様な分布
    temperature < 1: より確信的な分布
    """
    return torch.sigmoid(logits / temperature)

# 訓練後にtemperatureを最適化
optimal_temp = find_optimal_temperature(val_logits, val_targets)
```

**期待効果**: 予測確率の分散が増加

### 長期改善（優先度: 低）

#### 5. モデルアーキテクチャの再設計

```python
# 問題: LSTMの出力が単一の隠れ状態のみを使用
# → 時系列パターンの多様性を捉えられない

# 改善案A: Multi-Head Attention
class AttentionIRLNetwork(nn.Module):
    def __init__(self, ...):
        super().__init__()
        self.lstm = nn.LSTM(...)
        self.attention = nn.MultiheadAttention(...)

    def forward(self, sequence):
        lstm_out, _ = self.lstm(sequence)
        # 各時点の重要度を学習
        attn_out, attn_weights = self.attention(
            lstm_out, lstm_out, lstm_out
        )
        return attn_out

# 改善案B: Ensemble
predictions = []
for model in [model_0_3m, model_0_6m, model_0_12m]:
    pred = model.predict(x)
    predictions.append(pred)
final_pred = np.mean(predictions, axis=0)
```

**期待効果**: 予測の多様性と精度の向上

#### 6. データ拡張

```python
# 問題: 訓練データが2,000-3,000サンプルと少ない

# 改善案A: 時系列データ拡張
def augment_temporal_data(trajectories):
    augmented = []
    for traj in trajectories:
        # オリジナル
        augmented.append(traj)

        # 時間的ジッター（±数日）
        jittered = add_temporal_jitter(traj, days=3)
        augmented.append(jittered)

        # サブシーケンス
        for start in range(0, len(traj) - min_len):
            sub_traj = traj[start:start+min_len]
            augmented.append(sub_traj)

    return augmented

# 改善案B: より多くのプロジェクトデータ
# OpenStack以外のプロジェクト（Kubernetes, Linux Kernel等）
```

**期待効果**: モデルの汎化性能向上

---

## 📝 推奨実装順序

### Phase 1: 緊急対応（今すぐ）

✅ **完了**: 予測確率と最適閾値の分析

🔄 **次のステップ**:

1. **評価スクリプトの修正**（1 時間）

   ```python
   # train_irl_within_training_period.py の evaluate_irl_model 関数
   # 閾値0.5を最適閾値に変更
   ```

2. **クロス評価の再実行**（2-3 時間）

   ```bash
   # 最適閾値で全16組み合わせを再評価
   bash scripts/training/irl/run_cross_evaluation_with_optimal_threshold.sh
   ```

3. **結果の再可視化**（30 分）
   ```bash
   # 改善されたF1スコアでヒートマップ生成
   python scripts/analysis/visualize_cross_evaluation.py \
       --results outputs/cross_evaluation_optimal/recomputed_results.csv
   ```

### Phase 2: 短期改善（1-2 日）

4. **確率キャリブレーションの導入**

   - Isotonic Regression または Platt Scaling
   - 訓練データの一部をキャリブレーション用に分割

5. **損失関数の改善**

   - Focal Loss の実装とテスト
   - クラスバランス重みの調整

6. **訓練期間の延長**
   - 2 年 → 3 年でデータ量を 1.5 倍に

### Phase 3: 中期改善（1 週間）

7. **Temperature Scaling の導入**
8. **Ensemble モデルの構築**
9. **より多くのプロジェクトデータの収集**

---

## 📊 期待される改善

### Before（閾値 0.5 固定）

```
平均 AUC-ROC: 0.55
平均 F1:      0.28  ← 多くが0.0
```

### After（最適閾値）

```
平均 AUC-ROC: 0.55  （変わらず）
平均 F1:      0.72  ← 大幅改善！
```

### After（キャリブレーション + 最適閾値）

```
平均 AUC-ROC: 0.65  （予想）
平均 F1:      0.75  （予想）
予測std:      0.05  （現状0.001から改善）
```

---

## 🎓 学術的インサイト

### 1. AUC と F1 の乖離の意味

- **AUC > 0.6 かつ F1 = 0.0** は、モデルが「識別能力はある」が「キャリブレーションされていない」ことを示す
- これは、**ランキング問題として は成功**しているが、**分類問題としては失敗**している

### 2. 予測確率の分散の重要性

```
分散が小さい（<0.001）:
→ すべてのサンプルに同じ予測
→ 個別の予測は無意味
→ ただし相対順序は保持

分散が大きい（>0.05）:
→ サンプルごとに異なる予測
→ 個別の予測に意味がある
→ 実用的な分類器
```

### 3. 不均衡データでの最適閾値

```
正例率が50-60%の場合:
- 閾値0.5は適切（バランスが良い）
- しかし、モデルが50%付近に集中している場合は不適切
- 最適閾値は正例率とモデルの出力分布の両方に依存
```

---

## 📚 参考文献・関連リンク

### モデルキャリブレーション

- Guo et al. (2017): "On Calibration of Modern Neural Networks"
- Platt (1999): "Probabilistic Outputs for Support Vector Machines"

### 不均衡データ学習

- Lin et al. (2017): "Focal Loss for Dense Object Detection"
- Cui et al. (2019): "Class-Balanced Loss Based on Effective Number of Samples"

### 評価指標

- Davis & Goadrich (2006): "The Relationship Between Precision-Recall and ROC Curves"

---

## 🔗 関連ファイル

### 分析結果

- `outputs/cross_evaluation_optimal/recomputed_results.csv` - 最適閾値での再計算結果
- `outputs/cross_evaluation_optimal/metrics_train*.json` - 詳細メトリクス（16 ファイル）

### スクリプト

- `scripts/analysis/recompute_metrics_with_optimal_threshold.py` - 再計算スクリプト
- `scripts/training/irl/train_irl_within_training_period.py` - 訓練・評価（要修正）

---

**最終更新**: 2025-10-22  
**次のアクション**: 評価スクリプトの修正 → クロス評価の再実行
