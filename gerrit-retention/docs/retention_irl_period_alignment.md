# 長期貢献者予測と IRL 学習期間の対応方針

## 概要

長期貢献者予測（Retention Prediction）と Inverse Reinforcement Learning（IRL）の学習期間を対応させることで、より一貫性のある予測システムを構築する。

## 現在の状況

### Retention Prediction

- **予測期間**: 12 ヶ月（`target_window_months: 12`）
- **特徴量期間**: 過去 18 ヶ月（`feature_history_months: 18`）
- **予測対象**: 開発者が今後 12 ヶ月間に継続的に貢献するか

### IRL System

- **学習データ**: オフライン RL データ（JSONL 形式）
- **シーケンス長**: 可変（デフォルト 10）
- **予測対象**: 継続確率（`continuation_prob`）

## 対応方針

### 1. 期間の統一

IRL の学習期間を Retention Prediction の**特徴量期間**に合わせる：

- **IRL 学習期間**: 18 ヶ月相当の時系列データ
- **シーケンス長**: 18 タイムステップ（月単位）
- **予測対象**: 各タイムステップでの継続確率
- **対応関係**: IRL のシーケンス = Retention の特徴量期間

### 2. データ構造の設計

#### IRL 学習データの期間対応

```json
{
  "developer_id": "user@example.com",
  "transitions": [
    {
      "t": "2023-04-01T00:00:00", // 18ヶ月前の開始
      "state": [
        /* 20次元特徴量 */
      ],
      "action": 1,
      "reward": 1.0,
      "next_state": [
        /* 20次元特徴量 */
      ],
      "done": false
    },
    // ... 18ヶ月分のトランジション
    {
      "t": "2024-10-01T00:00:00", // 現在時刻
      "state": [
        /* 20次元特徴量 */
      ],
      "action": 1,
      "reward": 1.0,
      "next_state": null,
      "done": true // 予測対象期間の終了
    }
  ]
}
```

#### 期間のマッピング

- **IRL シーケンス**: 過去 18 ヶ月分の時系列データ
- **タイムステップ**: 月単位（30 日間隔）
- **予測対象**: 各月の継続確率 → 全体として 18 ヶ月の貢献パターン

### 3. 実装アプローチ

#### A. データ生成パイプライン

1. **Raw データ収集**: Gerrit changes データを抽出
2. **時系列シーケンス生成**: 開発者ごとの月次活動履歴
3. **期間フィルタリング**: 過去 18 ヶ月 + 未来 12 ヶ月のデータのみ使用
4. **特徴量エンジニアリング**: 月次特徴量を 20 次元に集約
5. **RL データ変換**: 各月の状態遷移を(s,a,r,s')に変換

#### B. IRL モデルの調整

- **シーケンス長**: `seq_len = 18`（18 ヶ月）
- **状態次元**: 20（既存）
- **行動次元**: 3（既存）
- **報酬設計**: 月次貢献を正報酬、不活動を負報酬
- **予測出力**: 18 ヶ月シーケンス全体の継続パターン

#### C. 評価方法

- **IRL 特徴量**: 18 ヶ月シーケンスの継続パターン → Retention Prediction の入力特徴量
- **統合アプローチ**: IRL 学習したパターンを Retention モデルの特徴量として使用
- **比較評価**: IRL 特徴量を含むモデル vs 従来モデルの比較
- **期間一致**: 両モデルの特徴量期間を 18 ヶ月に統一

### 4. 技術的考慮点

#### データ量と品質

- **最小データ要件**: 各開発者につき最低 30 ヶ月分のデータ
- **データ分割**: 時系列順に train/eval 分割
- **欠損処理**: 月次データの補間または除外

#### 特徴量設計

- **月次集約**: コミット数、レビュー数、レスポンスタイムなどの月次統計
- **トレンド特徴**: 活動量の時系列トレンド
- **相対特徴**: プロジェクト内での相対的な貢献度

#### モデル学習

- **バッチサイズ**: シーケンス長に応じて調整
- **学習率**: 時系列データの安定性を考慮
- **正則化**: 過学習防止のため適切な正則化

### 5. 実験計画: 期間変化による精度評価

#### 目的

IRL 学習期間と Retention Prediction 特徴量期間の対応関係を変化させながら、最適な期間設定を探索する。

#### 実験設定

- **変数**: IRL シーケンス長（学習期間）
- **水準**: 3 ヶ月, 6 ヶ月, 12 ヶ月, 24 ヶ月
- **対応関係**: IRL シーケンス長 = Retention 特徴量期間
- **評価指標**: AUC, PR-AUC, Accuracy, F1-Score

#### 実験プロトコル

1. **データ準備**: 各期間設定ごとに学習データを生成
2. **IRL 学習**: 各シーケンス長で IRL モデルを学習
3. **特徴量抽出**: 学習した IRL モデルから時系列パターンを特徴量として抽出
4. **Retention 予測**: IRL 特徴量を追加したモデルで予測
5. **評価**: 各期間設定での予測精度を比較

#### 期待される知見

- **最適期間**: 予測精度が最も高くなる IRL 学習期間
- **期間効果**: 学習期間が長いほど精度が向上するか
- **計算コスト**: 期間変化による学習・推論時間のトレードオフ
- **特徴量貢献**: IRL 特徴量が従来特徴量に比べてどの程度寄与するか

#### 結果解釈

- **3 ヶ月**: 非常に短期パターン → 即時貢献者の予測
- **6 ヶ月**: 短期パターン学習 → 新規参加者の定着予測
- **12 ヶ月**: 中期パターン → 標準的な貢献期間の予測
- **24 ヶ月**: 長期パターン → コア貢献者の長期定着予測

#### 実装考慮点

- **データ可用性**: 十分な長さの時系列データが必要
- **計算リソース**: 長いシーケンスほどメモリ使用量が増大
- **クロスバリデーション**: 時系列データの分割に注意
- **統計的有意性**: 複数回の実行による分散評価

#### Phase 1: データ準備

1. 既存の Gerrit データから 12 ヶ月期間のデータを抽出
2. 月次特徴量を計算
3. IRL 学習用 JSONL データを生成

#### Phase 2: モデル調整

1. IRL システムのシーケンス長を 12 に設定
2. 学習スクリプトの期間パラメータを追加
3. 評価スクリプトの期間対応を実装

#### Phase 3: 統合評価

1. IRL 学習した 18 ヶ月パターンを特徴量として抽出
2. Retention Prediction モデルに IRL 特徴量を追加
3. AUC/PR-AUC などのメトリクスで評価
4. 特徴量期間一致による予測精度の改善を確認

### 6. 実装ステップ

#### Phase 1: データ準備

1. 既存の Gerrit データから 18 ヶ月期間のデータを抽出
2. 月次特徴量を計算
3. IRL 学習用 JSONL データを生成

#### Phase 2: モデル調整

1. IRL システムのシーケンス長を 18 に設定
2. 学習スクリプトの期間パラメータを追加
3. 評価スクリプトの期間対応を実装

#### Phase 3: 統合評価

1. IRL 学習した 18 ヶ月パターンを特徴量として抽出
2. Retention Prediction モデルに IRL 特徴量を追加
3. AUC/PR-AUC などのメトリクスで評価
4. 特徴量期間一致による予測精度の改善を確認

### 8. 期待される効果

- **特徴量の強化**: IRL 学習した時系列パターンを Retention Prediction の特徴量として活用
- **解釈性の向上**: IRL の報酬関数による貢献動機の定量評価
- **予測精度の向上**: 時系列依存を考慮したより豊かな特徴表現
- **システム統合**: IRL と Retention Prediction のシームレスな連携

### 9. リスクと対策

#### データ不足

- **対策**: シミュレーションデータでの事前検証
- **代替**: より短い期間（6 ヶ月）から開始

#### 計算コスト

- **対策**: GPU 活用、バッチ処理の最適化
- **代替**: シーケンス長の段階的増加

#### モデル複雑度

- **対策**: アブレーション実験による特徴量削減
- **代替**: 単純な時系列モデルとの比較

## 結論

長期貢献者予測と IRL 学習期間の対応により、より堅牢で解釈可能な予測システムを実現できる。本方針に基づき、段階的に実装を進めていく。</content>
<parameter name="filePath">/Users/kazuki-h/rl/gerrit-retention/docs/retention_irl_period_alignment.md
