# 訓練データ構造の再確認

## 実際の訓練データ構造

### `extract_sliding_window_trajectories` の実装

```python
# 各レビュアーについて
for _, row in reviewer_history_sorted.iterrows():
    # 【重要】各活動を個別に保持
    activity = {
        'timestamp': row[date_col],
        'action_type': 'review',
        'project': row.get('project', 'unknown'),
    }
    activity_history.append(activity)  # 個別の活動
    step_labels.append(monthly_labels[month_key])  # 月次ラベル
```

**結果**:
- `activity_history`: 活動単位（例: 457個の個別活動）
- `step_labels`: 各活動に月次ラベルを付与（同じ月の活動は同じラベル）

## 評価データ構造

### `extract_cutoff_evaluation_trajectories` の実装

```python
# 各レビュアーについて
for _, row in reviewer_history.iterrows():
    activity = {
        'timestamp': row[date_col],
        'action_type': 'review',
        'project': row.get('project', 'unknown'),
    }
    activity_history.append(activity)  # 個別の活動

# レビュアー全体のラベル
future_contribution = len(reviewer_future) > 0
```

**結果**:
- `activity_history`: 活動単位（例: 457個の個別活動）
- `future_contribution`: レビュアー全体のラベル

## 違いはラベルの付け方だけ

| 項目 | 訓練データ | 評価データ |
|------|-----------|-----------|
| activity_history | 活動単位（457個） | 活動単位（457個） |
| ラベル | 各活動に月次ラベル | レビュアー全体に1つ |
| 形式 | ✅ 一致 | ✅ 一致 |

## では何が問題か？

### 訓練時のメトリクス（train_0-3m/metrics.json）

```json
{
  "precision": 0.923,
  "recall": 0.522,
  ...
}
```

これは**どのように計算されている**？

### 評価時のメトリクス（eval_0-3m/metrics.json）

```json
{
  "precision": 0.275,
  "recall": 1.000,
  ...
}
```

これは**どのように計算されている**？

## 仮説: メトリクス計算方法が異なる

### 訓練時

`train_irl_model_multi_step` で訓練後、同じ訓練データで評価？

### 評価時

`evaluate_model` で評価データを使用

## 確認すべきこと

1. **訓練時のmetrics.jsonはどこで計算されている？**
2. **評価時のmetrics.jsonはどこで計算されている？**
3. **予測確率の計算方法が訓練時と評価時で異なる？**
