# IRL 学習と予測の完全ガイド

## 📋 目次

1. [全体像](#全体像)
2. [学習フェーズ](#学習フェーズ)
3. [予測フェーズ](#予測フェーズ)
4. [Long-contributor 予測との対応](#long-contributor予測との対応)
5. [実装の詳細](#実装の詳細)

---

## 全体像

### 研究目的

**開発者の将来の継続可能性を予測する**

- 過去の活動履歴から、将来の一定期間内に貢献を続けるかを予測
- IRL を用いて「継続する開発者」と「継続しない開発者」のモチベーションの違いを学習

---

## 学習フェーズ

### 1. データの準備

```
学習期間: 2021-01-01 ~ 2023-01-01 (2年間)

サンプリング:
  サンプリング可能範囲: 2022-01-01 ~ 2022-10-01
  サンプリング間隔: 1ヶ月
  サンプリング時点数: 10時点
```

### 2. 各サンプリング時点での処理

**例: サンプリング時点 = 2022-05-01**

```
┌─────────────────────────────────────────┐
│ 履歴期間（過去12ヶ月）                    │
│ 2021-05-01 ~ 2022-05-01                 │
│                                         │
│ この期間に活動があった開発者を抽出        │
│ → 各開発者の活動履歴を取得              │
└─────────────────────────────────────────┘
              ↓
┌─────────────────────────────────────────┐
│ 将来窓（ラベル期間）                      │
│ 例: 0-3m → 2022-05-01 ~ 2022-08-01     │
│                                         │
│ この期間に貢献があるか？                  │
│ → ラベル: continued = True/False       │
└─────────────────────────────────────────┘
```

### 3. ラベル付け

**重要: 将来の貢献は「ラベル」として使用（状態特徴量ではない）**

```python
# 将来の貢献を計算（ラベルとして使用）
reviewer_future = future_df[future_df[reviewer_col] == reviewer]
future_contribution = len(reviewer_future) > 0  # True/False

trajectory = {
    'developer': developer_info,
    'activity_history': activity_history,  # 過去の履歴のみ
    'context_date': sampling_point,
    'future_contribution': future_contribution,  # ← ラベル
}
```

### 4. ラベル期間のバリエーション

訓練時に異なる将来窓でモデルを作成：

| 訓練ラベル | 期間の意味             | 例（sampling_point=2022-05-01） |
| ---------- | ---------------------- | ------------------------------- |
| 0-3m       | 直後 3 ヶ月以内に貢献  | 2022-05-01 ~ 2022-08-01         |
| 0-6m       | 直後 6 ヶ月以内に貢献  | 2022-05-01 ~ 2022-11-01         |
| 0-9m       | 直後 9 ヶ月以内に貢献  | 2022-05-01 ~ 2023-02-01         |
| 0-12m      | 直後 12 ヶ月以内に貢献 | 2022-05-01 ~ 2023-05-01         |

### 5. 学習プロセス

```
入力:
  - 開発者の状態（過去の活動から抽出した特徴）
  - 時系列シーケンス（LSTM入力、seq_len=20ステップ）

LSTM処理:
  各タイムステップ（t=1, 2, ..., 20）で状態を再構築
  → 隠れ状態を更新
  → 最後のステップ（t=20）の隠れ状態を使用

出力:
  - 継続確率: Pr(継続する)

ラベル:
  ⚠️ 重要: 現在は最後のステップのみにラベル

  現状:
    context_date（最後のステップ）から見た
    将来0-3m後の貢献のみ
    → 1つのラベル

  理想（将来の改善）:
    各タイムステップから見た将来の貢献
    → 20個のラベル（各ステップで1つ）
    → より豊富な学習信号

損失関数:
  - Focal Loss（クラス不均衡対応）
  - ターゲット: future_contribution (True/False)
  - 計算: 最後のステップの予測 vs ラベル

学習目標:
  「継続する開発者」と「継続しない開発者」を
  区別できる報酬関数を学習
```

---

## 予測フェーズ

### 1. 予測の実行タイミング

**Long-contributor 予測と同じ方式**

```
Cutoff日: 2023-01-01（訓練終了日）

┌─────────────────────────────────────────┐
│ 履歴期間（過去12ヶ月）                    │
│ 2022-01-01 ~ 2023-01-01                │
│                                         │
│ この期間に活動があった全開発者を対象      │
│ ↓                                       │
│ - 2022年1月に活動 → 対象               │
│ - 2022年6月に活動 → 対象               │
│ - 2022年12月に活動 → 対象              │
│ - 最近活動していない人も含む             │
└─────────────────────────────────────────┘
```

### 2. 予測対象者の選定

```python
# 履歴期間に活動があった全開発者を取得
history_df = df[(df['date'] >= history_start) & (df['date'] < history_end)]
active_reviewers = history_df['reviewer'].unique()

# ✓ この期間に活動があれば全員が対象
# ✗ cutoff前後1ヶ月に活動がある人だけ（間違った実装）
```

**重要: 最近活動していない人も予測対象に含める**

- これが Long-contributor 予測と同じ方式
- ドロップアウトしそうな人も識別できる

### 3. 予測する期間

Cutoff 日からの異なる将来期間を評価：

| 評価範囲 | 期間の意味                  | 例（cutoff=2023-01-01） |
| -------- | --------------------------- | ----------------------- |
| 0-3m     | 直後 3 ヶ月以内に貢献するか | 2023-01-01 ~ 2023-04-01 |
| 3-6m     | 3-6 ヶ月後に貢献するか      | 2023-04-01 ~ 2023-07-01 |
| 6-9m     | 6-9 ヶ月後に貢献するか      | 2023-07-01 ~ 2023-10-01 |
| 9-12m    | 9-12 ヶ月後に貢献するか     | 2023-10-01 ~ 2024-01-01 |

### 4. 予測の流れ

```
入力:
  開発者 A の過去12ヶ月の活動履歴
  (2022-01-01 ~ 2023-01-01)

  ↓

モデル:
  LSTM + 継続予測器

  ↓

出力:
  継続確率: Pr(A が将来の期間に貢献する)
  例: 0.85 → 85%の確率で継続する
```

### 5. クロス評価

異なる訓練ラベルと評価範囲の組み合わせ：

```
訓練ラベル: 0-3m, 0-6m, 0-9m, 0-12m
評価範囲: 0-3m, 3-6m, 6-9m, 9-12m

総実験数: 4 × 4 = 16通り

例:
  0-3m訓練 × 0-3m評価
  0-3m訓練 × 9-12m評価
  0-12m訓練 × 0-3m評価
  ...
```

---

## Long-contributor 予測との対応

### 類似点

| 項目       | Long-contributor 予測          | 本研究（IRL） |
| ---------- | ------------------------------ | ------------- |
| 予測対象者 | 履歴期間に活動があった全開発者 | ✓ 同じ        |
| 予測内容   | 将来も貢献を続けるか           | ✓ 同じ        |
| 履歴期間   | 過去 N 年間                    | 過去 12 ヶ月  |
| 予測期間   | N 年後                         | 0-12 ヶ月後   |

### 相違点

| 項目           | Long-contributor 予測      | 本研究（IRL）     |
| -------------- | -------------------------- | ----------------- |
| 手法           | 教師あり学習（SVM, RF 等） | 逆強化学習（IRL） |
| 学習目標       | 分類境界                   | 報酬関数          |
| 時系列処理     | 特徴量集約                 | LSTM              |
| モチベーション | 考慮しない                 | 考慮する          |

### 実装の対応

```python
# Long-contributor予測の典型的な実装
def predict_long_contributors(df, cutoff_date, history_months=12, future_months=12):
    # 1. 履歴期間に活動があった開発者
    history_start = cutoff_date - pd.DateOffset(months=history_months)
    history_df = df[(df['date'] >= history_start) & (df['date'] < cutoff_date)]
    developers = history_df['developer'].unique()

    # 2. 各開発者の特徴量を抽出
    for dev in developers:
        features = extract_features(dev, history_df)

        # 3. 将来の貢献をラベルとして取得
        future_start = cutoff_date
        future_end = cutoff_date + pd.DateOffset(months=future_months)
        future_df = df[(df['date'] >= future_start) & (df['date'] < future_end)]
        label = dev in future_df['developer'].values

    # 4. 予測
    predictions = model.predict(features)
    return predictions

# ✓ 本研究の実装も同じ構造
```

---

## 実装の詳細

### データの流れ

```
生データ
  ↓
[学習期間内完結型の軌跡抽出]
  - サンプリング（訓練時のみ）
  - 履歴期間に活動があった開発者を対象（予測時）
  ↓
軌跡 (trajectory)
  {
    'developer': {...},
    'activity_history': [...],  # 過去の履歴のみ
    'context_date': cutoff_date,
    'future_contribution': True/False  # ラベル
  }
  ↓
[IRL訓練]
  - 状態・行動を抽出
  - LSTMで時系列パターンを学習
  - Focal Lossで訓練
  ↓
訓練済みモデル
  ↓
[予測]
  - cutoff時点の開発者を評価
  - 継続確率を出力
  ↓
評価指標
  - AUC-ROC, Precision, Recall, F1
```

### 重要な設定パラメータ

```bash
# 学習
HISTORY_WINDOW=12      # 履歴期間（ヶ月）
FUTURE_START=0         # 将来窓の開始（ヶ月）
FUTURE_END=3           # 将来窓の終了（ヶ月）
EPOCHS=50              # 訓練エポック数
SEQ_LEN=50             # LSTMシーケンス長
MIN_HISTORY_EVENTS=3   # 最小活動回数

# 訓練期間
TRAIN_START=2021-01-01
TRAIN_END=2023-01-01

# 評価
CUTOFF_DATE=${TRAIN_END}  # 訓練終了日がcutoff
```

### 継続率の期待値

```
訓練データ:
  0-3m:  50-60%
  0-6m:  60-70%
  0-9m:  70-75%
  0-12m: 75-80%

評価データ（cutoff時点）:
  0-3m:  50-65%  ← 訓練と近い値であるべき
  3-6m:  45-60%
  6-9m:  40-55%
  9-12m: 35-50%  ← 時間が経つほど減少

⚠️ 評価データの継続率が90%以上:
  → 対象者選定が間違っている
  → アクティブな人だけを選んでいる
```

---

## まとめ

### ✅ 正しい実装のポイント

1. **予測対象者**

   - 履歴期間（過去 12 ヶ月）に活動があった全開発者
   - ✗ cutoff 前後 1 ヶ月に活動がある人だけ

2. **ラベル付け**

   - 将来の期間（0-3m 等）に貢献があるか
   - 状態特徴量には含めない

3. **時系列学習**

   - LSTM で過去の活動パターンを学習
   - 各タイムステップで状態を再構築

4. **サンプリング**

   - 訓練時: 複数時点でサンプリング（データ増強）
   - 予測時: cutoff 時点のみ（Long-contributor 予測と同じ）

5. **継続率**
   - 訓練と評価で大きく乖離しない
   - 50-70%程度が自然

### 📊 現在の最良結果

```
モデル: 0-3m訓練
評価: 0-3m予測

サンプル数: 260
継続率: 61.5%
AUC-ROC: 0.705
Precision: 0.935
F1: 0.967

→ 実用可能なレベル
```

### 🎯 今後の改善方向

1. **各タイムステップでのラベル付け（重要）**

   ```
   現状: 最後のステップのみ
     context_date から見た将来の貢献
     → 1つのラベル

   改善: 各ステップでラベル
     各タイムステップから見た将来の貢献
     → seq_len個のラベル（例: 20個）
     → より豊富な学習信号

   実装:
     - 各ステップの日付を計算
     - 各ステップから将来窓の貢献を確認
     - 各ステップの隠れ状態から予測
     - 全ステップの損失を合計

   期待効果:
     - より多くの情報を活用
     - 予測精度の向上
     - データ増強的効果
   ```

2. **seq_len の最適化（現在テスト中）**

   - 現状: seq_len=20
   - 中央値: 73 イベント
   - 推奨: seq_len=50-100

3. **特徴量の追加**

   - プロジェクトの人気度
   - 開発者のコミュニティ関与度
   - レビューの質
   - コードの複雑度

4. **より多くのデータ**

   - 訓練期間の延長
   - 複数プロジェクトの統合

5. **アンサンブル学習**
   - 複数モデルの予測を統合
   - バギング、ブースティング
