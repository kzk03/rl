# 閾値統一実装の詳細

**作成日**: 2025-10-22  
**目的**: クロス評価での公平な比較のため、訓練データで決定した閾値を全評価で統一使用

---

## 🎯 実装の概要

### Before（問題）

```
訓練0-3m → 評価0-3m: 最適閾値0.10 で F1=0.756
訓練0-3m → 評価3-6m: 最適閾値0.10 で F1=0.726
訓練0-6m → 評価0-3m: 最適閾値0.10 で F1=0.756
訓練0-6m → 評価3-6m: 最適閾値0.10 で F1=0.726
```

**問題点**: 各評価で個別に最適閾値を探索
→ テストデータに対する過学習（情報リーク）
→ 公平な比較が不可能

### After（解決策）

```
【訓練フェーズ】
訓練0-3m: 訓練データで最適閾値0.10を決定 ✅
訓練0-6m: 訓練データで最適閾値0.46を決定 ✅

【評価フェーズ】
訓練0-3m → 評価0-3m: 固定閾値0.10で評価 ✅
訓練0-3m → 評価3-6m: 固定閾値0.10で評価 ✅
訓練0-6m → 評価0-3m: 固定閾値0.46で評価 ✅
訓練0-6m → 評価3-6m: 固定閾値0.46で評価 ✅
```

**改善点**:

- ✅ 訓練データのみで閾値決定
- ✅ 同じモデルの全評価で同じ閾値を使用
- ✅ 情報リークなし
- ✅ 公平な比較が可能

---

## 📝 実装の詳細

### 1. 評価関数の修正（`train_irl_within_training_period.py`）

#### 最適閾値探索関数の追加

```python
def find_optimal_threshold(y_true, y_pred_proba):
    """
    F1スコアを最大化する閾値を探索
    """
    thresholds = np.linspace(0.1, 0.9, 81)
    best_f1 = 0
    best_threshold = 0.5
    best_metrics = None

    for threshold in thresholds:
        y_pred_binary = [1 if p >= threshold else 0 for p in y_pred_proba]

        try:
            f1 = f1_score(y_true, y_pred_binary, zero_division=0)
            if f1 > best_f1:
                best_f1 = f1
                best_threshold = threshold
                best_metrics = {
                    'f1': f1,
                    'precision': precision_score(y_true, y_pred_binary, zero_division=0),
                    'recall': recall_score(y_true, y_pred_binary, zero_division=0),
                }
        except:
            continue

    return best_threshold, best_metrics
```

#### 評価関数の修正

```python
def evaluate_irl_model(
    irl_system: RetentionIRLSystem,
    test_trajectories: List[Dict[str, Any]],
    fixed_threshold: float = None  # 👈 新しいパラメータ
) -> Dict[str, float]:
    """
    IRLモデルを評価

    Args:
        irl_system: 評価するIRLシステム
        test_trajectories: テスト軌跡
        fixed_threshold: 固定閾値（Noneの場合は最適閾値を探索）
    """
    # 予測確率を取得
    y_true = []
    y_pred = []
    for trajectory in test_trajectories:
        # ... 予測処理 ...
        y_pred.append(prediction['continuation_probability'])

    # 閾値を決定
    if fixed_threshold is not None:
        # 固定閾値を使用（クロス評価時）
        threshold = fixed_threshold
        logger.info(f"固定閾値: {fixed_threshold:.2f}")
    else:
        # 最適閾値を探索（訓練時）
        threshold, _ = find_optimal_threshold(y_true, y_pred)
        logger.info(f"最適閾値: {threshold:.2f}")

    # 閾値で2値予測
    y_pred_binary = [1 if p >= threshold else 0 for p in y_pred]

    # メトリクス計算
    metrics = {
        'f1': f1_score(y_true, y_pred_binary, zero_division=0),
        'precision': precision_score(y_true, y_pred_binary, zero_division=0),
        'recall': recall_score(y_true, y_pred_binary, zero_division=0),
        'optimal_threshold': threshold,
        # ... その他のメトリクス ...
    }

    return metrics
```

### 2. クロス評価スクリプトの修正（`run_cross_evaluation.sh`）

#### 訓練フェーズ

```bash
# ステップ1: 各訓練ラベルでモデル訓練
for TRAIN_RANGE in "${TRAIN_RANGES[@]}"; do
    # モデル訓練（閾値も訓練データで決定される）
    uv run python scripts/training/irl/train_irl_within_training_period.py \
        --reviews "${DATA_FILE}" \
        --train-start "${TRAIN_START}" \
        --train-end "${TRAIN_END}" \
        # ... その他のパラメータ ...
        --output "${TRAIN_DIR}"
done
```

#### 評価フェーズ

```bash
# ステップ2: クロス評価
for TRAIN_RANGE in "${TRAIN_RANGES[@]}"; do
    # 訓練データから最適閾値を取得 👈 重要！
    MODEL_DIR="${OUTPUT_BASE}/model_${TRAIN_RANGE//-/_}m"

    if [ -f "${MODEL_DIR}/evaluation_results.json" ]; then
        TRAIN_OPTIMAL_THRESHOLD=$(python3 -c "
import json
with open('${MODEL_DIR}/evaluation_results.json') as f:
    results = json.load(f)
print(results.get('optimal_threshold', 0.5))
")
        echo "訓練=${TRAIN_RANGE}m の最適閾値: ${TRAIN_OPTIMAL_THRESHOLD}"
    else
        TRAIN_OPTIMAL_THRESHOLD="0.5"
    fi

    # 全評価範囲で同じ閾値を使用 👈 重要！
    for EVAL_RANGE in "${EVAL_RANGES[@]}"; do
        echo "[$EVAL_COUNT/$TOTAL_EVALS] 訓練=${TRAIN_RANGE}m × 評価=${EVAL_RANGE}m (閾値=${TRAIN_OPTIMAL_THRESHOLD})"

        # 固定閾値で評価
        uv run python -c "
# ... モデル読み込み ...

# 固定閾値で評価 👈 重要！
fixed_threshold = float('${TRAIN_OPTIMAL_THRESHOLD}')
metrics = evaluate_irl_model(irl_system, trajectories, fixed_threshold=fixed_threshold)

# ... 結果保存 ...
"
    done
done
```

---

## 🔍 結果の見方

### CSV フォーマット

```csv
train_range,eval_range,auc_roc,auc_pr,f1,precision,recall,optimal_threshold,f1_threshold_0.5,pred_mean,pred_std,positive_rate,test_samples
0-3,0-3,0.667,0.749,0.756,0.608,1.0,0.10,0.756,0.504,0.001,0.608,3003
0-3,3-6,0.337,0.477,0.726,0.570,1.0,0.10,0.726,0.522,0.001,0.570,2421
```

**新しいカラム**:

- `optimal_threshold`: 使用した閾値（訓練データで決定）
- `f1_threshold_0.5`: 参考値（閾値 0.5 を使った場合の F1）
- `pred_mean`: 予測確率の平均
- `pred_std`: 予測確率の標準偏差

### 解釈

```
訓練0-3m（閾値0.10）の場合:
→ 評価0-3m: F1=0.756 ✅
→ 評価3-6m: F1=0.726 ✅
→ 評価6-9m: F1=0.701 ✅

訓練0-6m（閾値0.46）の場合:
→ 評価0-3m: F1=0.XXX
→ 評価3-6m: F1=0.XXX
→ 評価6-9m: F1=0.XXX

比較:
- 同じ閾値での性能低下 = 時間的汎化性能の低下
- 異なる訓練ラベル間の比較が公平に可能
```

---

## ⚠️ 注意事項

### 1. 訓練データでの閾値決定

```python
# ✅ 正しい（訓練データのみ使用）
threshold = find_optimal_threshold(y_train_true, y_train_pred)
y_test_pred_binary = (y_test_pred >= threshold).astype(int)

# ❌ 間違い（テストデータを使用 = 情報リーク）
threshold = find_optimal_threshold(y_test_true, y_test_pred)
y_test_pred_binary = (y_test_pred >= threshold).astype(int)
```

### 2. モデルごとの最適閾値

```bash
# ✅ 正しい（各モデルが独自の閾値を持つ）
訓練0-3m: 閾値0.10
訓練0-6m: 閾値0.46
訓練0-9m: 閾値0.51

# ❌ 間違い（全モデルで同じ閾値を強制）
全モデル: 閾値0.5
```

### 3. 評価範囲での閾値統一

```bash
# ✅ 正しい（同じモデルの全評価で同じ閾値）
訓練0-3m → 評価0-3m: 閾値0.10
訓練0-3m → 評価3-6m: 閾値0.10
訓練0-3m → 評価6-9m: 閾値0.10

# ❌ 間違い（評価ごとに閾値を変更）
訓練0-3m → 評価0-3m: 閾値0.10
訓練0-3m → 評価3-6m: 閾値0.15
訓練0-3m → 評価6-9m: 閾値0.20
```

---

## 📊 期待される改善

### Before（各評価で最適閾値探索）

```
訓練0-3m → 評価0-3m: F1=0.756（閾値0.10）
訓練0-3m → 評価3-6m: F1=0.726（閾値0.10）
訓練0-6m → 評価0-3m: F1=0.756（閾値0.10）
訓練0-6m → 評価3-6m: F1=0.726（閾値0.10）
```

**問題**: 閾値が偶然同じだが、本来は情報リーク

### After（訓練データで決定した閾値を統一使用）

```
訓練0-3m（閾値0.10）:
  → 評価0-3m: F1=0.756
  → 評価3-6m: F1=0.726
  → 評価6-9m: F1=0.701

訓練0-6m（閾値0.46）:
  → 評価0-3m: F1=0.XXX（予想: 低下）
  → 評価3-6m: F1=0.XXX
  → 評価6-9m: F1=0.XXX
```

**期待**:

- モデル間の公平な比較が可能
- 時間的汎化性能の正確な評価
- 情報リークによる過大評価の防止

---

## 🚀 実行方法

### 1. 既存モデルを削除（オプション）

```bash
# 訓練データでの閾値を再計算するため
rm -rf outputs/cross_evaluation/model_*
```

### 2. クロス評価の実行

```bash
cd /Users/kazuki-h/rl/gerrit-retention
bash scripts/training/irl/run_cross_evaluation.sh
```

### 3. 結果の確認

```bash
# 結果CSVを表示
cat outputs/cross_evaluation/cross_evaluation_results.csv | column -t -s,

# 各モデルの閾値を確認
for dir in outputs/cross_evaluation/model_*/; do
    echo "$(basename $dir):"
    cat "$dir/evaluation_results.json" | grep optimal_threshold
done
```

---

## 📚 関連ドキュメント

- [精度問題の分析と解決策.md](./精度問題の分析と解決策.md) - 閾値問題の詳細分析
- [IRL 設計と実験結果サマリー.md](./IRL設計と実験結果サマリー.md) - 実験結果の全体サマリー

---

**最終更新**: 2025-10-22  
**実装ステータス**: ✅ 完了  
**次のアクション**: クロス評価の再実行
