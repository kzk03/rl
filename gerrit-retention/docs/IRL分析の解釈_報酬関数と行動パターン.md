# IRL 分析の解釈：報酬関数と行動パターン

## 📋 概要

本ドキュメントでは、レビュー承諾予測に使用している **IRL (Inverse Reinforcement Learning)** が何を分析しているのか、どのように解釈すべきかを説明する。

---

## 🎯 IRL の基本的な考え方

### 通常の強化学習 (RL)

```
報酬関数（既知） → 最適な行動方策を学習
```

**例**: ゲームでスコアを最大化するための行動を学習

### 逆強化学習 (IRL)

```
観測された行動（エキスパートの軌跡） → 報酬関数を推定
```

**例**: 熟練プレイヤーの行動から「何を重視しているか」を逆算

---

## 🔍 本研究での IRL の役割

### 分析対象

**エキスパートの定義**: レビュー依頼を**承諾した開発者**

```
承諾者の過去の行動パターン → 「承諾する人が重視している報酬」を学習
拒否者の過去の行動パターン → 「拒否する人が重視していない報酬」を学習
```

### 学習プロセス

1. **正例（承諾者）の軌跡を収集**

   - 過去の活動履歴（状態特徴 9 次元）
   - 過去の行動パターン（行動特徴 4 次元）
   - ラベル: 将来期間にレビュー依頼を承諾した（1）

2. **負例（拒否者）の軌跡を収集**

   - 同様の特徴量を収集
   - ラベル: 将来期間にレビュー依頼を拒否/無視した（0）

3. **報酬関数を学習**

   - 承諾者が高い報酬を得るような報酬関数を推定
   - 拒否者が低い報酬を得るような報酬関数を推定

4. **予測**
   - 学習した報酬関数を使って、新しい開発者の「承諾確率」を予測

---

## 💡 具体例：何を分析しているか

### 例 1: train_3-6m モデル（最優秀、AUC-PR=0.839）

このモデルは **2021-01-01 ～ 2023-01-01 の 2 年間** に、**2023 年 4 月～ 6 月（3-6m）にレビュー依頼を承諾/拒否した開発者の行動パターン** から学習。

**承諾者の特徴（推定される報酬関数）**:

```python
# 状態特徴の重み（例）
高い報酬 = {
    '総レビュー数': +0.8,          # 多くのレビュー経験がある
    '最近の活動頻度': +0.6,        # 最近も活発に活動している
    '協力スコア': +0.5,            # 協力的な活動が多い
    '平均活動間隔': -0.4,          # 活動間隔が短い（頻繁に活動）
    'プロジェクト数': +0.3,        # 複数プロジェクトに関与
    ...
}
```

**拒否者の特徴（推定される報酬関数）**:

```python
# 状態特徴の重み（例）
低い報酬 = {
    '総レビュー数': -0.3,          # レビュー経験が少ない
    '最近の活動頻度': -0.5,        # 最近活動していない
    '平均活動間隔': +0.6,          # 活動間隔が長い（不定期）
    '協力スコア': -0.2,            # 協力的な活動が少ない
    ...
}
```

### 例 2: 特徴量重要度分析との関係

**Gradient-based Feature Importance** で測定される重みは、まさに **IRL で学習した報酬関数の重み** を表している。

| 特徴量         | 重要度 | 解釈                                           |
| -------------- | ------ | ---------------------------------------------- |
| 総レビュー数   | +0.8   | レビュー経験が多い人ほど承諾しやすい           |
| 平均活動間隔   | -0.4   | 活動間隔が短い人（頻繁に活動）ほど承諾しやすい |
| 協力スコア     | +0.5   | 協力的な人ほど承諾しやすい                     |
| 最近の活動頻度 | +0.6   | 最近も活動している人ほど承諾しやすい           |

**これは、承諾者が「何を重視して行動しているか」を示している。**

---

## 🎭 承諾者 vs 拒否者の行動パターン

### 承諾者のプロファイル（報酬関数が高い）

```yaml
典型的な承諾者:
  経験:
    - 総レビュー数: 多い（100+）
    - 経験日数: 長い（1年以上）
    - プロジェクト数: 複数（2-3個）

  活動パターン:
    - 最近の活動頻度: 高い（週に数回）
    - 平均活動間隔: 短い（3-5日）
    - 活動トレンド: 増加傾向

  協力性:
    - 協力スコア: 高い（0.7以上）
    - レスポンス時間: 短い（1-2日）
    - コード品質スコア: 高い（0.6以上）
```

### 拒否者のプロファイル（報酬関数が低い）

```yaml
典型的な拒否者:
  経験:
    - 総レビュー数: 少ない（<50）
    - 経験日数: 短い（<6ヶ月）
    - プロジェクト数: 単一（1個）

  活動パターン:
    - 最近の活動頻度: 低い（月に数回）
    - 平均活動間隔: 長い（10日以上）
    - 活動トレンド: 減少傾向

  協力性:
    - 協力スコア: 低い（<0.3）
    - レスポンス時間: 長い（5日以上）
    - コード品質スコア: 低い（<0.4）
```

---

## 🔬 IRL で分析できること

### 1. 承諾者が何を重視しているか

**報酬関数の正の重み**が示すもの：

- ✅ **レビュー経験が豊富** → 承諾の動機になる
- ✅ **最近も活発に活動** → 承諾の動機になる
- ✅ **協力的な活動パターン** → 承諾の動機になる

**解釈**:

> 「経験豊富で、最近も活発に活動しており、協力的な開発者は、レビュー依頼を承諾しやすい。彼らは『プロジェクトへの貢献』や『コミュニティへの協力』を重視している。」

### 2. 拒否者が何を重視していないか

**報酬関数の負の重み**が示すもの：

- ❌ **活動間隔が長い** → 拒否の要因
- ❌ **最近活動していない** → 拒否の要因
- ❌ **協力スコアが低い** → 拒否の要因

**解釈**:

> 「活動が不定期で、最近も活動していない開発者は、レビュー依頼を拒否しやすい。彼らは『時間的余裕がない』『プロジェクトから離れつつある』可能性がある。」

### 3. 時期による重視内容の変化

**train_0-3m（短期）モデル**:

- 「最近の活動頻度」の重要度が高い
- → 短期的には「今も活動しているか」が重要

**train_6-9m（中期）モデル**:

- 「総レビュー数」の重要度が高い
- → 中期的には「経験の蓄積」が重要

**train_9-12m（長期）モデル**:

- 「活動トレンド」の重要度が高い
- → 長期的には「持続的な成長」が重要

---

## 📊 報酬関数の可視化イメージ

### 承諾者の報酬マップ

```
高報酬（承諾しやすい）
    ↑
    |  ● 経験豊富＋活発 (報酬=1.5)
    |    ● 経験豊富＋普通 (報酬=1.0)
    |      ● 普通＋活発 (報酬=0.8)
    |        ○ 普通＋普通 (報酬=0.5)
    |          × 経験少＋不活発 (報酬=0.2)
    |            × 経験少＋離脱中 (報酬=-0.3)
    ↓
低報酬（拒否しやすい）
```

### 予測プロセス

```python
# 1. 開発者の状態を取得
developer_state = {
    '総レビュー数': 120,
    '最近の活動頻度': 0.8,
    '協力スコア': 0.7,
    ...
}

# 2. 学習した報酬関数で報酬を計算
reward = irl_model.calculate_reward(developer_state)
# reward = 1.2 （高報酬）

# 3. 報酬からSigmoid関数で確率に変換
probability = sigmoid(reward)
# probability = 0.77 （77%の確率で承諾）

# 4. 閾値で判定
if probability >= 0.5:
    prediction = "承諾"  # ✅
else:
    prediction = "拒否"  # ❌
```

---

## 🎯 特徴量重要度分析との統合

### Gradient-based Importance

**勾配ベース特徴量重要度**は、報酬関数の各特徴量に対する感度を測定：

```python
importance = ∂reward / ∂feature
```

**正の重要度（+）**:

- その特徴量が増えると報酬が増える
- → 承諾確率が上がる
- → 承諾者が重視している

**負の重要度（−）**:

- その特徴量が増えると報酬が減る
- → 承諾確率が下がる
- → 拒否者の特徴

### 分析例

```json
{
  "総レビュー数": {
    "importance": 0.85,
    "解釈": "レビュー経験が多いほど承諾しやすい（+85%）"
  },
  "平均活動間隔": {
    "importance": -0.42,
    "解釈": "活動間隔が長いほど拒否しやすい（-42%）"
  },
  "協力スコア": {
    "importance": 0.63,
    "解釈": "協力的な人ほど承諾しやすい（+63%）"
  }
}
```

---

## 💡 実用的な解釈

### 1. レビュー依頼戦略への応用

**高確率で承諾する人（報酬が高い人）**:

- 経験豊富で最近も活発に活動している
- → 優先的にレビュー依頼を送る

**低確率で承諾する人（報酬が低い人）**:

- 活動が不定期で最近も活動していない
- → レビュー依頼を控える、または別のアプローチ（メンタリングなど）

### 2. 開発者のリテンション施策

**拒否パターンの早期検出**:

- 「平均活動間隔」が増加 → 離脱の兆候
- 「協力スコア」が低下 → バーンアウトの兆候
- → 早期介入（サポート、負荷軽減）

**承諾パターンの強化**:

- 「総レビュー数」を増やす → 経験を積ませる
- 「協力スコア」を高める → 協力的な活動を奨励

### 3. プロジェクトマネジメント

**レビュアーの選定**:

- IRL 予測確率が高い（>0.7）人に依頼
- 確率が低い人（<0.3）には依頼しない

**ワークロードの調整**:

- 高確率の人に偏らないようにバランス
- 低確率の人をサポートして復帰を促す

---

## 🎓 まとめ

### IRL が分析していること

1. **承諾者の行動原理（報酬関数）**

   - 何を重視して承諾しているか
   - どのような状態・行動パターンが承諾につながるか

2. **拒否者の行動原理（報酬関数の逆）**

   - 何を重視せずに拒否しているか
   - どのような状態・行動パターンが拒否につながるか

3. **時期による重視内容の変化**
   - 短期（0-3m）、中期（3-6m）、長期（9-12m）で重要な要因が異なる

### 分析の意義

- ✅ **予測**: 誰が承諾/拒否するかを予測
- ✅ **説明**: なぜ承諾/拒否するかを説明（報酬関数の重み）
- ✅ **介入**: どう改善すべきかを示唆（特徴量の調整）

### 注意点

- **因果関係ではなく相関関係**: IRL は相関を捉えるが、因果は保証しない
- **データの質に依存**: ラベル（承諾/拒否）の正確性が重要
- **時期特異性**: 訓練期間によって報酬関数が変わる

---

## 🔗 関連ドキュメント

- `docs/最終モデル設定_LR00005_E20_S777.md` - 最終モデル設定
- `docs/IRL特徴量定義_9状態4行動.md` - 特徴量の定義
- `docs/論文用手法詳細解説.md` - 手法の詳細説明

---

_最終更新日: 2025-10-29_
_IRL: Inverse Reinforcement Learning - 行動から報酬を逆算する 🔍_
