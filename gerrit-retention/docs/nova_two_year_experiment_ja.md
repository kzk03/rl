# openstack/nova 二年間実験サマリー

## 研究設計の概要

- **目的**: `openstack/nova` プロジェクトに限定したレビューア推薦で、逆強化学習 (IRL) モデルが実運用ログに近い選択を再現できるかを定量的に検証する。
- **データ範囲**: 2021 年 7 月中旬〜2023 年 6 月末を学習用、2023 年 7 月初旬〜2025 年 6 月中旬を評価用に設定し、いずれも 24 か月分のタスクを利用。
- **候補者条件**: 削除された上限は設けず、各タスク時点から過去 6 か月以内に活動 (オーナー/レビュアー/CC) がある開発者のみを候補とする。
- **手法**: 6 か月の活動ウィンドウで候補者集合を構築し、生成したタスク (`tasks_train.jsonl` / `tasks_eval.jsonl`) を用いて IRL モデルを訓練。温度 0.8 のソフトマックスで擬似報酬を学習し、得られた重みでリプレイ評価を実施。
- **評価設計**: カットオフを境に、評価側は 1〜24 か月を連続ウィンドウで、学習側は -24〜0 か月を同様に区切って再現率・Top-k ヒット率・mAP・ECE を測定し、ランダム推薦との倍率を算出する。

## 実験設計

- 対象は `openstack/nova` のレビュー推薦タスクに限定し、学習・評価ともに二年間の時間枠を設定。
- 学習期間は 2021 年夏から 2023 年 6 月末まで、評価期間は 2023 年 7 月初旬から 2025 年 6 月中旬までの連続データ。
- レビュー候補者は最新 6 か月以内に活動履歴がある開発者に絞り、候補数の明示的な上限は設定しない方針。

## データ構成と分割

- 学習用タスク: 12,493 件（うち正例が存在する 12,259 件を学習に使用）。
- 評価用タスク: 7,717 件。
- 各タスクに紐づく候補者数は学習側で平均 133.7 人、評価側で平均 101.1 人。期間が過去になるほど候補者が多く、直近になるほど減少する傾向。
- 候補者としてカウントされるロールはオーナー・レビュアー・CC のいずれかであり、タイムスタンプ基準で 180 日以内の活動が条件。

## 候補者抽出方法

- 各タスクの発生時刻から 180 日遡った期間内で活動した開発者のみを候補に残す。
- プロジェクト外のレビュー履歴は除外し、単一プロジェクト内での活動頻度を優先。
- 候補リストの順序は時間局所的なシャッフルで固定化し、後続工程で再現性を確保。

## モデル学習手順

- 逆強化学習 (Inverse Reinforcement Learning) に基づく多項ロジスティック回帰モデルを採用。
- 主なハイパーパラメータ:
  - 学習反復回数: 400
  - 学習率: 0.05
  - L2 正則化: 1e-4
  - L1 正則化: 1e-3
  - 温度パラメータ: 0.8
- 17 個の特徴量とバイアス項を同時に学習し、実際のレビュー担当決定を模倣する報酬重みを獲得。

## 特徴量と報酬解釈

- 特徴量はタスク × 候補者のペアに紐づく統計量で、代表例は以下の通り。
  - `activity7` / `activity30` / `activity90`: 直近 7/30/90 日間のレビュー活動頻度。
  - `gap_days`: 最終レビューからの経過日数 (小さいほど最近活動している)。
  - `workload`, `workload_sq`: 現在の担当件数とその二乗 (多忙さを抑制する役割)。
  - `expertise`: パッチのファイルパス類似度や履歴に基づく専門度指標。
  - `path_jaccard_dir2_180d`, `path_tfidf_cosine_recent30`: 直近変更ファイルと候補者の過去担当ファイルの類似度。
  - `owner_reviewer_past_assignments_180d`, `..._ratio_180d`, `..._log1p`: パッチオーナーと候補者の過去 180 日内の協業回数。
  - `historical_accept_ratio`: 候補者がレビューしたチェンジのマージ率。
  - `backlog_open_reviews`: 現在未完了のレビュー件数 (負荷バランス指標)。
  - `reviewer_tenure_days`: プロジェクトでの在籍期間。
  - `interaction_activity_gap`: 活動頻度と最後の担当からのギャップを組み合わせた安定性指標。
- モデルは各特徴量に対する重みベクトル `w` とバイアス `b` を学習し、報酬 (正確には「擬似報酬」) を `u = w^T x + b` で算出。
- `temperature = 0.8` によりソフトマックスの尖り具合を調整し、推薦順位に直結する確率分布を生成。
- 得られた重みの符号や大きさは「どの要因がポジティブな割り当てにつながったか」を示し、たとえば類似ファイル歴 (`path_*`) や過去協業回数が正方向、過剰作業量 (`workload`) が負方向に働く傾向が見られる。

### 学習済み重みの一覧

| 特徴量                                       |    重み | 解釈メモ                                       |
| -------------------------------------------- | ------: | ---------------------------------------------- |
| `gap_days`                                   | -0.9729 | 直近でレビューしていない候補は大きく減点。     |
| `reviewer_tenure_days`                       |  0.7973 | プロジェクト在籍期間が長いほど優遇。           |
| `expertise`                                  |  0.6439 | パッチへの専門度が高い候補を加点。             |
| `activity7`                                  |  0.3718 | 直近 1 週間の活動が活発だと加点。              |
| `activity90`                                 | -0.1454 | 90 日の活動が高すぎると軽い減点 (過負荷回避)。 |
| `workload_sq`                                | -0.1285 | 作業量の二乗で過負荷を強めにペナルティ。       |
| `interaction_activity_gap`                   |  0.1132 | 活動と空白のバランスが良い候補を微加点。       |
| `activity30`                                 | -0.0289 | 30 日活動はほぼ中立 (わずかに減点)。           |
| `workload`                                   | -0.0286 | 現在の担当数が多いと微減点。                   |
| `backlog_open_reviews`                       |  0.0000 | L1 正則化の結果ゼロ化 (影響なし)。             |
| `historical_accept_ratio`                    |  0.0000 | 〃                                             |
| `active_hour_match`                          |  0.0000 | 〃                                             |
| `owner_reviewer_past_assignments_180d`       |  0.0000 | 〃                                             |
| `path_jaccard_dir2_180d`                     |  0.0000 | 〃                                             |
| `path_tfidf_cosine_recent30`                 |  0.0000 | 〃                                             |
| `owner_reviewer_past_assignments_180d_log1p` |  0.0000 | 〃                                             |
| `owner_reviewer_past_assignments_ratio_180d` |  0.0000 | 〃                                             |

- バイアス項: 約 0 (1.86e-18)。
- L1 正則化 (1e-3) の影響で上表後半の特徴量は完全に 0 となり、実質的に利用されていない。

#### 正規化前の重み (元特徴量スケール)

モデルは学習時に各特徴量を `z = (x - μ) / σ` に標準化しているため、保存されている係数は標準化後の値です。元スケールでの係数は次式で復元できます。

$$
w_{\text{raw}} = \frac{w_{\text{std}}}{\sigma}, \qquad b_{\text{raw}} = b - \sum_i w_{\text{raw}, i} \mu_i
$$

| 特徴量                                       | $w_{std}$ | $w_{raw}$ | 平均 $\mu$ | 標準偏差 $\sigma$ |
| -------------------------------------------- | --------: | --------: | ---------: | ----------------: |
| `activity30`                                 | -0.028869 | -0.000748 |    13.8585 |           38.6186 |
| `activity90`                                 | -0.145378 | -0.001289 |    38.0004 |          112.7900 |
| `workload`                                   | -0.028630 | -0.022095 |     0.4668 |            1.2958 |
| `gap_days`                                   | -0.972881 | -0.004105 |   155.0131 |          236.9952 |
| `expertise`                                  |  0.643938 |    3.0280 |     0.1375 |            0.2127 |
| `backlog_open_reviews`                       |  0.000000 |  0.000000 |     0.0000 |            1.0000 |
| `historical_accept_ratio`                    |  0.000000 |  0.000000 |     0.0000 |            1.0000 |
| `active_hour_match`                          |  0.000000 |  0.000000 |     0.0000 |            1.0000 |
| `owner_reviewer_past_assignments_180d`       |  0.000000 |  0.000000 |     0.0000 |            1.0000 |
| `path_jaccard_dir2_180d`                     |  0.000000 |  0.000000 |     0.0000 |            1.0000 |
| `path_tfidf_cosine_recent30`                 |  0.000000 |  0.000000 |     0.0000 |            1.0000 |
| `owner_reviewer_past_assignments_180d_log1p` |  0.000000 |  0.000000 |     0.0000 |            1.0000 |
| `owner_reviewer_past_assignments_ratio_180d` |  0.000000 |  0.000000 |     0.0000 |            1.0000 |
| `activity7`                                  |  0.371799 |  0.044005 |     2.5221 |            8.4490 |
| `workload_sq`                                | -0.128503 | -0.010327 |     1.8707 |           12.4440 |
| `reviewer_tenure_days`                       |  0.797295 |  0.001154 |  1129.5154 |          690.8995 |
| `interaction_activity_gap`                   |  0.113174 |  0.002951 |    10.8460 |           38.3494 |

- 元スケールのバイアス項: $b_{raw} = -1.1374$
- 係数が 0 の特徴量は L1 正則化により選択されず、元スケールでも影響がありません。

## 評価指標と方法

- 学習済み重みを用い、推薦順序を直接ランキング化して再現性評価 (replay evaluation) を実施。
- 評価期間は 3 か月単位の連続ウィンドウに分割。
  - ポストカットオフ側: 1 か月、1–3 か月、3–6 か月、6–9 か月、9–12 か月、12–18 か月、18–24 か月。
  - プレカットオフ側 (学習期間の再評価): -24––18 か月、-18––12 か月、-12––9 か月、-9––6 か月、-6––3 か月、-3–0 か月。
- 計測指標: トップ 1 正解率 (action match rate)、ランダム推薦時のトップ 1 正解率、トップ 3 / トップ 5 ヒット率、平均適合率 (mAP)、期待キャリブレーション誤差 (ECE)、平均候補数。
- トップ 1 正解率とランダム正解率の比を「倍率」として算出し、モデルの優位性を定量化。

## 主な結果

### 評価期間 (ポストカットオフ)

- トップ 1 正解率は 0.19–0.31 の範囲、ランダムベースラインは約 0.0095–0.0106。
- 倍率は概ね 20 倍前後で安定し、最初の 1 か月間は 32 倍を記録。
- トップ 3 ヒット率は 0.27–0.41、トップ 5 ヒット率は 0.28–0.52。
- mAP は 0.31–0.38 程度で推移し、12–18 か月区間が最も高い 0.3675。
- ECE は 0.17–0.34 の範囲で、短期ウィンドウほどキャリブレーションが崩れやすい。

| ウィンドウ | ステップ数 | トップ 1 正解率 | ランダム |     倍率 |   トップ 3 |   トップ 5 |        mAP | 平均候補数 |   ECE |
| ---------- | ---------: | --------------: | -------: | -------: | ---------: | ---------: | ---------: | ---------: | ----: |
| 1m         |        209 |          0.3062 |  0.00953 | **32.1** |     0.3206 |     0.3923 |     0.3783 |     105.01 | 0.317 |
| 1m–3m      |        414 |          0.2633 |  0.00972 |     27.1 |     0.2705 |     0.2754 |     0.3187 |     103.06 | 0.175 |
| 3m–6m      |        913 |          0.2147 |  0.01059 |     20.3 |     0.2629 |     0.3516 |     0.3102 |      94.61 | 0.215 |
| 6m–9m      |      1,065 |          0.1944 |  0.01047 |     18.5 | **0.4000** | **0.4573** |     0.3507 |      95.62 | 0.342 |
| 9m–12m     |        615 |          0.2065 |  0.00992 |     20.8 |     0.3545 |     0.4618 |     0.3519 |     100.92 | 0.203 |
| 12m–18m    |      2,342 |          0.2096 |  0.00983 |     21.3 | **0.4086** | **0.5179** | **0.3675** |     101.77 | 0.250 |
| 18m–24m    |      2,159 |          0.2015 |  0.00954 |     21.1 |     0.3529 |     0.4701 |     0.3472 |     105.00 | 0.245 |

### 学習期間の再評価 (プレカットオフ)

- トップ 1 正解率は 0.136–0.185、ランダムベースラインは 0.0067–0.0092。
- 倍率は 18–21 倍で推移し、評価期間と同程度の比率を維持。
- 候補者数が過去ほど多く (平均 120–150 人)、正解率はやや低下するが、ランダムとの倍率は安定。
- トップ 3 / トップ 5 ヒット率はそれぞれ 0.25–0.33 / 0.33–0.47。mAP は 0.27–0.29 程度。

| ウィンドウ | ステップ数 | トップ 1 正解率 | ランダム | 倍率 | トップ 3 | トップ 5 |    mAP | 平均候補数 |   ECE |
| ---------- | ---------: | --------------: | -------: | ---: | -------: | -------: | -----: | ---------: | ----: |
| -24m––18m  |      3,337 |          0.1468 |  0.00776 | 18.9 |   0.2829 |   0.4108 | 0.2846 |     130.49 | 0.207 |
| -18m––12m  |      3,800 |          0.1387 |  0.00669 | 20.7 |   0.3289 |   0.4618 | 0.2928 |     150.51 | 0.204 |
| -12m––9m   |      1,783 |          0.1363 |  0.00759 | 18.0 |   0.3236 |   0.4728 | 0.2907 |     131.84 | 0.176 |
| -9m––6m    |      1,261 |          0.1673 |  0.00787 | 21.3 |   0.2839 |   0.3759 | 0.2893 |     127.08 | 0.222 |
| -6m––3m    |      1,298 |          0.1595 |  0.00825 | 19.3 |   0.2519 |   0.3382 | 0.2683 |     121.42 | 0.299 |
| -3m–0m     |      1,014 |          0.1854 |  0.00920 | 20.1 |   0.2732 |   0.3343 | 0.2868 |     108.76 | 0.175 |

## 考察と今後の改善余地

- 過去期間の候補数増加はボットや CI アカウントの混入が一因と推察される。活動分類や正規化で候補集合を絞る余地あり。
- ランキングが高倍率を維持する一方、短期ウィンドウでのキャリブレーション誤差が目立つため、温度調整や信頼度再補正が有効と考えられる。
- 今後強化学習ポリシーを併用する際は、アクション空間の拡大に合わせた探索抑制策または候補上限の再導入を検討する必要がある。
- IRL で得た報酬重みは即座に強化学習ポリシーへ転移可能であり、以下の応用が想定される。
  - **初期値としての利用**: ポリシーネットワークの価値関数や Q 関数に IRL の線形報酬を組み込み、ウォームスタートすることで学習を安定化。
  - **報酬設計の補助**: IRL 重みを参照し、RL 環境で使用する手工の報酬設計 (例: 専門度の高い割り当てにボーナス、過負荷にペナルティ) を調整。
  - **模倣から探索へ**: 現在はログ再現のみだが、将来的には IRL モデルを行動モデルとして用い、その上で RL による探索 (例: 新人レビューア割当、負荷平準化策) を実施するハイブリッド構成が可能。
  - **キャリブレーション連携**: ECE 改善のための温度スケーリングやプルーニングを RL の報酬に組み込み、信頼度の高い推薦を優先する戦略へ拡張。
