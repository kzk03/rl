# IRLとベースラインの方法論の違い分析

## ユーザーの指摘

> 評価期間にラベル付はしてないはず．max-dateを設けている．データリークしないようにmax-date以降はラベル付のためだけにレビュー依頼の受諾を見ているだけなはず
> IRLでは訓練期間の最後らへんをラベル付のみに使用しているけど，対抗馬はそこも学習している？？

## 詳細調査結果

### IRLの方法論（月次訓練）

IRLは**月次ラベル**を使用した訓練を行っています。

#### コード実装（train_irl_review_acceptance.py: 240-296行）

```python
# 特徴量計算期間の月次ラベルを計算（訓練用、データリークなし）
history_months = pd.date_range(
    start=history_start,  # 2021-01-01
    end=history_end,      # 2023-01-01
    freq='MS'  # 月初
)

for month_start in history_months[:-1]:  # 最後の月を除く
    month_end = month_start + pd.DateOffset(months=1)

    # この月からfuture_window後のラベル計算期間
    future_start = month_end + pd.DateOffset(months=future_window_start_months)
    future_end = month_end + pd.DateOffset(months=future_window_end_months)

    # 重要：future_endがtrain_endを超えないようにクリップ（データリーク防止）
    if future_end > train_end:
        future_end = train_end

    # train_endを超える場合はこの月のラベルは作成しない
    if future_start >= train_end:
        continue
```

#### 具体例：train_period = "0-3m"の場合

**設定**:
- train_start: 2021-01-01
- train_end: 2023-01-01
- future_window: 0～3ヶ月

**月次訓練の流れ**:

| 月 | 特徴量期間 | ラベル期間（理論値） | ラベル期間（実際） | 備考 |
|----|-----------|-------------------|------------------|------|
| 2021-01 | ～2021-01 | 2021-01～2021-04 | 2021-01～2021-04 | OK |
| 2021-02 | ～2021-02 | 2021-02～2021-05 | 2021-02～2021-05 | OK |
| ... | ... | ... | ... | ... |
| 2022-09 | ～2022-09 | 2022-09～2022-12 | 2022-09～2022-12 | OK |
| **2022-10** | **～2022-10** | **2022-10～2023-01** | **2022-10～2023-01** | **max-date=train_endでクリップ** |
| **2022-11** | **～2022-11** | **2022-11～2023-02** | **スキップ** | **future_start >= train_end** |
| 2022-12 | ～2022-12 | 2022-12～2023-03 | スキップ | future_start >= train_end |

**重要な発見**:

1. **2022-10～2023-01の期間**（訓練期間の最後の3ヶ月）は：
   - 2022-10月時点の**ラベル計算に使用される**
   - でも、2022-11月以降の月の**特徴量計算には使用されない**（ラベルがtrain_endを超えるため、その月自体がスキップされる）

2. つまり、訓練期間の後半部分は「ラベル付けのためだけ」に使用される

3. これがユーザーの言う**「max-date以降はラベル付のためだけ」**の意味

### ベースラインの方法論（全期間訓練）

#### コード実装（run_baseline_nova_exact_match.py: 256-262行）

```python
train_trajectories = extract_trajectories_for_period(
    df,
    history_start=train_start,      # 2021-01-01
    history_end=train_end,          # 2023-01-01
    label_start=train_label_start,  # 2023-01-01（評価期間）
    label_end=train_label_end        # 2023-04-01
)
```

```python
# extract_trajectories_for_period内（93-95行）
history_df = df[(df['request_time'] >= history_start) &
                (df['request_time'] < history_end)]
```

**ベースラインの処理**:
- 特徴量期間：**2021-01-01 ～ 2023-01-01**（訓練期間全体）
- ラベル期間：2023-01-01 ～ 2023-04-01（評価期間）

**重要な問題**:

ベースラインは訓練期間全体（2021-2023）を特徴量計算に使用しています。
つまり、**2022-10～2023-01の期間も特徴量計算に使用している**。

IRLがラベル付けのためだけに使った期間を、ベースラインは特徴量計算に使っている！

## 比較表

| 期間 | IRL（月次訓練） | ベースライン（全期間訓練） | 公平か？ |
|------|----------------|------------------------|---------|
| 2021-01～2022-10 | 特徴量計算に使用 | 特徴量計算に使用 | ✅ |
| 2022-10～2023-01 | ラベル付けのみ（前の月のラベル計算） | **特徴量計算にも使用** | ❌ |
| 2023-01～2023-04 | ラベル付け（評価期間） | ラベル付け（評価期間） | ✅ |

## 結論

**ユーザーの指摘は正しいです。**

IRLとベースラインの比較は**不公平**です：

1. **IRLの制約**：
   - 訓練期間の後半（例：2022-10～2023-01）はラベル付けのためだけに使用
   - この期間のデータは特徴量計算には使えない

2. **ベースラインの優位性**：
   - 訓練期間全体（2021-2023）を特徴量計算に使用
   - IRLが使えなかった期間のデータも学習に使っている

3. **情報量の違い**：
   - ベースラインはIRLより**多くのデータ**を特徴量計算に使用している
   - これがベースラインの高性能（AUC-ROC 0.853 vs 0.758）の一因かもしれない

## 修正案

ベースラインを公平に比較するには：

### オプション1：ベースラインもmax-dateを使用

```python
# 訓練期間の後半をラベル付け用に確保
max_date = train_end - pd.DateOffset(months=future_window_end_months)  # 2022-10-01

# 特徴量計算期間を制限
history_start = train_start  # 2021-01-01
history_end = max_date       # 2022-10-01（IRLと同じ）

# ラベル期間（訓練期間内の後半部分）
label_start = max_date       # 2022-10-01
label_end = train_end        # 2023-01-01
```

### オプション2：IRLもeval期間をラベルに使用

IRLを評価期間でラベル付けするように変更（現在の実装を確認する必要あり）

## 次のステップ

1. ベースラインを修正してmax-date方式を実装
2. 公平な条件で再評価
3. 結果を比較

これにより、IRLとベースラインの真の性能差が明らかになります。
