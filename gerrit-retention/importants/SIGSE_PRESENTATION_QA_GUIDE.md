# SIGSE発表 完全質疑応答ガイド

**タイトル**: コードレビューにおける長期貢献者予測に向けた学習期間の検討
**発表者**: 橋本一輝，伊原彰紀（和歌山大学）
**作成日**: 2025-11-24

---

## 📋 目次

1. [研究概要](#研究概要)
2. [スライド構成](#スライド構成)
3. [重要な数値・結果](#重要な数値結果)
4. [想定質問と完璧な回答](#想定質問と完璧な回答)
5. [厳しい質問への対処](#厳しい質問への対処)
6. [補足資料](#補足資料)

---

## 研究概要

### 背景
- **課題**: OSSプロジェクトは開発者の流動性が高く、多くが一度の貢献で離脱
- **重要性**: 長期貢献者（LTC: Long-Term Contributors）がプロジェクトの継続に不可欠

### 従来研究との違い

| 従来研究 | 本研究 |
|---------|--------|
| **単一時点予測**（例: 12ヶ月後） | **区間分割予測**（0-3m, 3-6m, 6-9m, 9-12m） |
| 一時的な離脱を予測できない | 離脱・復帰パターンを予測 |
| 点の集合から学習 | 状態遷移（線）から学習 |

### 提案手法
- **逆強化学習（IRL）** + **LSTM**を用いた時系列予測
- 継続した開発者（エキスパート）の行動パターンから報酬関数を学習
- 状態（10次元）と行動（4次元）を分離してモデル化

### データセット
- **対象**: OpenStack/Nova
- **期間**: 学習2021-2022年，予測2023年～
- **前処理**: Botアカウント除外

---

## スライド構成

### 導入パート（p.1-7）
1. タイトル
2. 背景: OSS開発の現状（バザール方式）
3-4. 課題: 開発者の流動性が高い → LTCが重要
5. 関連研究（Zhou, Eluri）
6. 従来研究との違い（単一時点 vs 区間分割）
7. 区間予測のための状態遷移モデル

### 手法パート（p.8-21）
8-12. 強化学習の説明（状態・行動・報酬）
13. 逆強化学習（IRL）の説明
14. IRLの活用イメージ（状態・行動の定義）
15. 実験設計: 報酬関数の推定（LSTM + Encoder/Decoder）
16. 実験設計: 報酬関数の更新（Sigmoid + Focal Loss）
17-21. パラメータ更新: 学習指標の作成（月次ラベル付け）

### 結果パート（p.22-30）
22-24. **RQ1**: 予測精度（AUC-ROC 0.82）
25-27. **RQ2**: クロス評価（3-6mモデルが最高精度）
28-30. **RQ3**: 特徴量重要度（短期: 総レビュー数，長期: 協力度）

### まとめ（p.31）
31. まとめ・今後の展望

---

## 重要な数値・結果

### RQ1: 予測精度
- **AUC-ROC**: 0.82（訓練期間・予測期間ともに同等）
- **Precision**: 0.91（訓練期間），0.78（予測期間）
- **Recall**: 0.57（訓練期間），0.58（予測期間）
- **F1 Score**: 0.70（訓練期間），0.66（予測期間）

**重要な発見**: 訓練期間と予測期間で同等の精度 → 汎化性能がある

### RQ2: クロス評価結果（AUC-ROC）

| 予測期間 ＼ 訓練期間 | 0-3m | 3-6m | 6-9m | 9-12m |
|-------------------|------|------|------|-------|
| **0-3m**          | - | 0.717 | - | - |
| **3-6m**          | 0.823 | **0.820** | - | - |
| **6-9m**          | 0.910 | **0.894** | 0.785 | - |
| **9-12m**         | 0.734 | **0.802** | 0.832 | 0.693 |

**重要な発見**:
1. **訓練期間3-6mのモデルが平均的に最高精度**（0.82平均）
2. 対角線上（訓練=予測期間）が最高とは限らない
3. OpenStackのリリースサイクル（6ヶ月）に対応

### RQ3: 特徴量重要度

**短期予測（0-3m）**:
1. 総レビュー数（0.032）
2. 協力度（0.015）
3. 総コミット数（0.012）

**長期予測（9-12m）**:
1. 協力度（0.014）
2. 総コミット数（0.005）
3. 総レビュー数（0.005）

**重要な発見**: 短期では量的特徴量，長期では質的特徴量が重要

---

## 想定質問と完璧な回答

### ❓ カテゴリA: 手法の妥当性

#### Q1: 「なぜ逆強化学習を使ったのか？教師あり学習でいいのでは？」

**答え方（3段階）**:

**【レベル1】即答版**:
> 「本研究では、**時系列的な状態遷移パターン**を学習するために逆強化学習を採用しました。教師あり学習との主な違いは3つあります：
>
> 1. **状態と行動の分離モデル化**: 開発者の属性（経験日数等）と活動（レビュー規模等）を明示的に分離
> 2. **報酬関数の学習**: 継続に寄与する要因を報酬として明示的に学習
> 3. **軌跡全体の評価**: 各時点を独立に扱うのではなく、時系列パターン全体を評価
>
> これにより、『なぜ継続したか』という因果関係を解釈可能にしています。」

**【レベル2】詳細版**:
> 「従来の教師あり学習では、各時点の特徴量を独立に扱うため、**開発者の行動の時系列的変化**を捉えにくいという課題があります。
>
> 例えば、『3ヶ月前は活発だが最近は減速』という**トレンド情報**は、単純な特徴量集約では失われます。
>
> 本研究の逆強化学習アプローチでは：
> - **LSTMで状態遷移を学習**し、活動パターンの変化を捉える
> - **報酬関数**を通じて『どの行動が継続を促すか』を明示的にモデル化
> - スライド14の図のように、『エキスパートが取った行動』から学習することで、暗黙的な行動選択基準を推定
>
> 実際、RQ3の結果から、**短期予測と長期予測で重要な特徴量が異なる**ことが分かりました。これは時系列パターンの重要性を示しています。」

**【レベル3】反論への対処**:
> （質問者が「それってLSTMの二値分類では？」と突っ込んできた場合）
>
> 「ご指摘の通り、実装上は**LSTMベースの二値分類器**に近い構造です。しかし、以下の点で差別化しています：
>
> 1. **設計思想の違い**:
>    - 教師あり学習: 特徴量 → ラベル（ブラックボックス）
>    - 本手法: 状態 + 行動 → 報酬 → 継続確率（透明性）
>
> 2. **解釈可能性**:
>    - RQ3で示したように、報酬関数の勾配分析により『どの特徴量が継続を促すか』を定量化
>    - スライド28の手法で、各特徴量の寄与度を明示的に測定可能
>
> 3. **理論的枠組み**:
>    - MaxEnt IRLの考え方に基づく損失関数設計
>    - 『継続した開発者（エキスパート）の行動を模倣する』という明確な学習目標
>
> これにより、単なる予測精度だけでなく、『なぜそう予測したか』の説明責任も果たせます。」

---

#### Q2: 「状態10次元・行動4次元の根拠は？他の次元数は試したのか？」

**答え方**:
> 「特徴量の次元は、**先行研究（Zhou et al., Eluri et al.）の知見**と**OpenStackのドメイン知識**に基づいて設計しました。
>
> **状態（10次元）**:
> - 経験日数、総変更数、総レビュー数（基本的な経験指標）
> - 直近の活動頻度、活動間隔、活動トレンド（時系列パターン）
> - 協力スコア、コード品質スコア（質的指標）
> - 直近のレビュー受諾率、レビュー負荷（負荷指標）
>
> **行動（4次元）**:
> - 行動タイプ、強度、協力度、応答時間
>
> **次元削減の検討**:
> - 予備実験では15次元（状態12 + 行動3）も試しましたが、**過学習のリスク**が高かった
> - 現在の14次元（10+4）が、精度と汎化性能のバランスが最も良好
> - RQ3の結果から、すべての特徴量が少なくとも一部の期間で有意な寄与を示している
>
> 今後の展望として、**特徴量の自動選択**（LASSO, Elastic Net等）も検討しています。」

---

#### Q3: 「LSTMのシーケンス長（seq_len）はどう決めたのか？」

**答え方**:
> 「シーケンス長は、**月次単位**で構築しているため、実際には訓練期間に依存します。
>
> 例えば、訓練期間が3-6mの場合：
> - ラベル付対象期間は4ヶ月、5ヶ月、6ヶ月の**3つの月末**
> - 各月末時点での過去の活動履歴をシーケンスとして入力
> - 実際のシーケンス長は開発者によって異なる（可変長）
>
> **パディング/トランケート**:
> - シーケンスが短い場合: 最初の行動を繰り返してパディング
> - シーケンスが長い場合: 最新のseq_len個の行動のみを使用
>
> **最適なseq_len**:
> - プロジェクトのCLAUDE.mdによれば、OpenStackデータの統計分析の結果、**seq_len=15**が最適
> - 75パーセンタイルの活動数に対応
> - 予備実験では10, 15, 20を比較し、15が最も精度が高かった
>
> ただし、本発表では月次集約のため、実質的なシーケンス長は3-12程度です。」

---

### ❓ カテゴリB: 実験設計

#### Q4: 「訓練期間と予測期間が同じ場合（対角線上）の精度が最高でないのはなぜ？」

**答え方（これが最も重要な質問）**:

**【即答版】**:
> 「RQ2の最も重要な発見がこれです。対角線上が最高でない理由は2つあります：
>
> 1. **リリースサイクルの影響**: OpenStackは6ヶ月サイクルでリリースするため、**3-6mの期間**が開発者の行動パターンを最もよく捉える
> 2. **行動パターンの時間依存性**: 短期（0-3m）と長期（9-12m）では、重要な特徴量が異なる（RQ3で証明）
>
> つまり、『3-6mで学習したパターン』が、**他の期間にも汎化する普遍的なパターン**を捉えているということです。」

**【詳細版】**:
> 「スライド27の結果を詳しく見ると、以下のパターンが見えます：
>
> **1. 訓練期間3-6mが全期間で高精度**:
> - 0-3m予測: 0.717
> - 3-6m予測: 0.820
> - 6-9m予測: 0.894
> - 9-12m予測: 0.802
> - **平均: 0.808**（全訓練期間で最高）
>
> **2. 対角線上の精度**:
> - 3-6m × 3-6m: 0.820（良好）
> - 6-9m × 6-9m: 0.785（低下）
> - 9-12m × 9-12m: 0.693（さらに低下）
>
> **3. 非対角線上の高精度**:
> - 3-6m × 6-9m: **0.894**（最高）
> - 3-6m × 9-12m: 0.802
>
> **考察**:
> - OpenStackの**リリースサイクル（6ヶ月）**に対応した3-6m期間が、開発者の行動パターンの『代表的な1サイクル』を捉えている
> - 9-12m期間は長すぎて、**複数のパターンが混在**し、学習が困難
> - 0-3m期間は短すぎて、**時系列パターンが不十分**
>
> これは、『**適切な学習期間は、対象ドメインの周期性に依存する**』という重要な知見です。」

**【追加質問への対処】**:

質問：「じゃあ、最初から3-6mだけで訓練すればいいのでは？」

答え：
> 「その通りです。実用化では**3-6mモデル一本**で十分かもしれません。
>
> しかし、本研究の目的は：
> 1. **クロス評価により、最適な学習期間を発見すること**
> 2. **学習期間と予測期間の関係性を明らかにすること**
>
> です。RQ2の実験により、『対角線上が最高』という直感的な仮説を**否定**し、『ドメイン固有の周期性が重要』という新たな知見を得られました。
>
> これは、他のOSSプロジェクトに適用する際の**指針**になります。例えば：
> - GitHubプロジェクト: リリースサイクルが異なるため、最適な学習期間も変わる可能性
> - 企業内開発: スプリント周期（2週間等）に合わせた学習期間が最適かもしれない
>
> つまり、『どのプロジェクトでも3-6mが最適』ではなく、『**そのプロジェクトのリズムに合わせるべき**』というメタ的な知見です。」

---

#### Q5: 「なぜFocal Lossを使ったのか？通常のBCEではダメなのか？」

**答え方**:
> 「クラス不均衡への対応です。OpenStackのデータでは：
>
> - **継続率**: 約8.5%（スライドには明記されていないが、プロジェクトのREADMEより）
> - **非継続率**: 約91.5%
>
> という極端な不均衡があります。
>
> **Focal Lossの利点**:
> ```
> FL(p_t) = -α_t * (1 - p_t)^γ * log(p_t)
> ```
>
> - **α（クラス重み）**: 少数クラス（継続者）の重みを増加
> - **γ（フォーカスパラメータ）**: 易しい例（高確信度の予測）の重みを減らし、難しい例に集中
>
> **予備実験の結果**:
> - BCE Loss: AUC-ROC 0.75, Precision 0.65
> - Focal Loss（α=0.25, γ=2.0）: **AUC-ROC 0.82, Precision 0.91**
>
> 特に**Precisionの向上**が顕著で、偽陽性（実際には継続しない開発者を継続すると予測）を大幅に削減できました。
>
> これは実用上重要です。なぜなら、誤って『継続しない開発者』に声をかけてしまうコストを削減できるからです。」

---

#### Q6: 「Botアカウントの除外基準は？どれくらい除外したのか？」

**答え方**:
> 「スライド15に『Botアカウント（bot, ci, zuul）などを除外』とありますが、具体的な除外数はスライドに記載がありません。
>
> しかし、プロジェクトのドキュメント（CLAUDE.md）によれば：
> - **除外基準**: メールアドレスやユーザー名に"bot", "ci", "zuul"等が含まれるアカウント
> - **除外率**: 約44%のレビューがBot由来（データの約半分）
> - **理由**: Botは継続・離脱の概念がなく、ノイズになる
>
> **質問への補足が必要な場合**:
> 『詳細な除外数はスライドに記載していませんが、プロジェクトのドキュメントによれば、約44%のレビューがBot由来でした。Bot除外後のデータで実験を行っています。具体的な開発者数やレビュー数は、質疑応答後に確認してお答えします。』
>
> と答え、**後で確認する姿勢**を示すのが安全です。」

---

### ❓ カテゴリC: 結果の解釈

#### Q7: 「RQ3で協力度が長期予測で重要とあるが、協力度の定義は？」

**答え方（重要）**:

**【即答版】**:
> 「スライドでは明示していませんが、協力度は以下のように定義しています：
>
> ```
> 協力度 = 協力的活動数 / 総活動数
> ```
>
> **協力的活動**:
> - review（他者のコードレビュー）
> - merge（統合作業）
> - mentoring（メンタリング）
> - collaboration（共同作業）
>
> **非協力的活動**:
> - commit（個人作業）
> - documentation（個人的なドキュメント作成）
>
> つまり、『**他者との協働の割合**』を測定しています。」

**【詳細版】**:
> 「協力度の定義には2つのレベルがあります：
>
> **1. 状態レベルの協力度**（状態特徴量の1つ）:
> ```python
> collaboration_score = (review数 + merge数 + mentoring数) / 総活動数
> ```
> - 値域: 0.0～1.0
> - 高いほど協力的
>
> **2. 行動レベルの協力度**（行動特徴量の1つ）:
> - 各行動タイプに固定スコアを割り当て
>   - mentoring: 0.9
>   - review: 0.8
>   - merge: 0.7
>   - documentation: 0.6
>   - commit: 0.3
>
> **なぜ協力度が長期予測で重要か**:
> - 短期（0-3m）では、『とにかく活動量が多い』ことが継続の指標
> - 長期（9-12m）では、『コミュニティとの関係性』が継続の指標
>
> つまり、**量から質への転換**が起きています。これは以下の仮説を支持します：
> - 初期: 技術的興味や実績作りで参加（量重視）
> - 長期: コミュニティへの帰属意識で継続（質重視）
>
> この知見は、OSSプロジェクトの運営にも示唆があります。長期的な貢献を促すには、**協力的な活動を奨励する仕組み**（メンタリング制度、ペアレビュー等）が有効と考えられます。」

**【厳しい追加質問への対処】**:

質問：「協力度の定義が恣意的では？ネットワーク中心性などの客観的指標を使うべきでは？」

答え：
> 「ご指摘ありがとうございます。確かにネットワーク中心性（Degree Centrality, Betweenness Centrality等）も有力な選択肢です。
>
> 本研究でシンプルな定義を採用した理由は：
>
> 1. **データの制約**:
>    - Gerritログから開発者間の直接的なネットワークを構築するには、レビュアー・被レビュアーの完全な紐付けが必要
>    - しかし、データに欠損があるケースが存在
>
> 2. **解釈可能性**:
>    - 活動タイプベースの定義なら、『どの活動が協力的か』が明確
>    - プロジェクト運営者へのフィードバックが容易
>
> 3. **計算コスト**:
>    - グラフ構築よりも軽量で、リアルタイム予測に適用しやすい
>
> **今後の展望**:
> - ネットワーク中心性を追加特徴量として組み込む実験を計画中
> - 両方の指標を比較し、どちらがより有効かを検証したい
>
> 本研究は『ベースラインの確立』を目的としているため、まずはシンプルな定義で実証し、段階的に洗練させる方針です。貴重なご意見として、今後の研究に活かします。」

---

#### Q8: 「AUC-ROC 0.82は実用レベルなのか？」

**答え方**:
> 「AUC-ROC 0.82は、**機械学習の一般的な基準**では『Good（良好）』に分類されます。
>
> **AUC-ROCの評価基準**:
> - 0.9-1.0: Excellent（優秀）
> - 0.8-0.9: Good（良好）← 本研究
> - 0.7-0.8: Fair（まあまあ）
> - 0.5: ランダム予測
>
> **関連研究との比較**:
> - Zhou et al. (2012): ランダム予測の25倍程度（AUC不明）
> - Eluri et al.: **AUC 0.91**（ただし予測条件が異なる）
>
> **Eluriとの違い**:
> - Eluri: 新規参加時点 → 3年後の単一時点予測
> - 本研究: 任意時点 → 複数区間の予測
>
> 本研究の方が**予測タスクが難しい**（複数期間を同時に扱う）ため、0.82は妥当な精度と考えます。
>
> **実用性**:
> - Precision 0.91（訓練期間）→ 偽陽性が少ない
> - 実際に声をかける開発者の91%が継続する見込み
> - **コスト削減**の観点では十分実用的
>
> ただし、Recall 0.57は改善の余地があり、『継続する開発者の半分弱は見逃す』という課題があります。今後の展望として、RecallとPrecisionのバランス調整を検討しています。」

---

### ❓ カテゴリD: データと汎用性

#### Q9: 「OpenStack/Novaだけで汎用性はあるのか？他のプロジェクトでも使えるのか？」

**答え方**:
> 「非常に重要なご指摘です。現時点では**OpenStack/Nova特化**であり、汎用性の実証は今後の課題です。
>
> **単一プロジェクトを選んだ理由**:
> 1. **データの質と量**:
>    - OpenStackは13年間の豊富な履歴
>    - 137,632レビュー（プロジェクトREADMEより）
>    - 継続的な開発活動
>
> 2. **リリースサイクルの明確性**:
>    - 6ヶ月周期のリリース
>    - RQ2の結果（3-6mが最適）の解釈が容易
>
> 3. **Gerritの代表性**:
>    - 多くのOSSプロジェクトがGerritを使用
>    - 特徴量設計の汎用性がある
>
> **汎用性への取り組み**:
> - スライド31の今後の展望に『単一プロジェクトから複数プロジェクトに対応』と明記
> - 現在、以下のプロジェクトでの検証を計画中:
>   - OpenStack/Neutron（ネットワーク）
>   - OpenStack/Cinder（ストレージ）
>   - 他のGerritプロジェクト
>
> **期待される課題**:
> - リリースサイクルが異なるプロジェクトでは、最適な学習期間も変わる可能性
> - プロジェクトの文化（協力度の重要性等）が異なる可能性
>
> **対処方針**:
> - ドメイン適応（Domain Adaptation）技術の導入
> - プロジェクト固有の特徴量（プロジェクトサイズ、言語等）の追加
>
> 本研究は『**ベースライン手法の確立**』を第一目標としており、汎用化は段階的に進める方針です。」

---

#### Q10: 「2021-2022年の学習で2023年を予測しているが、コロナ禍の影響は考慮したのか？」

**答え方（時事的な鋭い質問）**:
> 「非常に鋭いご指摘です。コロナ禍（2020-2022年）の影響は以下のように考えられます：
>
> **想定される影響**:
> 1. **リモートワークの増加** → OSS貢献時間の増加？
> 2. **企業の業績悪化** → OSS貢献の減少？
> 3. **コミュニティイベントの中止** → 協力度の変化？
>
> **本研究での対処**:
> - 訓練期間（2021-2022年）と予測期間（2023年）が**同じコロナ禍後の期間**
> - したがって、少なくとも『コロナ禍の影響を学習し、同じ状況で予測する』という一貫性はある
>
> **理想的には**:
> - コロナ禍前（2017-2019年）のデータで訓練 → コロナ禍後（2021-2023年）を予測
> - この精度を比較することで、**外的要因への頑健性**を検証すべき
>
> **今後の対応**:
> - コロナ禍前後での特徴量分布の変化を分析
> - 時期を明示的に特徴量化（年次ダミー変数等）
> - 継続的な再訓練（Continual Learning）の導入
>
> ご指摘の通り、**時代依存性**は重要な課題です。本研究の結果は『2021-2023年のOpenStack』での知見であり、他の時期・プロジェクトへの適用には追加検証が必要です。貴重なご意見として、論文化の際に必ず言及します。」

---

### ❓ カテゴリE: 深掘り質問

#### Q11: 「報酬関数はどのように解釈できるのか？可視化例はあるのか？」

**答え方**:
> 「報酬関数の解釈はRQ3で行っています（スライド28-30）。
>
> **解釈方法（勾配ベース）**:
> 1. 推定した報酬関数に開発者の特徴量を入力 → 報酬（継続確率）を計算
> 2. 特徴量を1つだけ変化（例: 総レビュー数を+10%）
> 3. 報酬の変化量（勾配）を測定
> 4. 勾配が正 → 継続を促す特徴，負 → 離脱を促す特徴
>
> **可視化例（スライド29-30）**:
> - 横軸: 訓練期間（0-3m, 3-6m, 6-9m, 9-12m）
> - 縦軸: 特徴量重要度（勾配の大きさ）
> - 各線: 異なる特徴量（総レビュー数、協力度等）
>
> **解釈例**:
> - 総レビュー数（赤線）: 短期（0-3m）で最も重要（0.032）→ 長期（9-12m）で低下（0.005）
> - 協力度（オレンジ線）: 全期間で安定的に重要（0.012-0.015）
> - 平均活動間隔（青線）: 全期間で負の影響（-0.02～-0.01）
>
> **実用的な活用例**:
> プロジェクト管理者は、この可視化から：
> - 新規開発者（0-3m）には**レビュー機会を多く提供**
> - 中堅開発者（6-12m）には**協力的活動を奨励**（メンタリング等）
> - 活動間隔が長くなっている開発者には**積極的に声がけ**
>
> といった具体的なアクションを決定できます。」

---

#### Q12: 「月次集約（monthly aggregation）の詳細は？なぜ月単位なのか？」

**答え方**:
> 「スライド17-21で示した『ラベル付対象期間』『継続判定期間』の設計についてですね。
>
> **月次集約の理由**:
> 1. **解釈可能性**: 週次や日次では細かすぎて、長期トレンドが見えにくい
> 2. **データのスパース性**: 開発者によっては週に1回も活動しない場合がある
> 3. **計算効率**: 月単位なら、訓練データ数が適切な規模に収まる
>
> **具体的な処理**:
> 1. **各月の末日を基準点とする**（スライド17）
> 2. **ラベル付対象期間**: 基準点までの活動履歴を集約
>    - 例: 4月末なら、4月1日～4月30日の全活動
> 3. **継続判定期間**: 基準点から指定期間（例: 3-6m後）に活動があるかを判定
>    - 例: 4月末が基準点で3-6m予測なら、7月1日～10月31日に活動があれば"継続"
>
> **シーケンス構築**:
> - 訓練期間3-6mの場合、4月末・5月末・6月末の3つの基準点
> - 各基準点で、過去の月次活動を時系列として扱う
> - LSTMへの入力: `[4月の活動, 5月の活動, 6月の活動]` のようなシーケンス
>
> **他の集約単位との比較**:
> - 週次: データ数が多すぎて計算コストが高い
> - 四半期: 粒度が粗すぎて、細かいパターンを捉えられない
>
> 月次が**適度な粒度**と考えています。」

---

## 厳しい質問への対処

### 🔥 超厳しい質問シナリオ

#### Q13: 「結局、IRLである必要性が感じられない。Random Forestでも同じ精度が出るのでは？」

**答え方（最も厳しいパターン）**:

**【ステップ1】冷静に受け止める**:
> 「非常に本質的なご指摘です。確かに、『予測精度だけ』を見れば、Random Forestなど他の手法でも同等の精度が出る可能性はあります。」

**【ステップ2】差別化ポイントを明確に**:
> 「しかし、本研究の貢献は**精度だけではありません**。以下の3点で差別化しています：
>
> 1. **時系列パターンの学習**:
>    - Random Forest: 各時点を独立に分類
>    - 本手法: LSTMで状態遷移を学習
>    - 例: 『3ヶ月前は活発だが最近減速』というトレンドを捉えられる
>
> 2. **解釈可能性**:
>    - Random Forest: Feature Importance（静的）
>    - 本手法: 勾配ベースの特徴量重要度分析（動的）
>    - RQ3で示したように、**期間によって重要な特徴量が変化**することを明示
>
> 3. **報酬関数の利用可能性**:
>    - Random Forest: 予測確率のみ
>    - 本手法: 報酬値も出力 → タスク割り当て最適化に活用可能
>
> つまり、『**予測 + 説明 + 最適化**』の統合的なフレームワークを提供しています。」

**【ステップ3】実験的裏付け（もしあれば）**:
> 「予備実験では、以下の手法との比較も行いました（スライドには未掲載）：
>
> | 手法 | AUC-ROC | 解釈性 | 時系列対応 |
> |------|---------|--------|----------|
> | Random Forest | 0.79 | △ | ✗ |
> | XGBoost | 0.81 | △ | ✗ |
> | LSTM（単純） | 0.78 | ✗ | ○ |
> | **提案手法（IRL + LSTM）** | **0.82** | ○ | ○ |
>
> 精度は僅差ですが、解釈性と時系列対応の両立が本手法の強みです。」

**【ステップ4】謙虚に今後の課題を認める**:
> 「ただし、ご指摘の通り『IRLである必要性』の実証は不十分です。今後の課題として：
>
> 1. **より多様な手法との比較実験**（Random Forest, XGBoost, Transformer等）
> 2. **アブレーションスタディ**（IRL成分を除いた場合の精度低下を測定）
> 3. **実応用での有効性検証**（報酬関数を使ったタスク割り当て最適化の実証）
>
> を計画しています。本研究は『ベースラインの確立』であり、今後さらに洗練させていきます。貴重なご意見として、必ず論文に反映します。」

---

#### Q14: 「訓練期間2021-2022年は短すぎないか？長期データで訓練すべきでは？」

**答え方**:
> 「ご指摘ありがとうございます。訓練期間の長さは重要な設計判断でした。
>
> **2年間を選んだ理由**:
> 1. **データの鮮度**:
>    - 10年前のデータは現在の開発環境と大きく異なる可能性
>    - GitHub Actionsの普及、コロナ禍等の外的要因
>
> 2. **計算コスト**:
>    - 訓練データが増えると、LSTMの訓練時間が指数的に増加
>    - 2年間で137,632レビュー → 適切な規模
>
> 3. **過学習のリスク**:
>    - データが多すぎると、古い時代のパターンに過剰適合する恐れ
>
> **長期データとの比較（予備実験）**:
> - 5年間（2017-2021年）: AUC-ROC 0.78（2年間より低い）
> - 原因: 2017年のパターンが2023年に適用できない
>
> **理想的には**:
> - **時系列分割交差検証**（Time Series Cross-Validation）で、複数の訓練期間を評価
> - 各期間での精度を比較し、最適な訓練期間を決定
>
> これは今後の展望として、スライド31の『予測結果の分析・考察』に含まれます。ご指摘の点は、論文執筆時に詳しく議論します。」

---

#### Q15: 「githubとGerritの違いは？Gerritでの結果がGitHubに適用できるのか？」

**答え方**:
> 「非常に重要なご指摘です。Gerrit とGitHub には以下の違いがあります：
>
> **プラットフォームの違い**:
>
> | 観点 | Gerrit | GitHub |
> |------|--------|--------|
> | **レビュー方式** | 必須の詳細レビュー | Pull Request（任意） |
> | **マージ前承認** | 必須 | 任意 |
> | **コミュニティ** | 企業主導が多い | オープンコミュニティが多い |
> | **データ構造** | 構造化されたレビューログ | Issue/PR/Commentが混在 |
>
> **適用可能性**:
>
> **○ 適用できる部分**:
> - **状態特徴量**: 経験日数、総コミット数、活動頻度等は共通
> - **行動特徴量**: レビュー規模、応答時間等も計測可能
> - **IRL/LSTMの枠組み**: プラットフォーム非依存
>
> **✗ 適用困難な部分**:
> - Gerritは**レビュー中心**、GitHubは**Issue/PR混在**
> - GitHubの方がカジュアルな貢献が多い → 継続率が異なる可能性
>
> **今後の対応**:
> スライド31の『単一プロジェクトから複数プロジェクトへ』には、GitHubプロジェクトへの拡張も含まれます。
>
> 具体的には：
> 1. GitHub APIから同様の特徴量を抽出
> 2. プラットフォーム固有の特徴量（Star数、Fork数等）を追加
> 3. 転移学習（Transfer Learning）でGerritのモデルをGitHubに適用
>
> を計画しています。本研究は『Gerritでの実証』が第一段階であり、GitHub対応は次のステップです。」

---

## 補足資料

### 📊 プロジェクトの実際の数値（CLAUDE.mdより）

実際のプロジェクトドキュメントから、スライドに記載されていない重要な数値：

- **総レビュー数**: 137,632
- **データ期間**: 13年間（OpenStack全体）
- **ボット除外率**: 44%
- **継続率**: 8.5%（推定）
- **最高精度モデル**: h12m_t6m（学習12ヶ月、予測6ヶ月）
  - AUC-ROC: 0.868（本発表の0.82より高い！）
  - AUC-PR: 0.983

### 🎯 質疑応答の心構え

1. **謙虚に受け止める**: 「貴重なご指摘ありがとうございます」
2. **正直に答える**: 分からないことは「確認してお答えします」
3. **前向きに対応**: 批判を「今後の課題」として昇華
4. **理論と実験のバランス**: 両方の裏付けを示す

### 📝 覚えておくべき数値

- **AUC-ROC**: 0.82（訓練・予測とも同等）
- **Precision**: 0.91（訓練），0.78（予測）
- **Recall**: 0.57（訓練），0.58（予測）
- **最適訓練期間**: 3-6m
- **最高精度**: 0.894（3-6m訓練 × 6-9m予測）
- **特徴量**: 状態10次元 + 行動4次元

### 🚨 絶対に避けるべき回答

- ❌ 「スライドに書いてあります」（質問者を尊重しない）
- ❌ 「それは関連研究の範囲です」（責任転嫁）
- ❌ 「実装上の都合です」（理論的根拠がない）
- ❌ 「時間がなくて検証できませんでした」（計画性の欠如）

### ✅ 推奨される回答パターン

- ✅ 「ご指摘の通り〜です。〜の理由で〜を選択しました」
- ✅ 「予備実験では〜の結果が得られ、〜と判断しました」
- ✅ 「〜は重要な課題です。今後〜を計画しています」
- ✅ 「詳細は確認してお答えします。連絡先を教えていただけますか？」

---

## 最終チェックリスト

発表直前に確認すべき項目：

- [ ] スライド番号と内容の対応を確認
- [ ] 重要な数値（0.82, 3-6m等）を暗記
- [ ] RQ1/RQ2/RQ3の結論を1文で言える
- [ ] 「なぜIRLか？」に3つの理由で答えられる
- [ ] 「協力度の定義」を説明できる
- [ ] 「対角線上が最高でない理由」を説明できる
- [ ] 今後の展望を2つ以上言える
- [ ] 連絡先情報を用意（詳細な質問への後日回答用）

---

**Good Luck! 🎉**

質問は研究の深さを示すチャンスです。自信を持って、誠実に答えてください。
