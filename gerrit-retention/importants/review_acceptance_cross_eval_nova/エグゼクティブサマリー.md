# レビュー承諾予測IRLモデル - エグゼクティブサマリー

## 実験の目的と背景

**課題**: コードレビューにおいて、どのレビュアーがレビュー依頼を承諾するかを予測することは、効率的なタスク割り当てと開発者の継続的な参加を促進する上で重要である。

**アプローチ**: 逆強化学習（IRL）とLSTMを組み合わせたモデルにより、開発者の行動履歴から報酬関数を推定し、将来の承諾行動を予測する。

**データ**: OpenStack Novaプロジェクトの36ヶ月間（2021-2024）のレビュー履歴

---

## 主要な研究質問（RQ）と回答

### RQ1: 提案モデルの予測精度はどの程度か？

**回答: 実用的な高精度を達成（AUC-ROC 0.754, AUC-PR 0.656）**

#### キーファインディング
- **平均AUC-ROC**: 0.754（0.5=ランダム、0.7-0.8=優れた予測）
- **平均AUC-PR**: 0.656（ベースライン0.327の約2倍）
- **最高性能**: AUC-ROC 0.910（train: 0-3m, eval: 6-9m）
- **汎化性能**: 訓練-評価差わずか0.009（優秀）

#### 実用性の評価
- **Precision 0.778**: 予測した正例の78%が実際に承諾
- **Recall 0.538**: 実際に承諾する人の54%を検出
- → **レビュアー推薦システムとして実用可能な精度**

---

### RQ2: 期間長は予測精度にどう影響するか？

**回答: 中期期間（3-6ヶ月）で最高性能、長期で性能低下**

#### 訓練期間別の性能

| 期間 | AUC-ROC | AUC-PR | F1 Score | 評価 |
|------|---------|--------|----------|------|
| 0-3m | 0.717 | 0.579 | 0.591 | 短すぎる |
| **3-6m** | **0.820** | **0.766** | **0.645** | **最適** ⭐ |
| 6-9m | 0.785 | 0.742 | 0.581 | 良好 |
| 9-12m | 0.693 | 0.536 | 0.727 | 分布シフト |

#### キーファインディング

**最適設定**:
- **訓練期間**: 3-6ヶ月（十分なデータ量と安定したパターン）
- **予測期間**: 6-9ヶ月後（時間的ギャップがあっても高精度）
- **最高組み合わせ**: train 0-3m → eval 6-9m（AUC-ROC 0.910）

**期間長の影響**:
- **短期（0-3m）**: データ不足、ノイズが多い → AUC-ROC 0.717
- **中期（3-6m）**: 最適なバランス → **AUC-ROC 0.820** ⭐
- **長期（9-12m）**: 分布シフト、過去データの陳腐化 → AUC-ROC 0.693

**クロス評価の発見**:
- 異なる期間でのクロス評価が高精度（train: 早期 → eval: 後期）
- モデルが一般化されたパターンを学習している証拠
- 6-9m評価期間が最も予測しやすい（平均AUC-ROC 0.824）

---

### RQ3: 継続的な参加を促す動機要因は何か？

**回答: 「経験の蓄積」「協力行動」「活動の継続性」が鍵**

#### TOP5特徴量（全体）

| 順位 | 特徴量 | 重要度 | カテゴリ | 解釈 |
|-----|-------|--------|---------|------|
| 1 | **総レビュー数** | **+0.0165** | 状態 | 経験の蓄積 → 自信 → 継続 |
| 2 | **協力度** | **+0.0131** | 行動 | チームワーク → 社会的報酬 |
| 3 | **平均活動間隔** | **-0.0107** | 状態 | 長期化 → エンゲージメント喪失 |
| 4 | 強度（ファイル数） | +0.0083 | 行動 | 積極的な関与 |
| 5 | 総コミット数 | +0.0080 | 状態 | 開発経験 → スキル向上 |

#### ポジティブな動機（継続を促進）

**1. 経験の蓄積（総レビュー数 +0.0165）**
- 多くのレビュー経験 → スキル向上 → 自己効力感 → 継続意欲
- 初期期間（0-3m）で最も重要（+0.0316）
- プロジェクトへのコミットメントの指標

**2. 協力的な行動（協力度 +0.0131）**
- 他者との協働 → 社会的報酬（承認、達成感）
- 全期間で一貫して正の影響（最も安定した特徴量）
- 長期期間（9-12m）で最重要（+0.0146）

**3. 活発な活動（最近の活動頻度 +0.0076）**
- 定期的な参加 → プロジェクトへの関心維持
- 最新情報への追従 → スムーズなレビュー

#### ネガティブな動機（離脱を促進）

**1. 活動間隔の長期化（平均活動間隔 -0.0107）**
- 長期間の不在 → エンゲージメント喪失
- スキルの陳腐化 → レビューの困難化
- コミュニティからの疎外感

**2. 活動トレンドの減少（-0.0064）**
- 減少傾向 → 関心の低下、バーンアウトの兆候
- 他の優先事項への移行を示唆

**3. 過度な品質要求（コード品質スコア -0.0078）**
- 高すぎる品質基準 → 心理的負担
- 完璧主義 → レビュー疲れ

#### 期間別の変化パターン

**初期期間（0-3m）: 経験が最重要**
- 総レビュー数: +0.0316（圧倒的）
- プロジェクトへの初期エンゲージメントが鍵

**成熟期（3-6m）: バランスの取れた重要度**
- 複数の要因がバランス良く機能
- 最高性能の理由

**長期期間（9-12m）: 協力と継続性が重要**
- 協力度: +0.0146（最高）
- 経験よりも活動パターンが重要に

---

## 実務への応用

### 1. レビュアー推薦システム

**活用方法**:
- 新しいPR作成時に承諾確率の高いレビュアーを自動推薦
- Precision 0.78 → 推薦の約8割が実際に承諾

**実装手順**:
1. 直近3-6ヶ月のデータでモデル訓練
2. レビュアー候補全員の承諾確率を計算
3. 上位N名を推薦（Nはプロジェクトの慣習に応じて）

### 2. 離脱リスク検出

**ハイリスク指標**:
- 平均活動間隔 > 30日
- 活動トレンド < -0.5（減少傾向）
- 協力度 < 0.2（孤立）

**介入施策**:
- 個別メッセージでエンゲージメント回復
- 簡単なタスクの提供
- コミュニティイベントへの招待

### 3. エンゲージメント最適化

**経験促進**:
- 初心者向けの簡単なレビュー依頼
- レビュー経験の可視化（バッジ、ランキング）

**協力促進**:
- ペアレビューの推奨
- チーム内での知識共有
- 協力者への感謝の表明

**活動継続性**:
- 定期的なレビュー依頼
- 非活動期間のリマインダー
- 復帰者へのウェルカムメッセージ

---

## モデルの技術的特性

### アーキテクチャ
- **基盤モデル**: IRL（逆強化学習）+ LSTM
- **特徴量**: 状態10次元 + 行動4次元
- **隠れ層**: 128ユニット、Dropout 0.2
- **学習率**: 0.0001、エポック: 20

### データ処理
- **重み付きラベリング**: 拡張期間の負例に重み0.3
- **サンプルサイズ**: 39-60件/期間（バランス良好）
- **正例率**: 32.7-35.0%（適切なバランス）

### 評価手法
- **クロス評価**: 4訓練期間 × 4評価期間 = 16評価
- **メトリクス**: AUC-ROC, AUC-PR, Precision, Recall, F1
- **閾値最適化**: F1スコア最大化

---

## 結論と推奨事項

### モデルの強み

1. **高い予測精度**: AUC-ROC 0.754-0.910（実用的）
2. **優秀な汎化性能**: 訓練-評価差0.009（過学習なし）
3. **解釈可能な報酬関数**: 特徴量重要度で動機を理解
4. **時系列パターン学習**: LSTMで行動履歴を活用

### 推奨設定

**最適構成**:
- **訓練期間**: 3-6ヶ月
- **予測期間**: 6-9ヶ月後（または同期間）
- **更新頻度**: 月次（最新データで再訓練）

**運用シナリオ**:
1. レビュアー推薦（主要用途）
2. 離脱リスク検出（予防的介入）
3. エンゲージメント分析（戦略立案）

### 改善の方向性

**短期**:
- カ��ブレーション手法の導入（9-12m期間の改善）
- 閾値決定方法の改良

**中期**:
- プロジェクト横断の特徴量追加
- ソーシャルネットワーク特徴量の導入

**長期**:
- マルチタスク学習（承諾予測 + 品質予測）
- 強化学習エージェントとの統合（能動的タスク割り当て）

---

## 数値で見る主要成果

### 予測精度
- ✅ **AUC-ROC**: 0.754（平均）、**0.910（最高）**
- ✅ **AUC-PR**: 0.656（ベースラインの2倍）
- ✅ **Precision**: 0.778（推薦の約8割が的中）

### 最適設定
- ✅ **最適訓練期間**: 3-6ヶ月（AUC-ROC 0.820）
- ✅ **最適評価期間**: 6-9ヶ月（AUC-ROC 0.824）
- ✅ **最高組み合わせ**: train 0-3m → eval 6-9m（AUC-ROC 0.910）

### 重要な動機要因
- ✅ **総レビュー数**: +0.0165（経験の蓄積）
- ✅ **協力度**: +0.0131（社会的報酬）
- ⚠️ **平均活動間隔**: -0.0107（継続性の喪失）

### モデルの信頼性
- ✅ **汎化性能**: 訓練-評価差 0.009（過学習なし）
- ✅ **安定性**: 3-6m期間で訓練/評価が一致（差+0.002）
- ✅ **一貫性**: 全16評価で正の予測性能

---

## 参考資料

- **詳細分析**: [RQ分析レポート.md](RQ分析レポート.md)
- **総合レポート**: [総合分析レポート.md](総合分析レポート.md)
- **可視化**: [heatmaps/](heatmaps/) ディレクトリ内の各種図表

---

**作成日**: 2024年10月31日
**プロジェクト**: OpenStack Nova
**モデル**: IRL-LSTM
**評価件数**: 16通りのクロス評価
