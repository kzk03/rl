# IRL学習メカニズムの解説

## はじめに

本文書では、現在のIRLシステムがどのように状態と行動を学習しているのか、そして今回追加するタスク拒否機能がどのように学習に組み込まれるのかを、日本語の自然な文章で詳しく解説します。

---

## 第1章：現在の学習メカニズム

### 1.1 学習の全体像

現在のIRLシステムは、継続したレビュアーと離脱したレビュアーの行動パターンを学習し、新しいレビュアーが今後も継続するかどうかを予測します。この学習プロセスは、大きく分けて「状態の抽出」「行動の抽出」「時系列学習」「予測」の4つのステップから成り立っています。

まず、1人のレビュアーの活動履歴を「軌跡」として扱います。軌跡には、そのレビュアーがいつ、どのようなレビューを行ったかという情報が含まれています。この軌跡から、私たちは2種類の特徴量を抽出します。1つ目は「状態」と呼ばれる、レビュアーの累積的な特性です。2つ目は「行動」と呼ばれる、個別の活動の特性です。

### 1.2 状態とは何か

状態は、ある時点におけるレビュアーの全体的な特性を表す10次元のベクトルです。例えば、「このレビュアーはどれくらいの経験があるか」「最近どのくらいの頻度で活動しているか」「コードの品質はどうか」といった情報が含まれます。

重要なのは、状態は「累積的」であるということです。つまり、過去の活動すべてを総合して計算される指標であり、1つのタイムスタンプにおける開発者の「スナップショット」のようなものです。現在の実装では、状態には以下のような情報が含まれています。

経験日数は、レビュアーが最初に活動を開始してからの日数を表します。総変更数と総レビュー数は、これまでに行った活動の量を示します。最近の活動頻度は、直近30日間でどれくらい頻繁に活動しているかを表し、平均活動間隔は活動と活動の間の平均日数です。

活動トレンドは、活動が増加傾向にあるか、安定しているか、減少傾向にあるかを示します。協力スコアは、他の開発者とどれくらい協力的に活動しているかを表し、コード品質スコアは、レビューの質の高さを示します。

最近の受諾率は、直近30日間で自分が提出したコードがどれくらい受け入れられたかを示し、レビュー負荷は、直近30日間のレビュー依頼数が平均と比べてどれくらい多いか、または少ないかを表します。

### 1.3 行動とは何か

一方、行動は個別の活動そのものの特性を表す4次元のベクトルです。レビュアーが1つのレビューを行うたびに、1つの行動ベクトルが生成されます。現在の実装では、行動には以下のような情報が含まれています。

行動タイプは、それがコミットなのか、レビューなのか、マージなのかといった種類を表します。強度は、変更されたファイル数に基づいて計算され、その行動がどれくらい大規模だったかを示します。協力度は、その行動がどれくらい他者との協力を伴うものだったかを表します。

応答時間は、レビュー依頼を受けてから実際にレビューするまでの日数です。レビュー規模は、変更された行数に基づいて計算され、レビューの大きさを表します。

### 1.4 状態と行動の違い

ここで重要なのは、状態と行動の根本的な違いです。状態は「このレビュアーはどういう人か」という問いに答えるものであり、過去のすべての活動を集約した情報です。一方、行動は「このレビュアーは何をしたか」という問いに答えるものであり、個別のイベントの特性です。

例えば、「経験日数365日」という状態は、そのレビュアーの全体的な経験レベルを表しますが、「応答時間3日」という行動は、ある特定のレビュー依頼にどれくらい早く応答したかという個別の事実を表します。

### 1.5 時系列学習の仕組み

学習の核心は、LSTMと呼ばれるニューラルネットワークによる時系列学習です。LSTMは、時間的に並んだデータから、パターンや傾向を学習することに長けています。

具体的には、まず各行動をテンソルと呼ばれる数値配列に変換します。同様に、状態もテンソルに変換します。現在の実装では、状態は全てのタイムステップで同じ値を使用します。これは、状態が「スナップショット時点の累積情報」であるためです。

次に、各タイムステップにおいて、状態のテンソルと行動のテンソルを結合します。この結合されたテンソルがLSTMへの入力となります。LSTMは、この入力を順番に処理していき、各タイムステップで「隠れ状態」と呼ばれる内部表現を更新していきます。

重要なのは、タイムステップtの隠れ状態は、タイムステップt-1の隠れ状態を引き継ぐという点です。これにより、LSTMは過去の情報を「記憶」しながら、新しい情報を処理できます。例えば、タイムステップ0で「この人は最初は活発だった」という情報を記憶し、タイムステップ3で「最近は活動が減っている」という情報を統合して、「活動が減少傾向にある」というパターンを検出できます。

最後に、全てのタイムステップを処理した後、最終ステップの隠れ状態を使って、継続確率を予測します。この隠れ状態には、全時系列の情報が凝縮されているため、「この軌跡のパターンは継続者のパターンに似ているか、離脱者のパターンに似ているか」を判断できます。

### 1.6 損失関数と学習プロセス

学習は、予測と正解ラベルの差を最小化することで進みます。各軌跡には「このレビュアーは継続したか、離脱したか」という正解ラベルが付いています。継続した場合はラベルが1.0、離脱した場合は0.0です。

ネットワークが出力する継続確率（例えば0.8）と正解ラベル（例えば1.0）の差を計算し、この差が小さくなるようにネットワークのパラメータを調整します。これをバックプロパゲーションと呼びます。

この学習を何千回、何万回と繰り返すことで、ネットワークは「継続する人の軌跡のパターン」と「離脱する人の軌跡のパターン」を区別できるようになります。

### 1.7 現在学習できているパターン

現在のシステムは、主に以下のようなパターンを学習しています。

1つ目は活動頻度の減少パターンです。月1には5回行動し、月2には3回、月3には1回というように、行動の頻度が減っていくと離脱しやすいというパターンを学習します。これは、行動間の間隔が広がることで、応答時間が増加するという特徴として現れます。

2つ目は協力度の低下パターンです。最初は協力度0.9の活動をしていたのに、次第に協力度0.5、0.3と下がっていくと離脱しやすいというパターンです。協力的な行動が減るということは、プロジェクトへのコミットメントが下がっているサインと解釈できます。

3つ目はレビュー規模の減少パターンです。最初は大規模なレビュー（規模0.9）をしていたのに、次第に中規模（0.5）、小規模（0.2）と小さいレビューしかしなくなると、離脱の兆候として捉えられます。

---

## 第2章：タスク拒否機能を追加した場合の学習

### 2.1 なぜタスク拒否を学習するのか

現在のシステムは、「レビュー依頼があった」という事実しか記録していません。つまり、レビュアーがその依頼に実際に応答したのか、それとも無視したのかという重要な情報が欠けています。

実際のデータを見ると、レビュー依頼には「label」というカラムがあり、レビュアーが14日以内に応答した場合は1、応答しなかった場合は0が記録されています。この情報を学習に組み込むことで、「タスクを拒否し始めた人は離脱しやすい」「負荷が高くて拒否が増えると離脱する」といったパターンを学習できるようになります。

### 2.2 どのように追加するか：状態と行動の両方

タスク拒否の情報を学習に組み込む方法として、私たちは「状態と行動の両方に含める」という設計を選択しました。これが最も効果的である理由を、これから詳しく説明します。

まず、状態に拒否パターンの集約情報を追加します。具体的には、以下の4つの新しい特徴量を状態に追加します。

「最近の応答率」は、直近30日間でレビュー依頼のうち何パーセントに応答したかを表します。例えば、10件の依頼のうち4件にしか応答しなかった場合、応答率は0.4です。

「連続拒否回数」は、最新の依頼から遡って、何回連続で拒否しているかを表します。例えば、最新3件の依頼を全て拒否していれば、連続拒否回数は3です。

「拒否傾向」は、拒否が増加傾向にあるか、安定しているか、減少傾向にあるかを表します。これは、直近30日間の応答率と、60日前から90日前の応答率を比較して判定します。

「平均拒否率」は、全期間を通じて、どれくらいの割合で拒否してきたかを表します。例えば、これまで100件の依頼のうち30件を拒否していれば、平均拒否率は0.3です。

次に、行動に個別の拒否情報を追加します。各行動に「responded」という新しい特徴量を追加し、その依頼に応答した場合は1.0、拒否した場合は0.0を設定します。

### 2.3 なぜ両方に含めるのか：粒度の違い

ここで重要なのは、状態と行動では「粒度」が異なるということです。状態の「最近の応答率0.4」は、「過去30日間のマクロな傾向」を表します。一方、行動の「responded=0.0」は、「この特定の依頼への応答」というミクロな事実を表します。

例えば、ある人の状態が「最近の応答率0.4」だったとします。これは、過去30日間で40パーセントしか応答していないという全体的な傾向です。しかし、行動列を見ると、もっと詳細な情報がわかります。例えば、行動列が「1.0, 1.0, 0.0, 0.0, 0.0」だったとします。これは、最初の2回は応答したが、最近3回は連続で拒否しているということを示しています。

このように、状態は「最近の傾向」という大局的な情報を提供し、行動は「いつ、どのように変化したか」という詳細な時系列情報を提供します。LSTMは、この両方の情報を統合して、より正確なパターンを学習できます。

### 2.4 LSTMがどのように学習するか

LSTMへの入力は、各タイムステップにおける「状態テンソル + 行動テンソル」です。タスク拒否機能を追加した後、この入力がどのように変化するかを見てみましょう。

タイムステップ0では、状態が「応答率1.0、連続拒否0回」、行動が「応答した（1.0）」という情報が入力されます。タイムステップ1でも同様に、応答率は高く、応答した行動が記録されています。

しかし、タイムステップ2では変化が現れます。行動が「拒否した（0.0）」となり、状態の応答率も0.8に低下します。タイムステップ3では、さらに拒否が続き、応答率は0.7、連続拒否回数が1になります。タイムステップ4では、応答率0.6、連続拒否回数2と、さらに悪化します。

LSTMは、このような時系列パターンを学習します。具体的には、「状態の悪化（応答率の低下、連続拒否回数の増加）」と「行動の変化（応答から拒否へ）」が同時に起こると、離脱リスクが高いというパターンを学習します。

### 2.5 新しく学習できるパターン

タスク拒否機能を追加することで、以下のような新しいパターンを学習できるようになります。

**拒否増加パターン**：最初は全ての依頼に応答していたレビュアーが、次第に拒否し始め、最終的に連続で拒否するようになると、離脱しやすいというパターンです。例えば、応答→応答→拒否→拒否→拒否という時系列パターンを学習します。

**負荷過多パターン**：レビュー負荷が増加すると拒否が始まり、その後離脱するというパターンです。例えば、状態のレビュー負荷が0.5から1.8、2.0と増加し、同時に行動の応答が1.0から0.0に変化すると、離脱リスクが高いと学習します。

**選択的拒否パターン**：大規模なタスクだけを選択的に拒否する場合、それが必ずしも離脱に直結しないというパターンも学習できます。例えば、レビュー規模0.9のタスクは拒否するが、規模0.2のタスクは応答する、という行動パターンです。この場合、状態の応答率は低下しますが、行動の選択性をLSTMが学習できるため、総合的な判断が可能になります。

### 2.6 因果関係の学習

最も重要なのは、因果関係を学習できるということです。例えば、「レビュー負荷が増加する → タスク拒否が増える → 離脱する」という因果チェーンを考えてみましょう。

状態のレビュー負荷は、過去30日間の依頼数を平均と比較して計算されます。行動の応答情報は、各依頼に対する実際の応答を記録します。ラベルは、予測期間中に継続したか離脱したかを示します。

LSTMは、これらを時系列で学習することで、「負荷が高い状態（状態特徴）→ 拒否する行動（行動特徴）→ 離脱（ラベル）」という因果関係を捉えることができます。単に「拒否が多い人は離脱する」という相関を学習するだけでなく、「なぜ拒否が増えたのか（負荷が高い）」という原因も含めて学習できるのです。

---

## 第3章：設計の選択肢と比較

### 3.1 選択肢1：拒否を状態のみに含める

1つ目の選択肢は、拒否情報を状態にのみ含め、行動には含めないという方法です。この場合、状態に「最近の応答率」「連続拒否回数」などを追加しますが、行動には「responded」を追加しません。

この方法のメリットは、状態が「累積的な特性」であるため、拒否パターンが自然に馴染むという点です。また、予測時に「最近の拒否傾向」だけで判断できるため、タスクの詳細情報が不要になります。計算も軽くなります。

しかし、デメリットとして、個別の拒否イベントの詳細が失われます。例えば、「いつ拒否したか」という細かい時系列情報が粗くなります。LSTMが「拒否→拒否→拒否」という連続パターンを直接学習できません。

### 3.2 選択肢2：拒否を行動のみに含める

2つ目の選択肢は、拒否情報を行動にのみ含め、状態には含めないという方法です。この場合、各行動に「responded」を追加しますが、状態には拒否関連の特徴量を追加しません。

この方法のメリットは、個別の拒否イベントを時系列で詳細に学習できるという点です。LSTMが「応答→拒否→拒否」という細かいパターンを捉えられます。また、タスクの特徴（強度、レビュー規模など）と拒否の相関も学習できます。

しかし、デメリットとして、状態に比べて粒度が細かすぎる可能性があります。また、パディング時に最初の行動を繰り返すため、拒否情報が歪む可能性があります。例えば、最初の行動が「拒否」だった場合、それが15回繰り返されてしまいます。

### 3.3 選択肢3：拒否を両方に含める（採用）

3つ目の選択肢は、拒否情報を状態と行動の両方に含めるという方法です。これが、私たちが採用した設計です。

この方法の最大のメリットは、状態で大局的なパターンを捉え、行動で細かい時系列パターンを捉えるという、両方の利点を享受できる点です。状態の「最近の応答率0.4」は「過去30日間の傾向」を表し、行動の「responded」列は「いつ、どのように変化したか」を表します。

LSTMは、この両方の情報を統合できます。例えば、状態で「応答率40パーセント」という全体傾向を把握しつつ、行動列で「最近3回連続で拒否している」という具体的なパターンを検出します。この統合により、より正確な予測が可能になります。

また、この設計では予測時にタスク情報が不要です。状態が過去の拒否パターンを集約し、行動が個別の拒否イベントを記録しているため、「この人は最近どのくらい拒否しているか」という情報だけで、継続/離脱を予測できます。新しいタスクの詳細（プロジェクト、規模など）を知る必要がありません。

デメリットとしては、若干の情報重複があります。状態の「最近の応答率」と行動の「responded」は、どちらも拒否に関する情報です。しかし、粒度が異なるため、実際には問題になりません。むしろ、異なる粒度の情報が相互補完することで、より豊かな学習が可能になります。

### 3.4 選択肢4：別タスクとして学習

4つ目の選択肢は、タスク応答予測と継続予測を別々のモデルとして学習するという方法です。1つ目のモデルは「このレビュアーはこのタスクに応答するか」を予測し、2つ目のモデルは「このレビュアーは継続するか」を予測します。

この方法のメリットは、設計がクリーンであることです。各モデルが明確な責任を持ち、タスク応答予測では「どんなタスクが拒否されるか」を直接学習できます。

しかし、デメリットとして、2つのモデルが必要になるため複雑になります。また、最も重要なのは、「拒否→離脱」という因果関係を直接学習できないという点です。2つのモデルが独立しているため、タスク応答モデルが「このタスクは拒否される」と予測しても、それが継続予測モデルに伝わりません。

---

## 第4章：なぜ選択肢3が最適なのか

### 4.1 粒度の違いを活用する

選択肢3が最適である第1の理由は、粒度の違いを活用できるという点です。状態の「最近の応答率」は30日単位で集約された情報であり、「最近1ヶ月の全体傾向」を表します。一方、行動の「responded」は各イベント単位の情報であり、「この特定の依頼への応答」を表します。

例えば、状態が「応答率0.4」、行動列が「1.0, 1.0, 0.0, 0.0, 0.0」だったとします。状態だけを見ると「この人は最近40パーセントしか応答していない」という情報しかわかりません。しかし、行動列を見ると「前半は応答していたが、後半3回は連続で拒否している」という具体的なパターンがわかります。

LSTMは、この2つの情報を統合して、「応答率は全体的に40パーセントと低いが、特に最近は連続で拒否している。これは離脱の強いシグナルだ」と判断できます。もし状態だけであれば、「応答率40パーセント」という静的な情報しか得られません。もし行動だけであれば、「最近3回拒否」という局所的な情報しか得られません。両方を組み合わせることで、大局と詳細の両方を捉えられるのです。

### 4.2 LSTMの能力を最大限活用する

選択肢3が最適である第2の理由は、LSTMの時系列学習能力を最大限に活用できるという点です。

LSTMは、各タイムステップで「状態 + 行動」を入力として受け取ります。タイムステップ0では、状態が「応答率1.0、連続拒否0回」、行動が「応答した」という情報が入力されます。タイムステップ1でも同様です。

しかし、タイムステップ2で変化が起こります。行動が「拒否した」となり、状態の応答率も0.8に低下します。LSTMの隠れ状態には、タイムステップ0と1の「健全な状態」が記憶されています。タイムステップ2で初めて拒否が現れたとき、LSTMは「これは変化だ」と認識できます。

タイムステップ3では、さらに拒否が続き、状態の応答率が0.7、連続拒否回数が1になります。LSTMの隠れ状態には、「健全だった過去」と「拒否が始まった最近」の両方が記憶されているため、「悪化傾向」を検出できます。

タイムステップ4では、応答率0.6、連続拒否回数2と、さらに悪化します。LSTMは、この一連の変化を統合して、「状態の悪化（応答率の低下、連続拒否回数の増加）と行動の変化（応答から拒否へ）が同時に進行している。これは離脱の強いシグナルだ」と学習します。

このように、状態と行動の両方を時系列で入力することで、LSTMは変化の過程そのものを学習できます。単に「応答率が低い」という静的な事実ではなく、「応答率が高かったのに低くなった」という動的な変化を捉えられるのです。

### 4.3 予測時にタスク情報が不要

選択肢3が最適である第3の理由は、予測時にタスク情報が不要であるという点です。

予測時、私たちはレビュアーの過去の活動履歴から状態と行動を抽出します。状態には「最近の応答率0.3」「連続拒否回数5回」といった拒否パターンの集約情報が含まれています。行動列には、最近の各依頼への応答（例えば「0.0, 0.0, 0.0, 0.0」）が含まれています。

この情報だけで、モデルは継続/離脱を予測できます。新しいタスクが来たときに、「このタスクはどんなプロジェクトか」「どれくらいの規模か」といった情報を知る必要がありません。レビュアーの過去の拒否パターンが、状態と行動に十分表現されているからです。

これは非常に実用的な利点です。なぜなら、予測時点では「このレビュアーは今後も継続するか」を知りたいのであって、「このレビュアーは次のタスクに応答するか」を知りたいわけではないからです。後者を予測するには、タスクの詳細情報（プロジェクト、規模、緊急度など）が必要になりますが、前者を予測するには、レビュアーの最近の行動パターンだけで十分なのです。

### 4.4 因果関係を学習できる

選択肢3が最適である第4の理由は、因果関係を学習できるという点です。

現実のデータでは、「レビュー負荷が増加する → タスクを拒否し始める → 離脱する」という因果チェーンがよく見られます。状態と行動の両方に拒否情報を含めることで、この因果チェーンを直接学習できます。

具体的には、状態のレビュー負荷が0.5から1.5、2.0と増加していく様子をLSTMが追跡します。同時に、行動の応答が1.0から0.0に変化していく様子も追跡します。そして、最終的に離脱というラベルに到達します。

LSTMは、この時系列パターンから「負荷が増加すると拒否が始まり、拒否が続くと離脱する」という因果関係を学習します。単に「拒否が多い人は離脱する」という相関ではなく、「なぜ拒否が増えたのか（負荷が高い）」という原因も含めて理解できるのです。

もし状態だけに拒否情報を含めた場合、「応答率が低い人は離脱する」という相関は学習できますが、「なぜ応答率が低くなったのか」という過程を学習することは難しくなります。もし行動だけに拒否情報を含めた場合、「拒否が続くと離脱する」という局所的なパターンは学習できますが、「拒否が始まる前の状態（負荷が高い）」を捉えることが難しくなります。

両方に含めることで、「原因（負荷増加）→ 変化（拒否開始）→ 結果（離脱）」という全体の流れを学習できるのです。

### 4.5 実装コストが低い

選択肢3が最適である第5の理由は、実装コストが低いという点です。

既存のシステムは、状態10次元、行動4次元で設計されています。選択肢3では、これを状態14次元（4次元追加）、行動5次元（1次元追加）に拡張するだけです。ネットワークの構造自体は変更する必要がなく、入力の次元数を変更するだけで対応できます。

また、特徴量の計算も既存のパターンを踏襲できます。状態の計算では、既に「最近の受諾率」を計算するメソッドがあるため、同じパターンで「最近の応答率」を計算できます。行動の計算では、データから「responded」フィールドを読み取って追加するだけです。

学習プロセスも変更する必要がありません。既存の学習ループをそのまま使用でき、新しい特徴量が自動的にLSTMに入力されます。

---

## 第5章：実装時の注意点

### 5.1 状態と行動の独立性を保つ

実装時に注意すべき第1の点は、状態と行動の独立性を保つということです。

状態と行動は、情報の粒度が異なります。状態は過去30日間などの集約情報であり、行動は個別のイベント情報です。この違いを保つことが重要です。

例えば、「最新の行動が拒否だったか」という情報を状態に含めてはいけません。これは行動の情報であり、状態の「累積的な特性」という性質に合いません。もし状態に含めてしまうと、行動の「responded」と情報が完全に重複してしまい、学習の効率が悪くなります。

正しい設計は、状態には「最近の応答率0.4」という集約情報を含め、行動には「responded=0.0」という個別情報を含めることです。この2つは、同じ「拒否」に関する情報ですが、粒度が異なるため、重複ではなく相互補完の関係になります。

### 5.2 時系列の整合性を保つ

実装時に注意すべき第2の点は、時系列の整合性を保つということです。

状態は、スナップショット日時（context_date）における累積情報です。例えば、context_dateが2020年3月1日の場合、状態の「最近の応答率」は2020年2月1日から3月1日までの30日間のデータから計算されます。

一方、行動は、context_dateより前の各イベントです。行動列は、過去から現在（context_date直前）までの時系列データです。

この時間関係を正しく保つことが重要です。もし状態の計算期間と行動の時系列がずれてしまうと、LSTMが混乱してしまいます。例えば、状態が「2月の応答率」を表しているのに、行動列が「3月のイベント」を含んでいると、整合性が取れません。

### 5.3 パディング時の注意

実装時に注意すべき第3の点は、パディング時の挙動です。

LSTMは固定長の入力を期待するため、行動の数がシーケンス長（例えば15）より少ない場合、パディングを行います。現在の実装では、最初の行動を繰り返すことでパディングしています。

例えば、行動が3個しかない場合、最初の行動を12回繰り返して、合計15個にします。このとき、最初の行動の「responded」が1.0（応答）であれば、1.0が12回繰り返されます。最初の行動の「responded」が0.0（拒否）であれば、0.0が12回繰り返されます。

これは、状態の「最近の応答率」と矛盾する可能性があります。例えば、状態の応答率が0.4なのに、行動列がすべて1.0でパディングされていると、整合性が取れません。

この問題は、データが少ない場合に発生します。理想的には、十分なデータがあるレビュアーだけを学習に使用することで、この問題を回避できます。また、パディングの方法を改善することも考えられます。例えば、最初の行動を繰り返すのではなく、ゼロパディング（すべて0で埋める）や平均パディング（行動の平均値で埋める）などの方法があります。

---

## 第6章：期待される効果

### 6.1 予測精度の向上

タスク拒否機能を追加することで、予測精度が大幅に向上すると期待されます。現在のAUC-ROCは0.868ですが、この機能追加により0.90以上に達すると予想されます。

精度向上の理由は、拒否パターンが離脱の非常に強いシグナルであるためです。現在のシステムは、「活動頻度が減った」「協力度が下がった」といったパターンから離脱を予測していますが、「タスクを拒否し始めた」というシグナルはより直接的で明確です。

特に、連続拒否回数は非常に強力な特徴量になると予想されます。例えば、5回連続でタスクを拒否している人は、ほぼ確実に離脱する可能性が高いでしょう。このような明確なパターンを学習できることで、偽陽性（継続する人を離脱と予測してしまう）と偽陰性（離脱する人を継続と予測してしまう）の両方が減少すると期待されます。

### 6.2 早期離脱予測

タスク拒否機能のもう1つの重要な効果は、早期離脱予測が可能になるという点です。

現在のシステムでは、活動頻度が明らかに減少してから離脱を予測できます。しかし、拒否パターンを学習することで、活動頻度が減る前の段階、つまり「拒否が始まった時点」で離脱リスクを検出できるようになります。

例えば、あるレビュアーが、これまで月5回活動していたのに、最近3回連続でタスクを拒否し始めたとします。活動頻度自体はまだ高いかもしれませんが、拒否パターンは明確に変化しています。このような早期シグナルを捉えることで、実際に活動頻度が下がる前に、離脱リスクを予測できます。

早期予測が可能になれば、プロジェクトマネージャーが早めに介入できます。例えば、レビュアーに連絡して、負荷が高すぎないか、何か問題があるかを確認できます。このような介入により、離脱を防げる可能性があります。

### 6.3 負荷管理への応用

タスク拒否機能は、負荷管理にも応用できます。

「レビュー負荷が高い → 拒否が増える → 離脱する」という因果関係を学習することで、適切な負荷レベルを推定できます。例えば、レビュー負荷が1.5を超えると拒否が始まり、2.0を超えると離脱リスクが急激に高まるといったパターンが見えるかもしれません。

このような知見は、タスク割り当ての最適化に役立ちます。各レビュアーに対して、離脱リスクが低い範囲で最大限のタスクを割り当てることで、プロジェクト全体の効率を最大化できます。

### 6.4 プロジェクト別パターンの発見

タスク拒否機能により、プロジェクト別のパターンも発見できる可能性があります。

例えば、あるレビュアーがプロジェクトAでは高い応答率を維持しているが、プロジェクトBでは低い応答率だったとします。このパターンから、そのレビュアーはプロジェクトAには興味があるが、プロジェクトBには興味が薄いということがわかります。

このような知見は、レビュアーの興味や専門性を理解するのに役立ちます。各レビュアーを、彼らが最も貢献できるプロジェクトに割り当てることで、全体の満足度と生産性を向上させることができます。

---

## おわりに

本文書では、現在のIRLシステムの学習メカニズムと、タスク拒否機能を追加した場合の学習の変化について、詳しく解説しました。

重要なポイントは以下の通りです。

現在のシステムは、状態（累積的な特性）と行動（個別のイベント）を時系列で学習し、LSTMが「継続者のパターン」と「離脱者のパターン」を区別できるようになっています。

タスク拒否機能を追加する際、私たちは「状態と行動の両方に拒否情報を含める」という設計を選択しました。状態には集約された拒否パターン（応答率、連続拒否回数など）を含め、行動には個別の拒否イベント（responded）を含めます。

この設計の最大の強みは、粒度の違いを活用できることです。状態はマクロな傾向を捉え、行動はミクロな時系列を捉えます。LSTMがこの両方を統合することで、「応答率が低下し、最近は連続で拒否している」といった複合的なパターンを学習できます。

また、この設計では予測時にタスク情報が不要です。レビュアーの過去の拒否パターンだけで、継続/離脱を予測できます。これは実用上、非常に重要な利点です。

タスク拒否機能の追加により、予測精度の向上、早期離脱予測、負荷管理への応用など、多くの効果が期待されます。

実装時には、状態と行動の独立性、時系列の整合性、パディング時の挙動などに注意する必要がありますが、全体的な実装コストは低く抑えられます。

この機能は、開発者の継続予測という重要な課題に対して、大きな改善をもたらすと期待されます。
