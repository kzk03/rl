# LSTM状態の静的使用に関する問題点

## 発見日時
2025-11-12

## 問題の概要
現在のRetentionIRLSystemの実装では、LSTMの各タイムステップで同じ状態ベクトルを使用している（各レビュー時点での動的な状態変化を反映していない）。

## 該当コード
`src/gerrit_retention/rl_prediction/retention_irl_system.py:528-530`

```python
# 状態テンソル: 各タイムステップで同じ状態を使用（簡略化）
state_tensors = [self.state_to_tensor(state) for _ in range(self.seq_len)]
state_seq = torch.stack(state_tensors).unsqueeze(0)  # [1, seq_len, state_dim]
```

## 現在の挙動

### 状態の計算方法
- **予測時点（context_date）**での開発者の状態を1回だけ計算
- その状態ベクトル（10次元）を全タイムステップ（デフォルト15ステップ）で繰り返し使用
- 各レビュー時点での状態の時系列変化は考慮されていない

### 具体例
```
予測時点: 2023-01-01
直近30日の活動頻度: 0.5（2022-12-02～2023-01-01の統計）
累積レビュー数: 100件
経験日数: 365日

LSTM入力:
タイムステップ1（15件前のレビュー、2022-10-01）:
  state = [experience_days=365, total_reviews=100, recent_frequency=0.5, ...]
タイムステップ2（14件前のレビュー、2022-10-15）:
  state = [experience_days=365, total_reviews=100, recent_frequency=0.5, ...]  ← 同じ
...
タイムステップ15（最新のレビュー、2022-12-20）:
  state = [experience_days=365, total_reviews=100, recent_frequency=0.5, ...]  ← 同じ
```

## 問題点

### 1. 時系列情報の欠落
各レビュー時点での開発者の状態変化が反映されない：
- **経験日数**: 実際は時間経過とともに増加するが、全タイムステップで同じ値
- **累積レビュー数**: 実際はレビューごとに増加するが、全タイムステップで同じ値
- **最近の活動頻度**: 各時点での直近30日の頻度は異なるはずだが、予測時点の値のみ

### 2. 状態と行動の不一致
- **行動**: 各レビューの時系列変化を正しく反映（15件の異なるレビュー）
- **状態**: 予測時点の静的な値のみ
- LSTMが学習できるのは主に**行動の時系列パターン**のみ

### 3. モデルの表現力の制限
本来LSTMが持つ「状態と行動の相互作用の時系列学習」の能力を活かしきれていない

## 影響範囲

### 現在の性能
- **AUC-ROC**: 0.868（12ヶ月学習 × 6ヶ月予測）
- 行動パターンの学習のみでもそれなりの精度を達成

### 改善の可能性
各タイムステップで動的に状態を計算すれば、以下が期待できる：
- 状態遷移パターンの学習
- 経験の蓄積による行動変化の捕捉
- より細かい時間的文脈の理解

## 理想的な実装

### 各レビュー時点での状態計算
```python
# 各タイムステップで異なる状態を計算
state_tensors = []
for i, action in enumerate(padded_actions):
    # このレビュー時点での状態を計算
    review_date = action.timestamp
    state_at_time = self.extract_developer_state(
        developer,
        activity_history[:i+1],  # この時点までの履歴のみ
        review_date
    )
    state_tensors.append(self.state_to_tensor(state_at_time))

state_seq = torch.stack(state_tensors).unsqueeze(0)  # [1, seq_len, state_dim]
```

### 期待される効果
1. **動的な経験値**: experience_days が時系列で増加
2. **段階的な累積統計**: total_reviews, total_changes が徐々に増加
3. **時間変化する活動パターン**: recent_activity_frequency が各時点で異なる値
4. **状態-行動の因果関係**: 「経験が増えると行動が変わる」パターンを学習可能

## 計算コスト

### 現在（簡略化版）
- 状態計算: 1回/軌跡
- 計算時間: 比較的高速

### 理想的な実装
- 状態計算: seq_len回/軌跡（15倍）
- 計算時間: 増加するが、より正確な学習が可能

## 対応方針の選択肢

### オプション1: 動的状態計算の実装
- **メリット**: モデルの表現力向上、より正確な時系列学習
- **デメリット**: 計算コスト増加、実装の複雑化

### オプション2: ハイブリッドアプローチ
- 重要な特徴量のみ動的に計算（experience_days, total_reviews など）
- 統計量は予測時点の値を使用（recent_activity_frequency など）
- **メリット**: バランスの取れた性能とコスト

### オプション3: 現状維持
- 行動パターンのみでも十分な精度（AUC-ROC 0.868）
- 計算コストを優先
- **デメリット**: LSTMの能力を完全に活用できていない

## 備考
- コメントに「簡略化」と明記されており、意図的な実装と思われる
- パフォーマンスとのトレードオフを考慮した設計判断の可能性
- 改善の余地がある一方、現在の精度も悪くない

## 次のアクション候補
1. 動的状態計算版を実装して精度改善効果を測定
2. 計算時間の増加量を実測
3. A/Bテストで性能差を評価
4. 必要に応じてオプション2（ハイブリッド）を検討
