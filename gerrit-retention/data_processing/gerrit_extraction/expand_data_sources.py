#!/usr/bin/env python3
"""
ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹æ‹¡å¼µã‚¹ã‚¯ãƒªãƒ—ãƒˆ
Mozillaã€OpenStack Novaã€LibreOfficeãªã©ä»–ã®ä¸»è¦OSSãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã‚’è¿½åŠ 
"""

import json
import sys
import time
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Optional

import requests

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ‘ã‚¹ã‚’è¿½åŠ 
sys.path.insert(0, str(Path(__file__).parent.parent / 'src'))

from gerrit_retention.utils.logger import get_logger

logger = get_logger(__name__)

# æ‹¡å¼µGerritã‚½ãƒ¼ã‚¹ï¼ˆä¸»è¦OSSãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆï¼‰
EXPANDED_GERRIT_SOURCES = {
    "android": {
        "url": "https://android-review.googlesource.com",
        "projects": [
            "platform/frameworks/base",
            "platform/packages/apps/Settings", 
            "platform/system/core",
            "platform/packages/apps/Camera2",
            "platform/frameworks/native"
        ]
    },
    "chromium": {
        "url": "https://chromium-review.googlesource.com",
        "projects": [
            "chromium/src",
            "chromium/src/chrome",
            "chromium/src/v8",
            "chromium/src/third_party/blink"
        ]
    },
    "eclipse": {
        "url": "https://git.eclipse.org/r",
        "projects": [
            "platform/eclipse.platform.ui",
            "jdt/eclipse.jdt.core",
            "platform/eclipse.platform.runtime",
            "jdt/eclipse.jdt.ui"
        ]
    },
    "openstack": {
        "url": "https://review.opendev.org",
        "projects": [
            "openstack/nova",           # ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ãƒˆã‚µãƒ¼ãƒ“ã‚¹
            "openstack/neutron",        # ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚µãƒ¼ãƒ“ã‚¹
            "openstack/cinder",         # ãƒ–ãƒ­ãƒƒã‚¯ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸
            "openstack/keystone",       # ã‚¢ã‚¤ãƒ‡ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã‚µãƒ¼ãƒ“ã‚¹
            "openstack/glance",         # ã‚¤ãƒ¡ãƒ¼ã‚¸ã‚µãƒ¼ãƒ“ã‚¹
            "openstack/swift",          # ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸
            "openstack/heat",           # ã‚ªãƒ¼ã‚±ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
            "openstack/horizon"         # ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰
        ]
    },
    "libreoffice": {
        "url": "https://gerrit.libreoffice.org",
        "projects": [
            "core",                     # ãƒ¡ã‚¤ãƒ³ã‚³ã‚¢
            "help",                     # ãƒ˜ãƒ«ãƒ—ã‚·ã‚¹ãƒ†ãƒ 
            "translations",             # ç¿»è¨³
            "dictionaries"              # è¾æ›¸
        ]
    },
    "wikimedia": {
        "url": "https://gerrit.wikimedia.org/r",
        "projects": [
            "mediawiki/core",           # MediaWikiã‚³ã‚¢
            "mediawiki/extensions/VisualEditor",
            "mediawiki/extensions/Wikibase",
            "operations/puppet"         # ã‚¤ãƒ³ãƒ•ãƒ©ç®¡ç†
        ]
    },
    "go": {
        "url": "https://go-review.googlesource.com",
        "projects": [
            "go",                       # Goè¨€èªæœ¬ä½“
            "tools",                    # Goé–‹ç™ºãƒ„ãƒ¼ãƒ«
            "crypto",                   # æš—å·åŒ–ãƒ©ã‚¤ãƒ–ãƒ©ãƒª
            "net"                       # ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ãƒ©ã‚¤ãƒ–ãƒ©ãƒª
        ]
    },
    "qt": {
        "url": "https://codereview.qt-project.org",
        "projects": [
            "qt/qtbase",                # Qtãƒ™ãƒ¼ã‚¹
            "qt/qtdeclarative",         # QML/Qt Quick
            "qt/qtwidgets",             # ã‚¦ã‚£ã‚¸ã‚§ãƒƒãƒˆ
            "qt/qtnetwork"              # ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯
        ]
    }
}

class ExpandedDataCollector:
    """æ‹¡å¼µãƒ‡ãƒ¼ã‚¿åé›†å™¨"""
    
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Gerrit-Retention-Research/2.0'
        })
        
    def collect_changes_with_retry(self, base_url: str, project: str, limit: int = 100, max_retries: int = 3) -> List[Dict]:
        """ãƒªãƒˆãƒ©ã‚¤æ©Ÿèƒ½ä»˜ãChangeãƒ‡ãƒ¼ã‚¿åé›†"""
        logger.info(f"ğŸ“¥ Changeãƒ‡ãƒ¼ã‚¿åé›†é–‹å§‹: {project}")
        
        for attempt in range(max_retries):
            try:
                url = f"{base_url}/changes/"
                params = {
                    "q": f"project:{project} status:merged",
                    "n": limit,
                    "o": [
                        "DETAILED_ACCOUNTS",
                        "CURRENT_REVISION", 
                        "MESSAGES",
                        "DETAILED_LABELS",
                        "CURRENT_FILES"
                    ]
                }
                
                response = self.session.get(url, params=params, timeout=45)
                
                if response.status_code == 200:
                    content = response.text
                    if content.startswith(")]}'\n"):
                        content = content[5:]
                    
                    changes = json.loads(content)
                    logger.info(f"âœ… {len(changes)}ä»¶ã®Changeã‚’å–å¾—: {project}")
                    return changes
                    
                elif response.status_code == 429:  # Rate limit
                    wait_time = 2 ** attempt
                    logger.warning(f"â³ ãƒ¬ãƒ¼ãƒˆåˆ¶é™ - {wait_time}ç§’å¾…æ©Ÿä¸­...")
                    time.sleep(wait_time)
                    continue
                    
                else:
                    logger.error(f"âŒ APIå‘¼ã³å‡ºã—å¤±æ•—: {response.status_code}")
                    if attempt < max_retries - 1:
                        time.sleep(2 ** attempt)
                        continue
                    return []
                    
            except Exception as e:
                logger.error(f"âŒ ãƒ‡ãƒ¼ã‚¿åé›†ã‚¨ãƒ©ãƒ¼ ({project}, è©¦è¡Œ {attempt + 1}): {e}")
                if attempt < max_retries - 1:
                    time.sleep(2 ** attempt)
                    continue
                return []
        
        return []
    
    def analyze_project_activity(self, changes: List[Dict]) -> Dict:
        """ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæ´»å‹•åˆ†æ"""
        if not changes:
            return {}
        
        # æœŸé–“åˆ†æ
        dates = []
        for change in changes:
            if change.get('created'):
                dates.append(change['created'][:10])  # YYYY-MM-DD
        
        # é–‹ç™ºè€…åˆ†æ
        authors = set()
        reviewers = set()
        
        for change in changes:
            if change.get('owner', {}).get('email'):
                authors.add(change['owner']['email'])
            
            for label_name, label_info in change.get('labels', {}).items():
                for vote in label_info.get('all', []):
                    if vote.get('email'):
                        reviewers.add(vote['email'])
        
        # è¤‡é›‘åº¦åˆ†æ
        total_insertions = sum(change.get('insertions', 0) for change in changes)
        total_deletions = sum(change.get('deletions', 0) for change in changes)
        
        return {
            'total_changes': len(changes),
            'unique_authors': len(authors),
            'unique_reviewers': len(reviewers),
            'date_range': {
                'earliest': min(dates) if dates else None,
                'latest': max(dates) if dates else None
            },
            'code_changes': {
                'total_insertions': total_insertions,
                'total_deletions': total_deletions,
                'avg_insertions_per_change': total_insertions / len(changes) if changes else 0,
                'avg_deletions_per_change': total_deletions / len(changes) if changes else 0
            }
        }
    
    def collect_comprehensive_data(self, 
                                 sources_to_collect: List[str] = None,
                                 max_changes_per_project: int = 100) -> Dict:
        """åŒ…æ‹¬çš„ãƒ‡ãƒ¼ã‚¿åé›†"""
        logger.info("ğŸš€ æ‹¡å¼µGerritãƒ‡ãƒ¼ã‚¿åé›†é–‹å§‹")
        
        if sources_to_collect is None:
            sources_to_collect = list(EXPANDED_GERRIT_SOURCES.keys())
        
        all_changes = []
        all_developers = []
        project_stats = {}
        
        for source_name in sources_to_collect:
            if source_name not in EXPANDED_GERRIT_SOURCES:
                logger.warning(f"âš ï¸ æœªçŸ¥ã®ã‚½ãƒ¼ã‚¹: {source_name}")
                continue
                
            source_info = EXPANDED_GERRIT_SOURCES[source_name]
            logger.info(f"ğŸ“¡ {source_name} ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿åé›†ä¸­...")
            
            source_changes = []
            source_developers = []
            
            for project in source_info['projects']:
                logger.info(f"  ğŸ” ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ: {project}")
                
                changes = self.collect_changes_with_retry(
                    source_info['url'], 
                    project, 
                    max_changes_per_project
                )
                
                if changes:
                    source_changes.extend(changes)
                    all_changes.extend(changes)
                    
                    # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆçµ±è¨ˆ
                    project_stats[f"{source_name}/{project}"] = self.analyze_project_activity(changes)
                    
                    # é–‹ç™ºè€…æƒ…å ±ã‚’æŠ½å‡º
                    developers = self.extract_developer_info(changes, source_name, project)
                    source_developers.extend(developers)
                    all_developers.extend(developers)
                
                # ãƒ¬ãƒ¼ãƒˆåˆ¶é™å¯¾ç­–
                time.sleep(2)
            
            logger.info(f"âœ… {source_name}: {len(source_changes)}ä»¶ã®Change, {len(source_developers)}åã®é–‹ç™ºè€…")
        
        # é‡è¤‡é™¤å»
        unique_developers = self.deduplicate_developers(all_developers)
        
        logger.info(f"ğŸ“Š ç·åé›†çµæœ:")
        logger.info(f"   ğŸ“ Change: {len(all_changes)}ä»¶")
        logger.info(f"   ğŸ‘¥ é–‹ç™ºè€…: {len(unique_developers)}åï¼ˆé‡è¤‡é™¤å»å¾Œï¼‰")
        logger.info(f"   ğŸ¢ ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ: {len(project_stats)}å€‹")
        
        return {
            'changes': all_changes,
            'developers': unique_developers,
            'project_statistics': project_stats,
            'collection_timestamp': datetime.now().isoformat(),
            'sources': sources_to_collect,
            'collection_summary': {
                'total_changes': len(all_changes),
                'total_developers': len(unique_developers),
                'total_projects': len(project_stats)
            }
        }
    
    def extract_developer_info(self, changes: List[Dict], source: str, project: str) -> List[Dict]:
        """é–‹ç™ºè€…æƒ…å ±æŠ½å‡ºï¼ˆæ‹¡å¼µç‰ˆï¼‰"""
        developers = {}
        
        for change in changes:
            # ä½œæˆè€…æƒ…å ±
            owner = change.get('owner', {})
            if owner and 'email' in owner:
                dev_id = owner['email']
                if dev_id not in developers:
                    developers[dev_id] = {
                        'developer_id': dev_id,
                        'name': owner.get('name', 'Unknown'),
                        'first_seen': change.get('created', ''),
                        'changes_authored': 0,
                        'changes_reviewed': 0,
                        'total_insertions': 0,
                        'total_deletions': 0,
                        'projects': set(),
                        'sources': set(),
                        'last_activity': change.get('created', ''),
                        'review_scores': []
                    }
                
                developers[dev_id]['changes_authored'] += 1
                developers[dev_id]['total_insertions'] += change.get('insertions', 0)
                developers[dev_id]['total_deletions'] += change.get('deletions', 0)
                developers[dev_id]['projects'].add(project)
                developers[dev_id]['sources'].add(source)
                
                # æœ€æ–°æ´»å‹•æ—¥ã‚’æ›´æ–°
                if change.get('created', '') > developers[dev_id]['last_activity']:
                    developers[dev_id]['last_activity'] = change.get('created', '')
            
            # ãƒ¬ãƒ“ãƒ¥ã‚¢ãƒ¼æƒ…å ±
            for label_name, label_info in change.get('labels', {}).items():
                for vote in label_info.get('all', []):
                    reviewer = vote.get('email')
                    if reviewer and reviewer != owner.get('email'):
                        if reviewer not in developers:
                            developers[reviewer] = {
                                'developer_id': reviewer,
                                'name': vote.get('name', 'Unknown'),
                                'first_seen': change.get('created', ''),
                                'changes_authored': 0,
                                'changes_reviewed': 0,
                                'total_insertions': 0,
                                'total_deletions': 0,
                                'projects': set(),
                                'sources': set(),
                                'last_activity': change.get('created', ''),
                                'review_scores': []
                            }
                        
                        developers[reviewer]['changes_reviewed'] += 1
                        developers[reviewer]['projects'].add(project)
                        developers[reviewer]['sources'].add(source)
                        
                        # ãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚¹ã‚³ã‚¢ã‚’è¨˜éŒ²
                        if 'value' in vote:
                            developers[reviewer]['review_scores'].append(vote['value'])
        
        # ã‚»ãƒƒãƒˆã‚’ãƒªã‚¹ãƒˆã«å¤‰æ›
        for dev in developers.values():
            dev['projects'] = list(dev['projects'])
            dev['sources'] = list(dev['sources'])
            
        return list(developers.values())
    
    def deduplicate_developers(self, developers: List[Dict]) -> List[Dict]:
        """é–‹ç™ºè€…é‡è¤‡é™¤å»"""
        unique_devs = {}
        
        for dev in developers:
            dev_id = dev['developer_id']
            if dev_id not in unique_devs:
                unique_devs[dev_id] = dev
            else:
                # æ—¢å­˜ã®é–‹ç™ºè€…æƒ…å ±ã‚’ãƒãƒ¼ã‚¸
                existing = unique_devs[dev_id]
                existing['changes_authored'] += dev['changes_authored']
                existing['changes_reviewed'] += dev['changes_reviewed']
                existing['total_insertions'] += dev['total_insertions']
                existing['total_deletions'] += dev['total_deletions']
                
                # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã¨ã‚½ãƒ¼ã‚¹ã‚’ãƒãƒ¼ã‚¸
                existing['projects'] = list(set(existing['projects'] + dev['projects']))
                existing['sources'] = list(set(existing['sources'] + dev['sources']))
                
                # ã‚ˆã‚Šæ—©ã„åˆå›æ´»å‹•æ—¥ã‚’æ¡ç”¨
                if dev['first_seen'] < existing['first_seen']:
                    existing['first_seen'] = dev['first_seen']
                
                # ã‚ˆã‚Šé…ã„æœ€çµ‚æ´»å‹•æ—¥ã‚’æ¡ç”¨
                if dev['last_activity'] > existing['last_activity']:
                    existing['last_activity'] = dev['last_activity']
                
                # ãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚¹ã‚³ã‚¢ã‚’ãƒãƒ¼ã‚¸
                existing['review_scores'].extend(dev['review_scores'])
        
        return list(unique_devs.values())

def collect_expanded_data():
    """æ‹¡å¼µãƒ‡ãƒ¼ã‚¿åé›†ã®å®Ÿè¡Œ"""
    collector = ExpandedDataCollector()
    
    # åé›†ã™ã‚‹ã‚½ãƒ¼ã‚¹ã‚’é¸æŠï¼ˆå…¨éƒ¨ã¾ãŸã¯ä¸€éƒ¨ï¼‰
    sources_to_collect = [
        "openstack",      # Nova, Neutron, Cinderç­‰
        "libreoffice",    # LibreOffice
        "wikimedia",      # MediaWiki
        "android",        # Androidï¼ˆæ—¢å­˜ï¼‰
        "chromium"        # Chromiumï¼ˆæ—¢å­˜ï¼‰
    ]
    
    logger.info(f"ğŸ¯ åé›†å¯¾è±¡: {', '.join(sources_to_collect)}")
    
    # ãƒ‡ãƒ¼ã‚¿åé›†å®Ÿè¡Œ
    data = collector.collect_comprehensive_data(
        sources_to_collect=sources_to_collect,
        max_changes_per_project=150  # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã‚ãŸã‚Šæœ€å¤§150ä»¶
    )
    
    # ãƒ‡ãƒ¼ã‚¿ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆ
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    data_dir = Path("data/processed/unified")
    data_dir.mkdir(parents=True, exist_ok=True)
    
    raw_dir = Path("data/raw/expanded_gerrit")
    raw_dir.mkdir(parents=True, exist_ok=True)
    
    # æ‹¡å¼µç”Ÿãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜
    with open(raw_dir / f"expanded_data_{timestamp}.json", "w", encoding="utf-8") as f:
        json.dump(data, f, indent=2, ensure_ascii=False)
    
    # çµ±åˆå‡¦ç†æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜ï¼ˆæ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã¨çµ±åˆï¼‰
    with open(data_dir / "all_developers.json", "w", encoding="utf-8") as f:
        json.dump(data['developers'], f, indent=2, ensure_ascii=False)
    
    with open(data_dir / "all_reviews.json", "w", encoding="utf-8") as f:
        json.dump(data['changes'], f, indent=2, ensure_ascii=False)
    
    # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆçµ±è¨ˆã‚’ä¿å­˜
    with open(data_dir / f"project_statistics_{timestamp}.json", "w", encoding="utf-8") as f:
        json.dump(data['project_statistics'], f, indent=2, ensure_ascii=False)
    
    # ã‚µãƒãƒªãƒ¼ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ
    summary_report = generate_summary_report(data)
    with open(data_dir / f"collection_summary_{timestamp}.md", "w", encoding="utf-8") as f:
        f.write(summary_report)
    
    logger.info(f"âœ… æ‹¡å¼µãƒ‡ãƒ¼ã‚¿åé›†å®Œäº†:")
    logger.info(f"   ğŸ“Š é–‹ç™ºè€…: {len(data['developers'])}å")
    logger.info(f"   ğŸ“ Change: {len(data['changes'])}ä»¶")
    logger.info(f"   ğŸ¢ ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ: {len(data['project_statistics'])}å€‹")
    logger.info(f"   ğŸ“ ä¿å­˜å…ˆ: {data_dir}")
    
    return data

def generate_summary_report(data: Dict) -> str:
    """ã‚µãƒãƒªãƒ¼ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""
    report = f"""# æ‹¡å¼µGerritãƒ‡ãƒ¼ã‚¿åé›†ã‚µãƒãƒªãƒ¼

ç”Ÿæˆæ—¥æ™‚: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

## åé›†æ¦‚è¦

- **ç·Changeæ•°**: {data['collection_summary']['total_changes']:,}ä»¶
- **ç·é–‹ç™ºè€…æ•°**: {data['collection_summary']['total_developers']:,}å
- **ç·ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæ•°**: {data['collection_summary']['total_projects']}å€‹
- **åé›†ã‚½ãƒ¼ã‚¹**: {', '.join(data['sources'])}

## ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆåˆ¥çµ±è¨ˆ

"""
    
    for project_name, stats in data['project_statistics'].items():
        report += f"""### {project_name}

- Changeæ•°: {stats['total_changes']}ä»¶
- ä½œæˆè€…æ•°: {stats['unique_authors']}å
- ãƒ¬ãƒ“ãƒ¥ã‚¢ãƒ¼æ•°: {stats['unique_reviewers']}å
- æœŸé–“: {stats['date_range']['earliest']} ï½ {stats['date_range']['latest']}
- å¹³å‡æŒ¿å…¥è¡Œæ•°: {stats['code_changes']['avg_insertions_per_change']:.1f}è¡Œ
- å¹³å‡å‰Šé™¤è¡Œæ•°: {stats['code_changes']['avg_deletions_per_change']:.1f}è¡Œ

"""
    
    # é–‹ç™ºè€…æ´»å‹•çµ±è¨ˆ
    developers = data['developers']
    if developers:
        authored_counts = [dev.get('changes_authored', 0) for dev in developers]
        reviewed_counts = [dev.get('changes_reviewed', 0) for dev in developers]
        
        report += f"""## é–‹ç™ºè€…æ´»å‹•çµ±è¨ˆ

- å¹³å‡ä½œæˆChangeæ•°: {sum(authored_counts)/len(authored_counts):.2f}ä»¶
- å¹³å‡ãƒ¬ãƒ“ãƒ¥ãƒ¼Changeæ•°: {sum(reviewed_counts)/len(reviewed_counts):.2f}ä»¶
- æœ€å¤§ä½œæˆChangeæ•°: {max(authored_counts)}ä»¶
- æœ€å¤§ãƒ¬ãƒ“ãƒ¥ãƒ¼Changeæ•°: {max(reviewed_counts)}ä»¶
- è¤‡æ•°ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆå‚åŠ è€…: {len([d for d in developers if len(d.get('projects', [])) > 1])}å

## æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—

1. **ç¶™ç¶šè¦å› åˆ†æã®å®Ÿè¡Œ**
   ```bash
   uv run --directory gerrit-retention python -m gerrit_retention.cli analyze --type all
   ```

2. **å¯è¦–åŒ–ã®ç”Ÿæˆ**
   ```bash
   uv run --directory gerrit-retention python scripts/generate_visualizations.py
   ```

3. **äºˆæ¸¬ãƒ¢ãƒ‡ãƒ«ã®è¨“ç·´**
   ```bash
   uv run --directory gerrit-retention python -m gerrit_retention.cli train --component all
   ```
"""
    
    return report

if __name__ == "__main__":
    collect_expanded_data()