"""
A/Bãƒ†ã‚¹ãƒˆã«ã‚ˆã‚‹äºˆæ¸¬æˆ¦ç•¥æ¯”è¼ƒã‚·ã‚¹ãƒ†ãƒ 

è¤‡æ•°ã®äºˆæ¸¬æˆ¦ç•¥ã‚’åŒæ™‚ã«è©•ä¾¡ã—ã€æœ€é©ãªæˆ¦ç•¥ã‚’ç‰¹å®šã™ã‚‹
çµ±è¨ˆçš„ã«æœ‰æ„ãªæ¯”è¼ƒåˆ†æã‚·ã‚¹ãƒ†ãƒ 
"""

import json
import logging
import warnings
from datetime import datetime, timedelta
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import seaborn as sns
from scipy import stats
from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score
from sklearn.model_selection import StratifiedKFold

warnings.filterwarnings('ignore')

class ABTestingSystem:
    """A/Bãƒ†ã‚¹ãƒˆã«ã‚ˆã‚‹äºˆæ¸¬æˆ¦ç•¥æ¯”è¼ƒã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.logger = self._setup_logger()
        self.strategies = {}
        self.test_results = {}
        self.statistical_tests = {}
        
    def _setup_logger(self) -> logging.Logger:
        """ãƒ­ã‚¬ãƒ¼ã®è¨­å®š"""
        logger = logging.getLogger('ABTestingSystem')
        logger.setLevel(logging.INFO)
        
        if not logger.handlers:
            handler = logging.StreamHandler()
            formatter = logging.Formatter(
                '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
            )
            handler.setFormatter(formatter)
            logger.addHandler(handler)
        
        return logger
    
    def register_strategy(self, name: str, strategy_func: callable, description: str = ""):
        """äºˆæ¸¬æˆ¦ç•¥ã®ç™»éŒ²"""
        self.strategies[name] = {
            'function': strategy_func,
            'description': description,
            'results': []
        }
        self.logger.info(f"æˆ¦ç•¥ã‚’ç™»éŒ²: {name} - {description}")
    
    def create_baseline_strategies(self) -> Dict[str, callable]:
        """ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³æˆ¦ç•¥ã®ä½œæˆ"""
        strategies = {}
        
        # æˆ¦ç•¥1: æ´»å‹•é »åº¦ãƒ™ãƒ¼ã‚¹
        def activity_frequency_strategy(developer_data: Dict[str, Any]) -> float:
            """æ´»å‹•é »åº¦ã«åŸºã¥ãç¶™ç¶šäºˆæ¸¬"""
            try:
                first_seen = datetime.fromisoformat(
                    developer_data.get('first_seen', '').replace(' ', 'T')
                )
                last_activity = datetime.fromisoformat(
                    developer_data.get('last_activity', '').replace(' ', 'T')
                )
                
                activity_duration = (last_activity - first_seen).days
                total_activity = (developer_data.get('changes_authored', 0) + 
                                developer_data.get('changes_reviewed', 0))
                
                if activity_duration > 0:
                    frequency = total_activity / activity_duration
                    return min(1.0, frequency * 10)  # ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°
                else:
                    return 0.5
            except:
                return 0.0
        
        # æˆ¦ç•¥2: æœ€è¿‘ã®æ´»å‹•ãƒ™ãƒ¼ã‚¹
        def recent_activity_strategy(developer_data: Dict[str, Any]) -> float:
            """æœ€è¿‘ã®æ´»å‹•ã«åŸºã¥ãç¶™ç¶šäºˆæ¸¬"""
            try:
                last_activity = datetime.fromisoformat(
                    developer_data.get('last_activity', '').replace(' ', 'T')
                )
                days_since_last = (datetime.now() - last_activity).days
                
                if days_since_last <= 7:
                    return 0.9
                elif days_since_last <= 30:
                    return 0.7
                elif days_since_last <= 90:
                    return 0.4
                else:
                    return 0.1
            except:
                return 0.0
        
        # æˆ¦ç•¥3: ãƒãƒ©ãƒ³ã‚¹å‹
        def balanced_strategy(developer_data: Dict[str, Any]) -> float:
            """ãƒãƒ©ãƒ³ã‚¹å‹ç¶™ç¶šäºˆæ¸¬"""
            activity_score = activity_frequency_strategy(developer_data)
            recent_score = recent_activity_strategy(developer_data)
            
            # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆå¤šæ§˜æ€§ã®è€ƒæ…®
            project_count = len(developer_data.get('projects', []))
            diversity_bonus = min(0.2, project_count * 0.05)
            
            # ãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚¹ã‚³ã‚¢ã®è€ƒæ…®
            review_scores = developer_data.get('review_scores', [])
            if review_scores:
                avg_score = np.mean([abs(s) for s in review_scores])
                review_bonus = min(0.1, avg_score * 0.1)
            else:
                review_bonus = 0.0
            
            final_score = (activity_score * 0.4 + recent_score * 0.6 + 
                          diversity_bonus + review_bonus)
            
            return min(1.0, final_score)
        
        # æˆ¦ç•¥4: ä¿å®ˆçš„æˆ¦ç•¥
        def conservative_strategy(developer_data: Dict[str, Any]) -> float:
            """ä¿å®ˆçš„ç¶™ç¶šäºˆæ¸¬ï¼ˆé«˜ã„é–¾å€¤ï¼‰"""
            base_score = balanced_strategy(developer_data)
            
            # ã‚ˆã‚Šå³ã—ã„åˆ¤å®š
            total_activity = (developer_data.get('changes_authored', 0) + 
                            developer_data.get('changes_reviewed', 0))
            
            if total_activity < 20:
                return base_score * 0.5
            elif total_activity < 50:
                return base_score * 0.7
            else:
                return base_score
        
        # æˆ¦ç•¥5: ç©æ¥µçš„æˆ¦ç•¥
        def aggressive_strategy(developer_data: Dict[str, Any]) -> float:
            """ç©æ¥µçš„ç¶™ç¶šäºˆæ¸¬ï¼ˆä½ã„é–¾å€¤ï¼‰"""
            base_score = balanced_strategy(developer_data)
            
            # ã‚ˆã‚Šå¯›å®¹ãªåˆ¤å®š
            if base_score > 0.3:
                return min(1.0, base_score * 1.3)
            else:
                return base_score
        
        strategies['activity_frequency'] = activity_frequency_strategy
        strategies['recent_activity'] = recent_activity_strategy
        strategies['balanced'] = balanced_strategy
        strategies['conservative'] = conservative_strategy
        strategies['aggressive'] = aggressive_strategy
        
        return strategies
    
    def run_ab_test(self, developer_data: List[Dict[str, Any]], 
                    true_labels: np.ndarray, n_splits: int = 5) -> Dict[str, Any]:
        """A/Bãƒ†ã‚¹ãƒˆã®å®Ÿè¡Œ"""
        self.logger.info(f"A/Bãƒ†ã‚¹ãƒˆã‚’é–‹å§‹: {len(self.strategies)}æˆ¦ç•¥, {n_splits}åˆ†å‰²äº¤å·®æ¤œè¨¼")
        
        # çµæœæ ¼ç´ç”¨
        all_results = {name: [] for name in self.strategies.keys()}
        
        # äº¤å·®æ¤œè¨¼ã®å®Ÿè¡Œ
        skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)
        
        # ãƒ©ãƒ™ãƒ«ã‚’äºŒå€¤åŒ–ï¼ˆç¶™ç¶š/é›¢è„±ï¼‰
        binary_labels = (true_labels > 0.5).astype(int)
        
        for fold, (train_idx, test_idx) in enumerate(skf.split(developer_data, binary_labels)):
            self.logger.info(f"ãƒ•ã‚©ãƒ¼ãƒ«ãƒ‰ {fold + 1}/{n_splits} ã‚’å®Ÿè¡Œä¸­...")
            
            test_data = [developer_data[i] for i in test_idx]
            test_labels = true_labels[test_idx]
            test_binary_labels = binary_labels[test_idx]
            
            # å„æˆ¦ç•¥ã§äºˆæ¸¬
            for strategy_name, strategy_info in self.strategies.items():
                strategy_func = strategy_info['function']
                
                # äºˆæ¸¬ã®å®Ÿè¡Œ
                predictions = []
                for dev_data in test_data:
                    pred = strategy_func(dev_data)
                    predictions.append(pred)
                
                predictions = np.array(predictions)
                binary_predictions = (predictions > 0.5).astype(int)
                
                # ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®è¨ˆç®—
                metrics = {
                    'accuracy': accuracy_score(test_binary_labels, binary_predictions),
                    'precision': precision_score(test_binary_labels, binary_predictions, zero_division=0),
                    'recall': recall_score(test_binary_labels, binary_predictions, zero_division=0),
                    'f1': f1_score(test_binary_labels, binary_predictions, zero_division=0),
                    'mse': np.mean((predictions - test_labels) ** 2),
                    'mae': np.mean(np.abs(predictions - test_labels))
                }
                
                all_results[strategy_name].append(metrics)
        
        # çµæœã®çµ±è¨ˆå‡¦ç†
        final_results = {}
        for strategy_name, fold_results in all_results.items():
            metrics_summary = {}
            
            for metric_name in fold_results[0].keys():
                values = [result[metric_name] for result in fold_results]
                metrics_summary[metric_name] = {
                    'mean': np.mean(values),
                    'std': np.std(values),
                    'min': np.min(values),
                    'max': np.max(values),
                    'values': values
                }
            
            final_results[strategy_name] = metrics_summary
        
        self.test_results = final_results
        return final_results
    
    def perform_statistical_tests(self, metric: str = 'f1') -> Dict[str, Any]:
        """çµ±è¨ˆçš„æœ‰æ„æ€§æ¤œå®šã®å®Ÿè¡Œ"""
        self.logger.info(f"çµ±è¨ˆçš„æœ‰æ„æ€§æ¤œå®šã‚’å®Ÿè¡Œ: {metric}ãƒ¡ãƒˆãƒªãƒƒã‚¯")
        
        if not self.test_results:
            raise ValueError("A/Bãƒ†ã‚¹ãƒˆãŒå®Ÿè¡Œã•ã‚Œã¦ã„ã¾ã›ã‚“")
        
        strategy_names = list(self.test_results.keys())
        n_strategies = len(strategy_names)
        
        # ãƒšã‚¢ãƒ¯ã‚¤ã‚ºtæ¤œå®š
        pairwise_tests = {}
        significance_matrix = np.ones((n_strategies, n_strategies))
        
        for i, strategy1 in enumerate(strategy_names):
            for j, strategy2 in enumerate(strategy_names):
                if i != j:
                    values1 = self.test_results[strategy1][metric]['values']
                    values2 = self.test_results[strategy2][metric]['values']
                    
                    # å¯¾å¿œã®ã‚ã‚‹tæ¤œå®š
                    t_stat, p_value = stats.ttest_rel(values1, values2)
                    
                    pairwise_tests[f"{strategy1}_vs_{strategy2}"] = {
                        't_statistic': t_stat,
                        'p_value': p_value,
                        'significant': p_value < 0.05,
                        'effect_size': (np.mean(values1) - np.mean(values2)) / np.sqrt(
                            (np.var(values1) + np.var(values2)) / 2
                        )
                    }
                    
                    significance_matrix[i, j] = p_value
        
        # ANOVAæ¤œå®š
        all_values = [self.test_results[name][metric]['values'] for name in strategy_names]
        f_stat, anova_p_value = stats.f_oneway(*all_values)
        
        # æœ€è‰¯æˆ¦ç•¥ã®ç‰¹å®š
        strategy_means = {name: self.test_results[name][metric]['mean'] 
                         for name in strategy_names}
        best_strategy = max(strategy_means, key=strategy_means.get)
        
        statistical_results = {
            'metric': metric,
            'pairwise_tests': pairwise_tests,
            'anova': {
                'f_statistic': f_stat,
                'p_value': anova_p_value,
                'significant': anova_p_value < 0.05
            },
            'best_strategy': best_strategy,
            'strategy_ranking': sorted(strategy_means.items(), key=lambda x: x[1], reverse=True),
            'significance_matrix': significance_matrix.tolist(),
            'strategy_names': strategy_names
        }
        
        self.statistical_tests[metric] = statistical_results
        return statistical_results
    
    def generate_comparison_report(self) -> Dict[str, Any]:
        """æ¯”è¼ƒãƒ¬ãƒãƒ¼ãƒˆã®ç”Ÿæˆ"""
        if not self.test_results:
            raise ValueError("A/Bãƒ†ã‚¹ãƒˆãŒå®Ÿè¡Œã•ã‚Œã¦ã„ã¾ã›ã‚“")
        
        report = {
            'timestamp': datetime.now().isoformat(),
            'summary': {},
            'detailed_results': self.test_results,
            'statistical_tests': self.statistical_tests,
            'recommendations': []
        }
        
        # ã‚µãƒãƒªãƒ¼ã®ä½œæˆ
        metrics = ['accuracy', 'precision', 'recall', 'f1', 'mse', 'mae']
        
        for metric in metrics:
            metric_summary = {}
            
            for strategy_name, results in self.test_results.items():
                metric_summary[strategy_name] = {
                    'mean': results[metric]['mean'],
                    'std': results[metric]['std']
                }
            
            # æœ€è‰¯æˆ¦ç•¥ã®ç‰¹å®š
            if metric in ['accuracy', 'precision', 'recall', 'f1']:
                best_strategy = max(metric_summary, key=lambda x: metric_summary[x]['mean'])
            else:  # MSE, MAEï¼ˆå°ã•ã„æ–¹ãŒè‰¯ã„ï¼‰
                best_strategy = min(metric_summary, key=lambda x: metric_summary[x]['mean'])
            
            report['summary'][metric] = {
                'results': metric_summary,
                'best_strategy': best_strategy
            }
        
        # æ¨å¥¨äº‹é …ã®ç”Ÿæˆ
        recommendations = self._generate_recommendations()
        report['recommendations'] = recommendations
        
        return report
    
    def _generate_recommendations(self) -> List[str]:
        """æ¨å¥¨äº‹é …ã®ç”Ÿæˆ"""
        recommendations = []
        
        if not self.test_results:
            return recommendations
        
        # F1ã‚¹ã‚³ã‚¢ãƒ™ãƒ¼ã‚¹ã®åˆ†æ
        f1_results = {name: results['f1']['mean'] 
                     for name, results in self.test_results.items()}
        
        best_strategy = max(f1_results, key=f1_results.get)
        worst_strategy = min(f1_results, key=f1_results.get)
        
        best_f1 = f1_results[best_strategy]
        worst_f1 = f1_results[worst_strategy]
        
        recommendations.append(f"æœ€é«˜æ€§èƒ½æˆ¦ç•¥: {best_strategy} (F1: {best_f1:.4f})")
        
        if best_f1 - worst_f1 > 0.1:
            recommendations.append(f"{best_strategy}æˆ¦ç•¥ãŒä»–ã®æˆ¦ç•¥ã‚ˆã‚Šæ˜ç¢ºã«å„ªç§€ã§ã™")
            recommendations.append(f"{worst_strategy}æˆ¦ç•¥ã¯æ€§èƒ½ãŒä½ã„ãŸã‚ä½¿ç”¨ã‚’é¿ã‘ã¦ãã ã•ã„")
        
        # çµ±è¨ˆçš„æœ‰æ„æ€§ã®ç¢ºèª
        if 'f1' in self.statistical_tests:
            stat_results = self.statistical_tests['f1']
            if stat_results['anova']['significant']:
                recommendations.append("æˆ¦ç•¥é–“ã«çµ±è¨ˆçš„ã«æœ‰æ„ãªå·®ãŒã‚ã‚Šã¾ã™")
            
            # æœ€è‰¯æˆ¦ç•¥ã¨ã®æ¯”è¼ƒ
            for strategy_name in f1_results.keys():
                if strategy_name != best_strategy:
                    test_key = f"{best_strategy}_vs_{strategy_name}"
                    if test_key in stat_results['pairwise_tests']:
                        test_result = stat_results['pairwise_tests'][test_key]
                        if test_result['significant']:
                            recommendations.append(
                                f"{best_strategy}ã¯{strategy_name}ã‚ˆã‚Šçµ±è¨ˆçš„ã«æœ‰æ„ã«å„ªç§€ã§ã™ "
                                f"(p={test_result['p_value']:.4f})"
                            )
        
        # ç²¾åº¦ã¨å†ç¾ç‡ã®ãƒãƒ©ãƒ³ã‚¹
        precision_results = {name: results['precision']['mean'] 
                           for name, results in self.test_results.items()}
        recall_results = {name: results['recall']['mean'] 
                         for name, results in self.test_results.items()}
        
        best_precision = max(precision_results, key=precision_results.get)
        best_recall = max(recall_results, key=recall_results.get)
        
        if best_precision != best_recall:
            recommendations.append(
                f"ç²¾åº¦é‡è¦–ãªã‚‰{best_precision}ã€å†ç¾ç‡é‡è¦–ãªã‚‰{best_recall}ã‚’é¸æŠã—ã¦ãã ã•ã„"
            )
        
        # å®‰å®šæ€§ã®è©•ä¾¡
        stability_scores = {}
        for name, results in self.test_results.items():
            # F1ã‚¹ã‚³ã‚¢ã®æ¨™æº–åå·®ã§å®‰å®šæ€§ã‚’è©•ä¾¡
            stability_scores[name] = results['f1']['std']
        
        most_stable = min(stability_scores, key=stability_scores.get)
        recommendations.append(f"æœ€ã‚‚å®‰å®šã—ãŸæˆ¦ç•¥: {most_stable} (F1 std: {stability_scores[most_stable]:.4f})")
        
        return recommendations
    
    def create_visualization(self, output_path: str):
        """çµæœã®å¯è¦–åŒ–"""
        if not self.test_results:
            raise ValueError("A/Bãƒ†ã‚¹ãƒˆãŒå®Ÿè¡Œã•ã‚Œã¦ã„ã¾ã›ã‚“")
        
        # å›³ã®ã‚µã‚¤ã‚ºã¨ã‚¹ã‚¿ã‚¤ãƒ«è¨­å®š
        plt.style.use('seaborn-v0_8')
        fig, axes = plt.subplots(2, 3, figsize=(18, 12))
        fig.suptitle('A/Bãƒ†ã‚¹ãƒˆçµæœæ¯”è¼ƒ', fontsize=16, fontweight='bold')
        
        metrics = ['accuracy', 'precision', 'recall', 'f1', 'mse', 'mae']
        strategy_names = list(self.test_results.keys())
        
        for idx, metric in enumerate(metrics):
            row = idx // 3
            col = idx % 3
            ax = axes[row, col]
            
            # ãƒ‡ãƒ¼ã‚¿ã®æº–å‚™
            means = [self.test_results[name][metric]['mean'] for name in strategy_names]
            stds = [self.test_results[name][metric]['std'] for name in strategy_names]
            
            # ãƒãƒ¼ãƒ—ãƒ­ãƒƒãƒˆ
            bars = ax.bar(range(len(strategy_names)), means, yerr=stds, 
                         capsize=5, alpha=0.7, color='skyblue', edgecolor='navy')
            
            # æœ€è‰¯æˆ¦ç•¥ã‚’ãƒã‚¤ãƒ©ã‚¤ãƒˆ
            if metric in ['accuracy', 'precision', 'recall', 'f1']:
                best_idx = np.argmax(means)
            else:
                best_idx = np.argmin(means)
            
            bars[best_idx].set_color('gold')
            bars[best_idx].set_edgecolor('orange')
            
            ax.set_title(f'{metric.upper()}', fontweight='bold')
            ax.set_xticks(range(len(strategy_names)))
            ax.set_xticklabels(strategy_names, rotation=45, ha='right')
            ax.grid(True, alpha=0.3)
            
            # å€¤ã‚’ãƒãƒ¼ã®ä¸Šã«è¡¨ç¤º
            for i, (mean, std) in enumerate(zip(means, stds)):
                ax.text(i, mean + std + 0.01, f'{mean:.3f}', 
                       ha='center', va='bottom', fontsize=8)
        
        plt.tight_layout()
        
        # ä¿å­˜
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        plot_file = f"{output_path}/ab_test_comparison_{timestamp}.png"
        plt.savefig(plot_file, dpi=300, bbox_inches='tight')
        plt.close()
        
        self.logger.info(f"å¯è¦–åŒ–çµæœã‚’ä¿å­˜: {plot_file}")
        return plot_file
    
    def save_results(self, output_path: str) -> str:
        """çµæœã®ä¿å­˜"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # æ¯”è¼ƒãƒ¬ãƒãƒ¼ãƒˆã®ç”Ÿæˆ
        report = self.generate_comparison_report()
        
        # JSON serializable ã«å¤‰æ›
        def make_json_serializable(obj):
            """ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’JSON serializable ã«å¤‰æ›"""
            if isinstance(obj, np.ndarray):
                return obj.tolist()
            elif isinstance(obj, (np.integer, np.int64)):
                return int(obj)
            elif isinstance(obj, (np.floating, np.float64)):
                return float(obj)
            elif isinstance(obj, np.bool_):
                return bool(obj)
            elif isinstance(obj, dict):
                return {key: make_json_serializable(value) for key, value in obj.items()}
            elif isinstance(obj, list):
                return [make_json_serializable(item) for item in obj]
            elif isinstance(obj, tuple):
                return [make_json_serializable(item) for item in obj]
            else:
                return obj
        
        serializable_report = make_json_serializable(report)
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜
        results_file = f"{output_path}/ab_test_results_{timestamp}.json"
        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump(serializable_report, f, ensure_ascii=False, indent=2)
        
        self.logger.info(f"A/Bãƒ†ã‚¹ãƒˆçµæœã‚’ä¿å­˜: {results_file}")
        return results_file

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    config = {
        'output_path': 'outputs/ab_testing',
        'n_splits': 5
    }
    
    # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ä½œæˆ
    Path(config['output_path']).mkdir(parents=True, exist_ok=True)
    
    # ã‚·ã‚¹ãƒ†ãƒ ã®åˆæœŸåŒ–
    ab_system = ABTestingSystem(config)
    
    # ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³æˆ¦ç•¥ã®ç™»éŒ²
    baseline_strategies = ab_system.create_baseline_strategies()
    
    for name, func in baseline_strategies.items():
        description = {
            'activity_frequency': 'æ´»å‹•é »åº¦ãƒ™ãƒ¼ã‚¹ã®ç¶™ç¶šäºˆæ¸¬',
            'recent_activity': 'æœ€è¿‘ã®æ´»å‹•ãƒ™ãƒ¼ã‚¹ã®ç¶™ç¶šäºˆæ¸¬',
            'balanced': 'ãƒãƒ©ãƒ³ã‚¹å‹ç¶™ç¶šäºˆæ¸¬',
            'conservative': 'ä¿å®ˆçš„ç¶™ç¶šäºˆæ¸¬ï¼ˆé«˜é–¾å€¤ï¼‰',
            'aggressive': 'ç©æ¥µçš„ç¶™ç¶šäºˆæ¸¬ï¼ˆä½é–¾å€¤ï¼‰'
        }.get(name, '')
        
        ab_system.register_strategy(name, func, description)
    
    print("ğŸ§ª A/Bãƒ†ã‚¹ãƒˆã‚·ã‚¹ãƒ†ãƒ ãŒåˆæœŸåŒ–ã•ã‚Œã¾ã—ãŸ")
    print(f"ç™»éŒ²ã•ã‚ŒãŸæˆ¦ç•¥: {len(ab_system.strategies)}å€‹")
    
    return ab_system

if __name__ == "__main__":
    main()