"""
é«˜åº¦ãªäºˆæ¸¬ç²¾åº¦æ”¹å–„ã‚·ã‚¹ãƒ†ãƒ 

ç¾åœ¨ã®217.7%æ”¹å–„ã‚’é”æˆã—ãŸã‚·ã‚¹ãƒ†ãƒ ã‚’ãƒ™ãƒ¼ã‚¹ã«ã€
ã•ã‚‰ãªã‚‹ç²¾åº¦å‘ä¸Šã‚’å®Ÿç¾ã™ã‚‹åŒ…æ‹¬çš„ãªæ”¹å–„ã‚·ã‚¹ãƒ†ãƒ 
"""

import json
import logging
import pickle
import warnings
from datetime import datetime, timedelta
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple

import numpy as np
import pandas as pd
from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor
from sklearn.linear_model import Lasso, LinearRegression, Ridge
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.model_selection import GridSearchCV, cross_val_score
from sklearn.neural_network import MLPRegressor
from sklearn.preprocessing import RobustScaler, StandardScaler
from sklearn.svm import SVR

warnings.filterwarnings('ignore')

class AdvancedAccuracyImprover:
    """é«˜åº¦ãªäºˆæ¸¬ç²¾åº¦æ”¹å–„ã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.logger = self._setup_logger()
        self.models = {}
        self.scalers = {}
        self.feature_importance = {}
        self.ensemble_weights = {}
        self.performance_history = []
        
    def _setup_logger(self) -> logging.Logger:
        """ãƒ­ã‚¬ãƒ¼ã®è¨­å®š"""
        logger = logging.getLogger('AdvancedAccuracyImprover')
        logger.setLevel(logging.INFO)
        
        if not logger.handlers:
            handler = logging.StreamHandler()
            formatter = logging.Formatter(
                '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
            )
            handler.setFormatter(formatter)
            logger.addHandler(handler)
        
        return logger
    
    def extract_advanced_features(self, developer_data: Dict[str, Any]) -> Dict[str, float]:
        """é«˜åº¦ãªç‰¹å¾´é‡æŠ½å‡º"""
        features = {}
        
        # åŸºæœ¬çµ±è¨ˆç‰¹å¾´é‡
        features.update(self._extract_basic_features(developer_data))
        
        # æ™‚ç³»åˆ—ç‰¹å¾´é‡
        features.update(self._extract_temporal_features(developer_data))
        
        # è¡Œå‹•ãƒ‘ã‚¿ãƒ¼ãƒ³ç‰¹å¾´é‡
        features.update(self._extract_behavioral_features(developer_data))
        
        # ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ç‰¹å¾´é‡
        features.update(self._extract_network_features(developer_data))
        
        # å“è³ªç‰¹å¾´é‡
        features.update(self._extract_quality_features(developer_data))
        
        return features
    
    def _extract_basic_features(self, data: Dict[str, Any]) -> Dict[str, float]:
        """åŸºæœ¬çµ±è¨ˆç‰¹å¾´é‡ã®æŠ½å‡º"""
        features = {}
        
        # æ´»å‹•é‡ç‰¹å¾´é‡
        features['changes_authored'] = data.get('changes_authored', 0)
        features['changes_reviewed'] = data.get('changes_reviewed', 0)
        features['total_insertions'] = data.get('total_insertions', 0)
        features['total_deletions'] = data.get('total_deletions', 0)
        
        # æ¯”ç‡ç‰¹å¾´é‡
        total_changes = features['changes_authored'] + features['changes_reviewed']
        if total_changes > 0:
            features['author_review_ratio'] = features['changes_authored'] / total_changes
            features['review_author_ratio'] = features['changes_reviewed'] / total_changes
        else:
            features['author_review_ratio'] = 0.0
            features['review_author_ratio'] = 0.0
        
        # ã‚³ãƒ¼ãƒ‰å¤‰æ›´ç‰¹å¾´é‡
        total_lines = features['total_insertions'] + features['total_deletions']
        if total_lines > 0:
            features['insertion_ratio'] = features['total_insertions'] / total_lines
            features['deletion_ratio'] = features['total_deletions'] / total_lines
            features['avg_lines_per_change'] = total_lines / max(features['changes_authored'], 1)
        else:
            features['insertion_ratio'] = 0.0
            features['deletion_ratio'] = 0.0
            features['avg_lines_per_change'] = 0.0
        
        # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆå¤šæ§˜æ€§
        features['project_count'] = len(data.get('projects', []))
        features['source_count'] = len(data.get('sources', []))
        
        return features
    
    def _extract_temporal_features(self, data: Dict[str, Any]) -> Dict[str, float]:
        """æ™‚ç³»åˆ—ç‰¹å¾´é‡ã®æŠ½å‡º"""
        features = {}
        
        try:
            first_seen = datetime.fromisoformat(data.get('first_seen', '').replace(' ', 'T'))
            last_activity = datetime.fromisoformat(data.get('last_activity', '').replace(' ', 'T'))
            
            # æ´»å‹•æœŸé–“
            activity_duration = (last_activity - first_seen).days
            features['activity_duration_days'] = activity_duration
            
            # ç¾åœ¨ã‹ã‚‰ã®çµŒéæ™‚é–“
            now = datetime.now()
            days_since_last = (now - last_activity).days
            features['days_since_last_activity'] = days_since_last
            
            # æ´»å‹•é »åº¦
            if activity_duration > 0:
                total_changes = data.get('changes_authored', 0) + data.get('changes_reviewed', 0)
                features['activity_frequency'] = total_changes / activity_duration
            else:
                features['activity_frequency'] = 0.0
            
            # æœ€è¿‘ã®æ´»å‹•åº¦ï¼ˆé‡è¦ãªç¶™ç¶šäºˆæ¸¬æŒ‡æ¨™ï¼‰
            if days_since_last <= 7:
                features['recent_activity_score'] = 1.0
            elif days_since_last <= 30:
                features['recent_activity_score'] = 0.7
            elif days_since_last <= 90:
                features['recent_activity_score'] = 0.3
            else:
                features['recent_activity_score'] = 0.1
                
        except (ValueError, TypeError):
            features['activity_duration_days'] = 0
            features['days_since_last_activity'] = 999
            features['activity_frequency'] = 0.0
            features['recent_activity_score'] = 0.0
        
        return features
    
    def _extract_behavioral_features(self, data: Dict[str, Any]) -> Dict[str, float]:
        """è¡Œå‹•ãƒ‘ã‚¿ãƒ¼ãƒ³ç‰¹å¾´é‡ã®æŠ½å‡º"""
        features = {}
        
        # ãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚¹ã‚³ã‚¢åˆ†æ
        review_scores = data.get('review_scores', [])
        if review_scores:
            features['avg_review_score'] = np.mean(review_scores)
            features['review_score_std'] = np.std(review_scores)
            features['positive_review_ratio'] = sum(1 for s in review_scores if s > 0) / len(review_scores)
            features['negative_review_ratio'] = sum(1 for s in review_scores if s < 0) / len(review_scores)
            features['neutral_review_ratio'] = sum(1 for s in review_scores if s == 0) / len(review_scores)
        else:
            features['avg_review_score'] = 0.0
            features['review_score_std'] = 0.0
            features['positive_review_ratio'] = 0.0
            features['negative_review_ratio'] = 0.0
            features['neutral_review_ratio'] = 1.0
        
        # ä¸€è²«æ€§æŒ‡æ¨™
        changes_authored = data.get('changes_authored', 0)
        changes_reviewed = data.get('changes_reviewed', 0)
        
        if changes_authored > 0 and changes_reviewed > 0:
            features['contribution_balance'] = min(changes_authored, changes_reviewed) / max(changes_authored, changes_reviewed)
        else:
            features['contribution_balance'] = 0.0
        
        return features
    
    def _extract_network_features(self, data: Dict[str, Any]) -> Dict[str, float]:
        """ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ç‰¹å¾´é‡ã®æŠ½å‡º"""
        features = {}
        
        # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆå‚åŠ ãƒ‘ã‚¿ãƒ¼ãƒ³
        projects = data.get('projects', [])
        sources = data.get('sources', [])
        
        # å¤šæ§˜æ€§æŒ‡æ¨™
        features['project_diversity'] = len(set(projects)) if projects else 0
        features['source_diversity'] = len(set(sources)) if sources else 0
        
        # é›†ä¸­åº¦æŒ‡æ¨™ï¼ˆç‰¹å®šãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã¸ã®é›†ä¸­åº¦ï¼‰
        if projects:
            project_counts = {}
            for project in projects:
                project_counts[project] = project_counts.get(project, 0) + 1
            
            max_project_count = max(project_counts.values())
            total_project_activities = sum(project_counts.values())
            features['project_concentration'] = max_project_count / total_project_activities
        else:
            features['project_concentration'] = 0.0
        
        return features
    
    def _extract_quality_features(self, data: Dict[str, Any]) -> Dict[str, float]:
        """å“è³ªç‰¹å¾´é‡ã®æŠ½å‡º"""
        features = {}
        
        # ã‚³ãƒ¼ãƒ‰å“è³ªæŒ‡æ¨™
        total_insertions = data.get('total_insertions', 0)
        total_deletions = data.get('total_deletions', 0)
        changes_authored = data.get('changes_authored', 0)
        
        if changes_authored > 0:
            features['avg_insertions_per_change'] = total_insertions / changes_authored
            features['avg_deletions_per_change'] = total_deletions / changes_authored
            
            # å¤‰æ›´ã‚µã‚¤ã‚ºã®ä¸€è²«æ€§
            if total_insertions + total_deletions > 0:
                features['change_size_consistency'] = min(total_insertions, total_deletions) / max(total_insertions, total_deletions)
            else:
                features['change_size_consistency'] = 0.0
        else:
            features['avg_insertions_per_change'] = 0.0
            features['avg_deletions_per_change'] = 0.0
            features['change_size_consistency'] = 0.0
        
        # ãƒ¬ãƒ“ãƒ¥ãƒ¼å“è³ªæŒ‡æ¨™
        review_scores = data.get('review_scores', [])
        changes_reviewed = data.get('changes_reviewed', 0)
        
        if changes_reviewed > 0 and review_scores:
            features['review_engagement'] = len(review_scores) / changes_reviewed
            features['review_quality_score'] = np.mean([abs(s) for s in review_scores])
        else:
            features['review_engagement'] = 0.0
            features['review_quality_score'] = 0.0
        
        return features
    
    def create_ensemble_models(self) -> Dict[str, Any]:
        """ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ãƒ¢ãƒ‡ãƒ«ã®ä½œæˆ"""
        models = {
            'random_forest': RandomForestRegressor(
                n_estimators=100,
                max_depth=10,
                random_state=42,
                n_jobs=-1
            ),
            'gradient_boosting': GradientBoostingRegressor(
                n_estimators=100,
                max_depth=6,
                learning_rate=0.1,
                random_state=42
            ),
            'neural_network': MLPRegressor(
                hidden_layer_sizes=(128, 64, 32),
                max_iter=1000,
                random_state=42,
                early_stopping=True
            ),
            'support_vector': SVR(
                kernel='rbf',
                C=1.0,
                gamma='scale'
            ),
            'ridge_regression': Ridge(
                alpha=1.0,
                random_state=42
            ),
            'lasso_regression': Lasso(
                alpha=0.1,
                random_state=42
            )
        }
        
        return models
    
    def optimize_hyperparameters(self, X: np.ndarray, y: np.ndarray, model_name: str, model: Any) -> Any:
        """ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®æœ€é©åŒ–"""
        param_grids = {
            'random_forest': {
                'n_estimators': [50, 100, 200],
                'max_depth': [5, 10, 15, None],
                'min_samples_split': [2, 5, 10]
            },
            'gradient_boosting': {
                'n_estimators': [50, 100, 200],
                'max_depth': [3, 6, 9],
                'learning_rate': [0.01, 0.1, 0.2]
            },
            'neural_network': {
                'hidden_layer_sizes': [(64, 32), (128, 64), (128, 64, 32)],
                'learning_rate_init': [0.001, 0.01, 0.1]
            },
            'support_vector': {
                'C': [0.1, 1.0, 10.0],
                'gamma': ['scale', 'auto', 0.001, 0.01]
            },
            'ridge_regression': {
                'alpha': [0.1, 1.0, 10.0, 100.0]
            },
            'lasso_regression': {
                'alpha': [0.01, 0.1, 1.0, 10.0]
            }
        }
        
        if model_name in param_grids:
            self.logger.info(f"{model_name}ã®ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–ã‚’é–‹å§‹")
            
            grid_search = GridSearchCV(
                model,
                param_grids[model_name],
                cv=5,
                scoring='neg_mean_squared_error',
                n_jobs=-1
            )
            
            grid_search.fit(X, y)
            self.logger.info(f"{model_name}ã®æœ€é©ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿: {grid_search.best_params_}")
            
            return grid_search.best_estimator_
        
        return model
    
    def train_ensemble_models(self, X: np.ndarray, y: np.ndarray) -> Dict[str, Any]:
        """ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ãƒ¢ãƒ‡ãƒ«ã®è¨“ç·´"""
        models = self.create_ensemble_models()
        trained_models = {}
        model_scores = {}
        
        # ãƒ‡ãƒ¼ã‚¿ã®å‰å‡¦ç†
        scaler = RobustScaler()
        X_scaled = scaler.fit_transform(X)
        self.scalers['main'] = scaler
        
        for name, model in models.items():
            self.logger.info(f"{name}ãƒ¢ãƒ‡ãƒ«ã®è¨“ç·´ã‚’é–‹å§‹")
            
            try:
                # ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–
                optimized_model = self.optimize_hyperparameters(X_scaled, y, name, model)
                
                # ãƒ¢ãƒ‡ãƒ«è¨“ç·´
                optimized_model.fit(X_scaled, y)
                trained_models[name] = optimized_model
                
                # ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³è©•ä¾¡
                cv_scores = cross_val_score(
                    optimized_model, X_scaled, y,
                    cv=5, scoring='neg_mean_squared_error'
                )
                model_scores[name] = -cv_scores.mean()
                
                self.logger.info(f"{name}: CV MSE = {model_scores[name]:.4f}")
                
                # ç‰¹å¾´é‡é‡è¦åº¦ã®å–å¾—ï¼ˆå¯èƒ½ãªå ´åˆï¼‰
                if hasattr(optimized_model, 'feature_importances_'):
                    self.feature_importance[name] = optimized_model.feature_importances_
                elif hasattr(optimized_model, 'coef_'):
                    self.feature_importance[name] = np.abs(optimized_model.coef_)
                
            except Exception as e:
                self.logger.error(f"{name}ãƒ¢ãƒ‡ãƒ«ã®è¨“ç·´ã§ã‚¨ãƒ©ãƒ¼: {e}")
                continue
        
        # ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«é‡ã¿ã®è¨ˆç®—ï¼ˆæ€§èƒ½ã®é€†æ•°ã§é‡ã¿ä»˜ã‘ï¼‰
        total_inverse_score = sum(1/score for score in model_scores.values())
        for name, score in model_scores.items():
            self.ensemble_weights[name] = (1/score) / total_inverse_score
        
        self.models = trained_models
        return trained_models
    
    def predict_with_ensemble(self, X: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:
        """ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«äºˆæ¸¬"""
        if not self.models:
            raise ValueError("ãƒ¢ãƒ‡ãƒ«ãŒè¨“ç·´ã•ã‚Œã¦ã„ã¾ã›ã‚“")
        
        # ãƒ‡ãƒ¼ã‚¿ã®å‰å‡¦ç†
        X_scaled = self.scalers['main'].transform(X)
        
        predictions = []
        weights = []
        
        for name, model in self.models.items():
            try:
                pred = model.predict(X_scaled)
                predictions.append(pred)
                weights.append(self.ensemble_weights.get(name, 1.0))
            except Exception as e:
                self.logger.error(f"{name}ãƒ¢ãƒ‡ãƒ«ã®äºˆæ¸¬ã§ã‚¨ãƒ©ãƒ¼: {e}")
                continue
        
        if not predictions:
            raise ValueError("äºˆæ¸¬å¯èƒ½ãªãƒ¢ãƒ‡ãƒ«ãŒã‚ã‚Šã¾ã›ã‚“")
        
        # é‡ã¿ä»˜ãå¹³å‡
        predictions = np.array(predictions)
        weights = np.array(weights)
        weights = weights / weights.sum()  # æ­£è¦åŒ–
        
        ensemble_pred = np.average(predictions, axis=0, weights=weights)
        
        # äºˆæ¸¬ã®ä¸ç¢ºå®Ÿæ€§ï¼ˆæ¨™æº–åå·®ï¼‰
        uncertainty = np.std(predictions, axis=0)
        
        return ensemble_pred, uncertainty
    
    def evaluate_model_performance(self, X_test: np.ndarray, y_test: np.ndarray) -> Dict[str, Any]:
        """ãƒ¢ãƒ‡ãƒ«æ€§èƒ½ã®è©•ä¾¡"""
        results = {}
        
        # ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«äºˆæ¸¬
        ensemble_pred, uncertainty = self.predict_with_ensemble(X_test)
        
        # è©•ä¾¡æŒ‡æ¨™ã®è¨ˆç®—
        mse = mean_squared_error(y_test, ensemble_pred)
        rmse = np.sqrt(mse)
        mae = mean_absolute_error(y_test, ensemble_pred)
        r2 = r2_score(y_test, ensemble_pred)
        
        results['ensemble'] = {
            'mse': mse,
            'rmse': rmse,
            'mae': mae,
            'r2': r2,
            'mean_uncertainty': np.mean(uncertainty)
        }
        
        # å€‹åˆ¥ãƒ¢ãƒ‡ãƒ«ã®è©•ä¾¡
        X_scaled = self.scalers['main'].transform(X_test)
        
        for name, model in self.models.items():
            try:
                pred = model.predict(X_scaled)
                
                results[name] = {
                    'mse': mean_squared_error(y_test, pred),
                    'rmse': np.sqrt(mean_squared_error(y_test, pred)),
                    'mae': mean_absolute_error(y_test, pred),
                    'r2': r2_score(y_test, pred)
                }
            except Exception as e:
                self.logger.error(f"{name}ãƒ¢ãƒ‡ãƒ«ã®è©•ä¾¡ã§ã‚¨ãƒ©ãƒ¼: {e}")
                continue
        
        return results
    
    def analyze_feature_importance(self, feature_names: List[str]) -> Dict[str, Any]:
        """ç‰¹å¾´é‡é‡è¦åº¦ã®åˆ†æ"""
        importance_analysis = {}
        
        # å„ãƒ¢ãƒ‡ãƒ«ã®ç‰¹å¾´é‡é‡è¦åº¦ã‚’çµ±åˆ
        if self.feature_importance:
            # é‡ã¿ä»˜ãå¹³å‡ã«ã‚ˆã‚‹çµ±åˆé‡è¦åº¦
            weighted_importance = np.zeros(len(feature_names))
            total_weight = 0
            
            for model_name, importance in self.feature_importance.items():
                if model_name in self.ensemble_weights:
                    weight = self.ensemble_weights[model_name]
                    weighted_importance += importance * weight
                    total_weight += weight
            
            if total_weight > 0:
                weighted_importance /= total_weight
            
            # ç‰¹å¾´é‡é‡è¦åº¦ã®ãƒ©ãƒ³ã‚­ãƒ³ã‚°
            importance_ranking = sorted(
                zip(feature_names, weighted_importance),
                key=lambda x: x[1],
                reverse=True
            )
            
            importance_analysis['ranking'] = importance_ranking
            importance_analysis['top_10'] = importance_ranking[:10]
            
            # é‡è¦åº¦ã®çµ±è¨ˆ
            importance_analysis['statistics'] = {
                'mean': np.mean(weighted_importance),
                'std': np.std(weighted_importance),
                'max': np.max(weighted_importance),
                'min': np.min(weighted_importance)
            }
        
        return importance_analysis
    
    def generate_improvement_recommendations(self, performance_results: Dict[str, Any], 
                                           feature_analysis: Dict[str, Any]) -> List[str]:
        """æ”¹å–„ææ¡ˆã®ç”Ÿæˆ"""
        recommendations = []
        
        # æ€§èƒ½ãƒ™ãƒ¼ã‚¹ã®ææ¡ˆ
        ensemble_r2 = performance_results.get('ensemble', {}).get('r2', 0)
        
        if ensemble_r2 < 0.7:
            recommendations.append("äºˆæ¸¬ç²¾åº¦ãŒä½ã„ãŸã‚ã€ã‚ˆã‚Šå¤šãã®ç‰¹å¾´é‡ã®è¿½åŠ ã‚’æ¤œè¨ã—ã¦ãã ã•ã„")
            recommendations.append("ãƒ‡ãƒ¼ã‚¿ã®å‰å‡¦ç†æ–¹æ³•ã‚’è¦‹ç›´ã—ã€å¤–ã‚Œå€¤ã®å‡¦ç†ã‚’å¼·åŒ–ã—ã¦ãã ã•ã„")
        
        if ensemble_r2 < 0.5:
            recommendations.append("ç¾åœ¨ã®ãƒ¢ãƒ‡ãƒ«ã§ã¯ååˆ†ãªäºˆæ¸¬ç²¾åº¦ãŒå¾—ã‚‰ã‚Œã¦ã„ã¾ã›ã‚“ã€‚æ·±å±¤å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã®å°å…¥ã‚’æ¤œè¨ã—ã¦ãã ã•ã„")
        
        # ä¸ç¢ºå®Ÿæ€§ãƒ™ãƒ¼ã‚¹ã®ææ¡ˆ
        mean_uncertainty = performance_results.get('ensemble', {}).get('mean_uncertainty', 0)
        
        if mean_uncertainty > 0.3:
            recommendations.append("äºˆæ¸¬ã®ä¸ç¢ºå®Ÿæ€§ãŒé«˜ã„ãŸã‚ã€ã‚ˆã‚Šå¤šæ§˜ãªãƒ¢ãƒ‡ãƒ«ã‚’ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ã«è¿½åŠ ã—ã¦ãã ã•ã„")
            recommendations.append("ãƒ‡ãƒ¼ã‚¿åé›†æœŸé–“ã‚’å»¶é•·ã—ã€ã‚ˆã‚Šå¤šãã®è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã‚’ç¢ºä¿ã—ã¦ãã ã•ã„")
        
        # ç‰¹å¾´é‡é‡è¦åº¦ãƒ™ãƒ¼ã‚¹ã®ææ¡ˆ
        if 'top_10' in feature_analysis:
            top_features = [name for name, _ in feature_analysis['top_10'][:3]]
            recommendations.append(f"æœ€é‡è¦ç‰¹å¾´é‡ï¼ˆ{', '.join(top_features)}ï¼‰ã«é–¢é€£ã™ã‚‹æ–°ã—ã„ç‰¹å¾´é‡ã®è¿½åŠ ã‚’æ¤œè¨ã—ã¦ãã ã•ã„")
        
        # ãƒ¢ãƒ‡ãƒ«å›ºæœ‰ã®ææ¡ˆ
        model_performances = {name: results.get('r2', 0) 
                            for name, results in performance_results.items() 
                            if name != 'ensemble'}
        
        if model_performances:
            best_model = max(model_performances, key=model_performances.get)
            worst_model = min(model_performances, key=model_performances.get)
            
            if model_performances[best_model] - model_performances[worst_model] > 0.2:
                recommendations.append(f"{best_model}ãŒæœ€ã‚‚å„ªç§€ãªæ€§èƒ½ã‚’ç¤ºã—ã¦ã„ã¾ã™ã€‚ã“ã®ãƒ¢ãƒ‡ãƒ«ã®é‡ã¿ã‚’å¢—åŠ ã•ã›ã‚‹ã“ã¨ã‚’æ¤œè¨ã—ã¦ãã ã•ã„")
                recommendations.append(f"{worst_model}ã®æ€§èƒ½ãŒä½ã„ãŸã‚ã€ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ã‹ã‚‰é™¤å¤–ã™ã‚‹ã“ã¨ã‚’æ¤œè¨ã—ã¦ãã ã•ã„")
        
        return recommendations
    
    def save_improvement_results(self, results: Dict[str, Any], output_path: str):
        """æ”¹å–„çµæœã®ä¿å­˜"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # çµæœã®ä¿å­˜
        results_file = f"{output_path}/accuracy_improvement_results_{timestamp}.json"
        with open(results_file, 'w', encoding='utf-8') as f:
            # NumPyé…åˆ—ã‚’ãƒªã‚¹ãƒˆã«å¤‰æ›
            serializable_results = self._make_serializable(results)
            json.dump(serializable_results, f, ensure_ascii=False, indent=2)
        
        # ãƒ¢ãƒ‡ãƒ«ã®ä¿å­˜
        models_file = f"{output_path}/improved_models_{timestamp}.pkl"
        with open(models_file, 'wb') as f:
            pickle.dump({
                'models': self.models,
                'scalers': self.scalers,
                'ensemble_weights': self.ensemble_weights,
                'feature_importance': self.feature_importance
            }, f)
        
        self.logger.info(f"æ”¹å–„çµæœã‚’ä¿å­˜ã—ã¾ã—ãŸ: {results_file}")
        self.logger.info(f"æ”¹å–„ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜ã—ã¾ã—ãŸ: {models_file}")
        
        return results_file, models_file
    
    def _make_serializable(self, obj):
        """ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’JSON serializable ã«å¤‰æ›"""
        if isinstance(obj, np.ndarray):
            return obj.tolist()
        elif isinstance(obj, np.integer):
            return int(obj)
        elif isinstance(obj, np.floating):
            return float(obj)
        elif isinstance(obj, dict):
            return {key: self._make_serializable(value) for key, value in obj.items()}
        elif isinstance(obj, list):
            return [self._make_serializable(item) for item in obj]
        else:
            return obj

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    config = {
        'data_path': 'data/processed/unified/all_developers.json',
        'output_path': 'outputs/accuracy_improvement',
        'test_size': 0.2,
        'random_state': 42
    }
    
    # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ä½œæˆ
    Path(config['output_path']).mkdir(parents=True, exist_ok=True)
    
    # ã‚·ã‚¹ãƒ†ãƒ ã®åˆæœŸåŒ–
    improver = AdvancedAccuracyImprover(config)
    
    print("ğŸš€ é«˜åº¦ãªäºˆæ¸¬ç²¾åº¦æ”¹å–„ã‚·ã‚¹ãƒ†ãƒ ã‚’é–‹å§‹ã—ã¾ã™...")
    print("=" * 60)
    
    return improver

if __name__ == "__main__":
    main()