"""
Nova ãƒ¬ãƒ“ãƒ¥ãƒ¼å—è«¾äºˆæ¸¬ - Enhanced IRL (importantsè¨­å®šæº–æ‹ )

importants/baseline_nova_6month_windows ã¨åŒã˜è¨­å®š:
- è¨“ç·´æœŸé–“: 2021-01-01 ~ 2023-01-01 (24ãƒ¶æœˆ)
- è©•ä¾¡æœŸé–“: 2023-01-01 ~ 2024-01-01 (4æœŸé–“: 0-3m, 3-6m, 6-9m, 9-12m)
- æœˆæ¬¡é›†ç´„: å„æœˆæœ«æ™‚ç‚¹ã§ã®ç‰¹å¾´é‡ + å°†æ¥çª“ã§ã®ãƒ¬ãƒ“ãƒ¥ãƒ¼å—è«¾
- ã‚¿ã‚¹ã‚¯: ãƒ¬ãƒ“ãƒ¥ãƒ¼å—è«¾äºˆæ¸¬ï¼ˆå°†æ¥æœŸé–“ã«å°‘ãªãã¨ã‚‚1å›å—è«¾ã™ã‚‹ã‹ï¼‰
"""

import sys
from pathlib import Path

project_root = Path(__file__).parent.parent.parent.parent
sys.path.insert(0, str(project_root))

import warnings
from datetime import datetime, timedelta

import numpy as np
import pandas as pd
import torch
import torch.nn as nn
from sklearn.metrics import roc_auc_score
from torch.utils.data import DataLoader, Dataset

warnings.filterwarnings('ignore')

# Enhanced IRLãƒ¢ãƒ‡ãƒ«ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
enhanced_irl_path = Path(__file__).parent.parent.parent / "enhanced_irl"
sys.path.insert(0, str(enhanced_irl_path))
from models.attention_irl import AttentionIRLNetwork
from models.temporal_feature_extractor import TemporalFeatureExtractor


class ReviewerDataset(Dataset):
    """ãƒ¬ãƒ“ãƒ¥ãƒ¯ãƒ¼ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ"""
    
    def __init__(self, state_features: np.ndarray, temporal_features: np.ndarray, labels: np.ndarray):
        self.state_features = torch.FloatTensor(state_features)
        self.temporal_features = torch.FloatTensor(temporal_features)
        self.labels = torch.FloatTensor(labels)
    
    def __len__(self):
        return len(self.labels)
    
    def __getitem__(self, idx):
        return {
            'state': self.state_features[idx],
            'temporal': self.temporal_features[idx],
            'label': self.labels[idx]
        }


def calculate_state_features(df: pd.DataFrame, reviewer: str, context_date: datetime) -> np.ndarray:
    """IRLçŠ¶æ…‹ç‰¹å¾´é‡ï¼ˆ10æ¬¡å…ƒï¼‰"""
    reviewer_df = df[df['reviewer_email'] == reviewer].copy()
    
    if len(reviewer_df) == 0:
        return np.zeros(10)
    
    reviewer_df['timestamp'] = pd.to_datetime(reviewer_df['request_time'])
    reviewer_df = reviewer_df[reviewer_df['timestamp'] < context_date]
    
    if len(reviewer_df) == 0:
        return np.zeros(10)
    
    # çµŒé¨“æ—¥æ•°
    first_seen = reviewer_df['timestamp'].min()
    experience_days = (context_date - first_seen).days / 730.0
    
    # ç·å¤‰æ›´æ•°ãƒ»ãƒ¬ãƒ“ãƒ¥ãƒ¼æ•°
    total_changes = len(reviewer_df) / 500.0
    total_reviews = len(reviewer_df) / 500.0
    
    # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæ•°
    project_count = 1.0
    
    # æœ€è¿‘ã®æ´»å‹•é »åº¦
    recent_cutoff = context_date - timedelta(days=30)
    recent_df = reviewer_df[reviewer_df['timestamp'] >= recent_cutoff]
    recent_activity_frequency = len(recent_df) / 30.0
    
    # å¹³å‡æ´»å‹•é–“éš”
    if len(reviewer_df) > 1:
        sorted_times = reviewer_df['timestamp'].sort_values()
        time_diffs = sorted_times.diff().dt.total_seconds().dropna()
        avg_activity_gap = time_diffs.mean() / 86400.0 if len(time_diffs) > 0 else 1.0
        avg_activity_gap = min(avg_activity_gap, 60.0) / 60.0
    else:
        avg_activity_gap = 0.5
    
    # æ´»å‹•ãƒˆãƒ¬ãƒ³ãƒ‰
    if len(reviewer_df) > 1:
        midpoint = first_seen + (context_date - first_seen) / 2
        recent_half = reviewer_df[reviewer_df['timestamp'] >= midpoint]
        past_half = reviewer_df[reviewer_df['timestamp'] < midpoint]
        
        if len(past_half) > 0:
            activity_trend = len(recent_half) / len(past_half)
            if activity_trend > 1.5:
                activity_trend = 1.0
            elif activity_trend > 0.8:
                activity_trend = 0.5
            else:
                activity_trend = 0.0
        else:
            activity_trend = 0.5
    else:
        activity_trend = 0.5
    
    # æœ€çµ‚æ´»å‹•ã‹ã‚‰ã®çµŒéæ—¥æ•°
    last_activity = reviewer_df['timestamp'].max()
    days_since_last = (context_date - last_activity).days / 365.0
    
    # ãƒ¬ãƒ“ãƒ¥ãƒ¼å—ã‘å…¥ã‚Œç‡
    acceptance_rate = reviewer_df['label'].mean() if 'label' in reviewer_df.columns else 0.0
    
    # æœ€è¿‘30æ—¥ã®å—ã‘å…¥ã‚Œç‡
    if len(recent_df) > 0:
        recent_acceptance = recent_df['label'].mean() if 'label' in recent_df.columns else 0.0
    else:
        recent_acceptance = 0.0
    
    return np.array([
        experience_days,
        total_changes,
        total_reviews,
        project_count,
        recent_activity_frequency,
        avg_activity_gap,
        activity_trend,
        days_since_last,
        acceptance_rate,
        recent_acceptance
    ])


def prepare_monthly_trajectories(df: pd.DataFrame, train_start: datetime, train_end: datetime,
                                  future_window_months: int = 6):
    """
    æœˆæ¬¡é›†ç´„è»Œè·¡ã‚’ç”Ÿæˆï¼ˆãƒ‡ãƒ¼ã‚¿ãƒªãƒ¼ã‚¯ãªã—ç‰ˆï¼‰
    
    é‡è¦ï¼šè¨“ç·´æœŸé–“å†…ã§å®Œçµã•ã›ã‚‹ãŸã‚ã€è¨“ç·´æœŸé–“ã‚’åˆ†å‰²ï¼š
    - ç‰¹å¾´é‡è¨ˆç®—æœŸé–“: train_start ~ (train_end - future_window_months)
    - ãƒ©ãƒ™ãƒ«è¨ˆç®—æœŸé–“: (train_end - future_window_months) ~ train_end
    
    å„æœˆæœ«ã‚’åŸºæº–ç‚¹ã¨ã—ã¦:
    - ç‰¹å¾´é‡: train_start ~ æœˆæœ«ã¾ã§ã®ç´¯ç©æ´»å‹•
    - ãƒ©ãƒ™ãƒ«: æœˆæœ«ã‹ã‚‰ future_window_months å¾Œã¾ã§ã®å—è«¾ï¼ˆtrain_endå†…ï¼‰
    """
    trajectories = []
    
    # ç‰¹å¾´é‡è¨ˆç®—æœŸé–“ã®çµ‚äº†ï¼ˆãƒ©ãƒ™ãƒ«æœŸé–“ã®é–‹å§‹ï¼‰
    feature_end = train_end - pd.DateOffset(months=future_window_months)
    
    # æœˆæœ«ã‚’åˆ—æŒ™ï¼ˆç‰¹å¾´é‡è¨ˆç®—æœŸé–“å†…ã®ã¿ï¼‰
    current = train_start
    while current < feature_end:
        month_end = current + pd.DateOffset(months=1)
        if month_end > feature_end:
            month_end = feature_end
        
        # ã“ã®æœˆæœ«ã¾ã§ã«æ´»å‹•ã—ãŸäºº
        history_df = df[(df['request_time'] >= train_start) & (df['request_time'] < month_end)]
        active_reviewers = history_df['reviewer_email'].unique()
        
        # å°†æ¥çª“ï¼ˆè¨“ç·´æœŸé–“å†…ã«åã¾ã‚‹ï¼‰
        future_start = month_end
        future_end = month_end + pd.DateOffset(months=future_window_months)
        # train_endã‚’è¶…ãˆãªã„ã‚ˆã†ã«åˆ¶é™
        if future_end > train_end:
            future_end = train_end
        
        future_df = df[(df['request_time'] >= future_start) & (df['request_time'] < future_end)]
        
        # å°†æ¥çª“ã§å—è«¾ã—ãŸäºº
        accepted = future_df[future_df['label'] == 1]['reviewer_email'].unique()
        accepted_set = set(accepted)
        
        for reviewer in active_reviewers:
            label = 1 if reviewer in accepted_set else 0
            trajectories.append({
                'reviewer': reviewer,
                'month_end': month_end,
                'label': label
            })
        
        current = month_end
    
    return pd.DataFrame(trajectories)


def prepare_eval_data(df: pd.DataFrame, eval_start: datetime, eval_months: int,
                      train_start: datetime, future_window_months: int = 6):
    """
    è©•ä¾¡ãƒ‡ãƒ¼ã‚¿æº–å‚™ï¼ˆãƒ‡ãƒ¼ã‚¿ãƒªãƒ¼ã‚¯ãªã—ç‰ˆï¼‰
    
    è©•ä¾¡æœŸé–“ã‚’åˆ†å‰²ï¼š
    - ç‰¹å¾´é‡è¨ˆç®—æœŸé–“: eval_start ~ (eval_start + eval_months - future_window_months)
    - ãƒ©ãƒ™ãƒ«è¨ˆç®—æœŸé–“: (eval_start + eval_months - future_window_months) ~ (eval_start + eval_months)
    """
    eval_end = eval_start + pd.DateOffset(months=eval_months)
    
    # ç‰¹å¾´é‡è¨ˆç®—æœŸé–“ã®çµ‚äº†ï¼ˆãƒ©ãƒ™ãƒ«æœŸé–“ã®é–‹å§‹ï¼‰
    feature_end = eval_end - pd.DateOffset(months=future_window_months)
    
    # è©•ä¾¡æœŸé–“ã®ç‰¹å¾´é‡è¨ˆç®—æœŸé–“ã«æ´»å‹•ã—ãŸäºº
    eval_df = df[(df['request_time'] >= eval_start) & (df['request_time'] < feature_end)]
    eval_reviewers = eval_df['reviewer_email'].unique()
    
    # å°†æ¥çª“ã§ãƒ¬ãƒ“ãƒ¥ãƒ¼å—è«¾ï¼ˆeval_endå†…ã«åã¾ã‚‹ï¼‰
    future_start = feature_end
    future_end = eval_end
    future_df = df[(df['request_time'] >= future_start) & (df['request_time'] < future_end)]
    accepted = future_df[future_df['label'] == 1]['reviewer_email'].unique()
    accepted_set = set(accepted)
    
    eval_samples = []
    for reviewer in eval_reviewers:
        label = 1 if reviewer in accepted_set else 0
        eval_samples.append({
            'reviewer': reviewer,
            'cutoff_date': feature_end,  # ç‰¹å¾´é‡è¨ˆç®—ã®çµ‚äº†æ™‚ç‚¹
            'label': label
        })
    
    return pd.DataFrame(eval_samples)


def main():
    print("=" * 80)
    print("Nova ãƒ¬ãƒ“ãƒ¥ãƒ¼å—è«¾äºˆæ¸¬ - Enhanced IRL (importantsè¨­å®šæº–æ‹ )")
    print("=" * 80)
    
    # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    data_path = project_root / "data" / "review_requests_openstack_multi_5y_detail.csv"
    print(f"\nğŸ“‚ ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿: {data_path}")
    
    df = pd.read_csv(data_path)
    df['request_time'] = pd.to_datetime(df['request_time'])
    
    # novaå˜ä¸€ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ
    df = df[df['project'] == 'openstack/nova'].copy()
    print(f"âœ… Nova: {len(df)} ãƒ¬ã‚³ãƒ¼ãƒ‰, {df['reviewer_email'].nunique()} ãƒ¬ãƒ“ãƒ¥ãƒ¯ãƒ¼")
    
    # importantsè¨­å®š
    train_start = datetime(2021, 1, 1)
    train_end = datetime(2023, 1, 1)
    eval_start = datetime(2023, 1, 1)
    future_window = 6  # 0-6ãƒ¶æœˆ
    
    print(f"\nğŸ“… å®Ÿé¨“è¨­å®šï¼ˆimportantsæº–æ‹ ï¼‰:")
    print(f"  è¨“ç·´æœŸé–“: {train_start.date()} ~ {train_end.date()} (24ãƒ¶æœˆ)")
    print(f"  è©•ä¾¡é–‹å§‹: {eval_start.date()}")
    print(f"  å°†æ¥çª“: {future_window}ãƒ¶æœˆ")
    print(f"  Seed: 42")
    
    # è¨“ç·´ãƒ‡ãƒ¼ã‚¿: æœˆæ¬¡é›†ç´„è»Œè·¡
    print("\n" + "=" * 80)
    print("è¨“ç·´ãƒ‡ãƒ¼ã‚¿æº–å‚™ï¼ˆæœˆæ¬¡é›†ç´„ï¼‰")
    print("=" * 80)
    
    train_trajectories = prepare_monthly_trajectories(
        df, train_start, train_end, future_window_months=future_window
    )
    print(f"âœ… è¨“ç·´è»Œè·¡: {len(train_trajectories)} è»Œè·¡")
    print(f"   ç¶™ç¶šç‡: {train_trajectories['label'].mean():.3f}")
    
    # è©•ä¾¡ãƒ‡ãƒ¼ã‚¿: 0-3mæœŸé–“
    print("\n" + "=" * 80)
    print("è©•ä¾¡ãƒ‡ãƒ¼ã‚¿æº–å‚™ï¼ˆ0-3mæœŸé–“ï¼‰")
    print("=" * 80)
    
    eval_df = prepare_eval_data(
        df, eval_start, eval_months=3,
        train_start=train_start, future_window_months=future_window
    )
    print(f"âœ… è©•ä¾¡ã‚µãƒ³ãƒ—ãƒ«: {len(eval_df)} äºº")
    print(f"   ç¶™ç¶šç‡: {eval_df['label'].mean():.3f}")
    
    # ç‰¹å¾´é‡è¨ˆç®—
    print("\n" + "=" * 80)
    print("ç‰¹å¾´é‡è¨ˆç®—")
    print("=" * 80)
    
    print("Trainç‰¹å¾´é‡...")
    X_train_state = []
    y_train = []
    for _, row in train_trajectories.iterrows():
        features = calculate_state_features(df, row['reviewer'], row['month_end'])
        X_train_state.append(features)
        y_train.append(row['label'])
    
    X_train_state = np.array(X_train_state)
    y_train = np.array(y_train)
    
    # æ™‚ç³»åˆ—ç‰¹å¾´é‡: çŠ¶æ…‹ç‰¹å¾´é‡ã‚’æ™‚ç³»åˆ—æ¬¡å…ƒã§è¤‡è£½ (B, T, D) -> (B, T, 10)
    # Tã¯ä»»æ„ã®ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·ï¼ˆã“ã“ã§ã¯1ã‚’ä½¿ç”¨ï¼‰
    X_train_temporal = X_train_state[:, np.newaxis, :]  # (B, 1, 10)
    
    print(f"âœ… Train: state={X_train_state.shape}, temporal={X_train_temporal.shape}")
    
    print("Evalç‰¹å¾´é‡...")
    X_eval_state = []
    y_eval = []
    for _, row in eval_df.iterrows():
        features = calculate_state_features(df, row['reviewer'], row['cutoff_date'])
        X_eval_state.append(features)
        y_eval.append(row['label'])
    
    X_eval_state = np.array(X_eval_state)
    y_eval = np.array(y_eval)
    X_eval_temporal = X_eval_state[:, np.newaxis, :]  # (B, 1, 10)
    
    print(f"âœ… Eval: state={X_eval_state.shape}, temporal={X_eval_temporal.shape}")
    
    # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆ
    train_dataset = ReviewerDataset(X_train_state, X_train_temporal, y_train)
    eval_dataset = ReviewerDataset(X_eval_state, X_eval_temporal, y_eval)
    
    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
    eval_loader = DataLoader(eval_dataset, batch_size=32, shuffle=False)
    
    # ãƒ¢ãƒ‡ãƒ«æ§‹ç¯‰
    print("\n" + "=" * 80)
    print("Enhanced IRL ãƒ¢ãƒ‡ãƒ«æ§‹ç¯‰")
    print("=" * 80)
    
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model = AttentionIRLNetwork(
        state_dim=10,
        temporal_dim=10,  # ç°¡æ˜“æ™‚ç³»åˆ—ï¼ˆ10æ¬¡å…ƒï¼‰
        hidden_dim=128,
        num_layers=2,
        dropout=0.3,
        use_temporal=True
    ).to(device)
    
    print(f"âœ… ãƒ‡ãƒã‚¤ã‚¹: {device}")
    print(f"âœ… ãƒ¢ãƒ‡ãƒ«: AttentionIRL (state=10, temporal=10, hidden=128, layers=2)")
    
    # å­¦ç¿’
    print("\n" + "=" * 80)
    print("å­¦ç¿’é–‹å§‹")
    print("=" * 80)
    
    criterion = nn.BCELoss()
    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
    
    best_auc = 0.0
    epochs = 50
    
    for epoch in range(epochs):
        model.train()
        train_loss = 0.0
        
        for batch in train_loader:
            state = batch['state'].to(device).unsqueeze(1)  # (B, 1, 10)
            temporal = batch['temporal'].to(device)  # (B, 1, 10)
            labels = batch['label'].to(device)
            
            optimizer.zero_grad()
            outputs, _ = model(state, temporal)
            outputs = outputs.squeeze()
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()
            
            train_loss += loss.item()
        
        # è©•ä¾¡
        model.eval()
        eval_probs = []
        eval_labels = []
        
        with torch.no_grad():
            for batch in eval_loader:
                state = batch['state'].to(device).unsqueeze(1)  # (B, 1, 10)
                temporal = batch['temporal'].to(device)  # (B, 1, 10)
                labels = batch['label']
                
                outputs, _ = model(state, temporal)
                outputs = outputs.squeeze()
                eval_probs.extend(outputs.cpu().numpy())
                eval_labels.extend(labels.numpy())
        
        eval_auc = roc_auc_score(eval_labels, eval_probs)
        
        if eval_auc > best_auc:
            best_auc = eval_auc
        
        if (epoch + 1) % 10 == 0:
            print(f"Epoch {epoch+1}/{epochs}, Loss: {train_loss/len(train_loader):.4f}, "
                  f"AUC: {eval_auc:.4f}, Best: {best_auc:.4f}")
    
    # çµæœä¿å­˜
    print("\n" + "=" * 80)
    print("çµæœä¿å­˜")
    print("=" * 80)
    
    result_dir = Path(__file__).parent.parent / "results"
    result_dir.mkdir(exist_ok=True)
    
    result = {
        'model': 'Enhanced_IRL',
        'project': 'nova',
        'seed': 42,
        'train_start': train_start.strftime('%Y-%m-%d'),
        'train_end': train_end.strftime('%Y-%m-%d'),
        'eval_start': eval_start.strftime('%Y-%m-%d'),
        'eval_months': 3,
        'future_window_months': future_window,
        'train_trajectories': len(train_trajectories),
        'eval_samples': len(eval_df),
        'train_continuation_rate': float(train_trajectories['label'].mean()),
        'eval_continuation_rate': float(eval_df['label'].mean()),
        'best_auc': float(best_auc),
        'final_auc': float(eval_auc)
    }
    
    result_file = result_dir / "enhanced_irl_result.json"
    import json
    with open(result_file, 'w') as f:
        json.dump(result, f, indent=2)
    
    print(f"âœ… çµæœä¿å­˜: {result_file}")
    
    # ã‚µãƒãƒªãƒ¼
    print("\n" + "=" * 80)
    print("å®Ÿé¨“ã‚µãƒãƒªãƒ¼")
    print("=" * 80)
    print(f"è¨“ç·´è»Œè·¡: {len(train_trajectories)}")
    print(f"è©•ä¾¡ã‚µãƒ³ãƒ—ãƒ«: {len(eval_df)}")
    print(f"Best AUC: {best_auc:.4f}")
    print(f"")
    print(f"æ¯”è¼ƒï¼ˆimportantsï¼‰:")
    print(f"  Attentionãªã—IRL: 0.801")
    print(f"  Enhanced IRL (Attention): {best_auc:.4f}")
    print("=" * 80)


if __name__ == "__main__":
    main()
