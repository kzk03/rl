#!/usr/bin/env python3
"""
æœ¬æ ¼çš„PPOã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆè¨“ç·´ã‚¹ã‚¯ãƒªãƒ—ãƒˆï¼ˆæ™‚é–“ã‚’ã‹ã‘ãŸæ·±ã„å­¦ç¿’ï¼‰
"""

import sys
import os
sys.path.append(os.path.join(os.path.dirname(__file__), '..', '..', 'src'))

from gerrit_retention.rl_environment.review_env import ReviewAcceptanceEnvironment
from gerrit_retention.rl_environment.ppo_agent import PPOAgent, PPOConfig
import torch
import torch.nn as nn
import numpy as np
from tqdm import tqdm
import time
import json
from datetime import datetime

def main():
    """æœ¬æ ¼çš„PPOè¨“ç·´ï¼ˆæ™‚é–“ã‚’ã‹ã‘ãŸæ·±ã„å­¦ç¿’ï¼‰"""
    print('=== ğŸ“ æœ¬æ ¼çš„gerrit-retentionå¼·åŒ–å­¦ç¿’ã‚·ã‚¹ãƒ†ãƒ å®Ÿè¡Œ ===')
    print('ï¼ˆæ™‚é–“ã‚’ã‹ã‘ãŸæ·±ã„å­¦ç¿’ç‰ˆï¼‰')
    
    # æœ¬æ ¼çš„è¨­å®šï¼ˆæ™‚é–“ã‚’ã‹ã‘ã‚‹ï¼‰
    config = PPOConfig(
        hidden_size=512,        # å¤§è¦æ¨¡ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯
        num_layers=4,           # æ·±ã„ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯
        learning_rate=5e-5,     # ä½å­¦ç¿’ç‡ï¼ˆæ…é‡ãªå­¦ç¿’ï¼‰
        gamma=0.995,            # é•·æœŸå ±é…¬é‡è¦–
        gae_lambda=0.98,        # é«˜ç²¾åº¦GAE
        clip_epsilon=0.1,       # ä¿å®ˆçš„ã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°
        value_loss_coef=1.0,    # ä¾¡å€¤å­¦ç¿’é‡è¦–
        entropy_coef=0.02,      # æ¢ç´¢ä¿ƒé€²
        batch_size=256,         # å¤§è¦æ¨¡ãƒãƒƒãƒ
        mini_batch_size=32,     # ç´°ã‹ã„æ›´æ–°
        ppo_epochs=20,          # ååˆ†ãªæ›´æ–°
        buffer_size=4096        # å¤§å®¹é‡ãƒãƒƒãƒ•ã‚¡
    )
    
    # è¤‡é›‘ãªç’°å¢ƒè¨­å®š
    env_config = {
        'max_episode_length': 500,  # é•·ã„ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰
        'max_queue_size': 30,       # å¤§ããªã‚­ãƒ¥ãƒ¼
        'stress_threshold': 0.9     # å³ã—ã„é–¾å€¤
    }
    
    print(f'ğŸ“Š æœ¬æ ¼çš„è¨“ç·´è¨­å®š:')
    print(f'  éš ã‚Œå±¤ã‚µã‚¤ã‚º: {config.hidden_size}')
    print(f'  ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯å±¤æ•°: {config.num_layers}')
    print(f'  å­¦ç¿’ç‡: {config.learning_rate}')
    print(f'  ãƒãƒƒãƒ•ã‚¡ã‚µã‚¤ã‚º: {config.buffer_size}')
    print(f'  ãƒãƒƒãƒã‚µã‚¤ã‚º: {config.batch_size}')
    print(f'  PPOã‚¨ãƒãƒƒã‚¯: {config.ppo_epochs}')
    print(f'  æœ€å¤§ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰é•·: {env_config[\"max_episode_length\"]}')
    
    # ç’°å¢ƒã¨ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆåˆæœŸåŒ–
    env = ReviewAcceptanceEnvironment(env_config)
    obs_dim = env.observation_space.shape[0]
    action_dim = env.action_space.n
    
    agent = PPOAgent(obs_dim, action_dim, config)
    
    # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°è¨ˆç®—
    total_params = sum(p.numel() for p in agent.policy_net.parameters()) + sum(p.numel() for p in agent.value_net.parameters())
    
    print(f'\\nğŸ—ï¸ ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–å®Œäº†:')
    print(f'  è¦³æ¸¬æ¬¡å…ƒ: {obs_dim}')
    print(f'  è¡Œå‹•æ¬¡å…ƒ: {action_dim}')
    print(f'  ç·ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: {total_params:,}')
    print(f'  ãƒ‡ãƒã‚¤ã‚¹: {agent.device}')
    
    # è¨“ç·´çµ±è¨ˆ
    episode_rewards = []
    episode_lengths = []
    update_count = 0
    action_counts = {'reject': 0, 'accept': 0, 'wait': 0}
    action_names = ['reject', 'accept', 'wait']
    
    print(f'\\n=== ğŸ¯ æœ¬æ ¼çš„PPOè¨“ç·´é–‹å§‹ï¼ˆæ·±ã„å­¦ç¿’ç‰ˆï¼‰ ===')
    start_time = time.time()
    best_avg_reward = float('-inf')
    
    total_episodes = 2000  # å¤§é‡ã®ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰
    
    # å­¦ç¿’ç‡ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼
    scheduler = torch.optim.lr_scheduler.StepLR(
        agent.policy_optimizer, step_size=500, gamma=0.8
    )
    
    # æ¢ç´¢ç‡ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼
    initial_exploration = 0.3
    final_exploration = 0.05
    exploration_decay = (initial_exploration - final_exploration) / total_episodes
    
    try:
        for episode in tqdm(range(total_episodes), desc='æœ¬æ ¼çš„PPOè¨“ç·´'):
            # æ¢ç´¢ç‡èª¿æ•´
            current_exploration = max(
                final_exploration, 
                initial_exploration - episode * exploration_decay
            )
            
            obs, _ = env.reset()
            episode_reward = 0.0
            episode_length = 0
            
            # é•·ã„ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰å®Ÿè¡Œ
            for step in range(env_config['max_episode_length']):
                try:
                    # æ¢ç´¢çš„è¡Œå‹•é¸æŠ
                    if np.random.random() < current_exploration:
                        action = np.random.randint(action_dim)
                        log_prob = np.log(1.0 / action_dim)
                        value = 0.0
                    else:
                        action, log_prob, value = agent.select_action(obs, training=True)
                    
                    next_obs, reward, terminated, truncated, info = env.step(action)
                    
                    # çµ±è¨ˆè¨˜éŒ²
                    action_counts[action_names[action]] += 1
                    
                    # çµŒé¨“ã‚’ä¿å­˜
                    agent.store_experience(obs, action, reward, value, log_prob, terminated or truncated)
                    
                    episode_reward += reward
                    episode_length += 1
                    
                    obs = next_obs
                    
                    if terminated or truncated:
                        break
                        
                except Exception as e:
                    print(f'ã‚¹ãƒ†ãƒƒãƒ—ã‚¨ãƒ©ãƒ¼ï¼ˆã‚¹ã‚­ãƒƒãƒ—ï¼‰: {e}')
                    break
            
            # PPOæ›´æ–°ï¼ˆå¤§å®¹é‡ãƒãƒƒãƒ•ã‚¡ãŒæºœã¾ã£ãŸã‚‰ï¼‰
            if agent.buffer.size >= config.buffer_size:
                try:
                    # è¤‡æ•°å›æ›´æ–°
                    for _ in range(3):  # è¿½åŠ æ›´æ–°
                        losses = agent.update()
                        if losses:
                            update_count += 1
                    
                    # å­¦ç¿’ç‡èª¿æ•´
                    scheduler.step()
                    
                except Exception as e:
                    print(f'æ›´æ–°ã‚¨ãƒ©ãƒ¼ï¼ˆã‚¹ã‚­ãƒƒãƒ—ï¼‰: {e}')
            
            episode_rewards.append(episode_reward)
            episode_lengths.append(episode_length)
            
            # å®šæœŸçš„ãªè©•ä¾¡
            if (episode + 1) % 100 == 0:
                avg_reward = np.mean(episode_rewards[-100:])
                avg_length = np.mean(episode_lengths[-100:])
                
                print(f'\\nğŸ“Š ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ {episode + 1:,}/{total_episodes:,}:')
                print(f'  å¹³å‡å ±é…¬: {avg_reward:.3f}')
                print(f'  å¹³å‡é•·: {avg_length:.1f}')
                print(f'  æ›´æ–°å›æ•°: {update_count:,}')
                print(f'  æ¢ç´¢ç‡: {current_exploration:.3f}')
                print(f'  å­¦ç¿’ç‡: {scheduler.get_last_lr()[0]:.6f}')
                
                # è¡Œå‹•åˆ†å¸ƒ
                total_actions = sum(action_counts.values())
                if total_actions > 0:
                    reject_pct = action_counts['reject'] / total_actions * 100
                    accept_pct = action_counts['accept'] / total_actions * 100
                    wait_pct = action_counts['wait'] / total_actions * 100
                    print(f'  è¡Œå‹•åˆ†å¸ƒ: Reject {reject_pct:.1f}%, Accept {accept_pct:.1f}%, Wait {wait_pct:.1f}%')
                
                if avg_reward > best_avg_reward:
                    best_avg_reward = avg_reward
                    print(f'  ğŸ† æ–°è¨˜éŒ²! ãƒ™ã‚¹ãƒˆå¹³å‡å ±é…¬: {best_avg_reward:.3f}')
                
                # ä¸­é–“ä¿å­˜
                if (episode + 1) % 500 == 0:
                    torch.save(agent.state_dict(), f'models/intensive_ppo_checkpoint_{episode+1}.pth')
                    print(f'  ğŸ’¾ ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä¿å­˜: episode_{episode+1}')
    
    except KeyboardInterrupt:
        print('\\nâš ï¸ è¨“ç·´ãŒä¸­æ–­ã•ã‚Œã¾ã—ãŸ')
    except Exception as e:
        print(f'\\nâŒ äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼: {e}')
    
    # æœ€çµ‚çµ±è¨ˆ
    training_time = time.time() - start_time
    completed_episodes = len(episode_rewards)
    
    print(f'\\n=== âœ… æœ¬æ ¼çš„PPOè¨“ç·´å®Œäº† ===')
    print(f'å®Œäº†ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰æ•°: {completed_episodes:,}/{total_episodes:,}')
    print(f'ç·è¨“ç·´æ™‚é–“: {training_time:.2f}ç§’ ({training_time/60:.1f}åˆ†)')
    print(f'ç·ã‚¹ãƒ†ãƒƒãƒ—æ•°: {sum(episode_lengths):,}')
    print(f'ç·æ›´æ–°å›æ•°: {update_count:,}')
    print(f'ç·ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: {total_params:,}')
    
    if episode_rewards:
        print(f'å¹³å‡ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰å ±é…¬: {np.mean(episode_rewards):.3f} Â± {np.std(episode_rewards):.3f}')
        print(f'ãƒ™ã‚¹ãƒˆå¹³å‡å ±é…¬: {best_avg_reward:.3f}')
        print(f'å¹³å‡ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰é•·: {np.mean(episode_lengths):.1f}')
        
        # å­¦ç¿’åŠ¹æœåˆ†æ
        if len(episode_rewards) >= 200:
            early_avg = np.mean(episode_rewards[:100])
            late_avg = np.mean(episode_rewards[-100:])
            improvement = late_avg - early_avg
            improvement_pct = improvement/abs(early_avg)*100 if early_avg != 0 else 0
            print(f'\\nğŸ“ˆ å­¦ç¿’åŠ¹æœ:')
            print(f'åˆæœŸ100ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰å¹³å‡: {early_avg:.3f}')
            print(f'æœ€çµ‚100ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰å¹³å‡: {late_avg:.3f}')
            print(f'æ”¹å–„åº¦: {improvement:.3f} ({improvement_pct:.1f}%)')
    
    # æœ€çµ‚ãƒ¢ãƒ‡ãƒ«ä¿å­˜
    torch.save(agent.state_dict(), 'models/intensive_ppo_final.pth')
    
    # çµæœä¿å­˜
    results = {
        'timestamp': datetime.now().isoformat(),
        'completed_episodes': completed_episodes,
        'total_steps': sum(episode_lengths),
        'total_updates': update_count,
        'total_params': int(total_params),
        'training_time': training_time,
        'mean_reward': float(np.mean(episode_rewards)) if episode_rewards else 0.0,
        'best_reward': float(best_avg_reward) if best_avg_reward != float('-inf') else 0.0,
        'action_distribution': {k: int(v) for k, v in action_counts.items()},
        'status': 'intensive_training_completed'
    }
    
    os.makedirs('outputs', exist_ok=True)
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    with open(f'outputs/intensive_rl_results_{timestamp}.json', 'w') as f:
        json.dump(results, f, indent=2)
    
    print(f'\\nğŸ’¾ çµæœä¿å­˜: outputs/intensive_rl_results_{timestamp}.json')
    print(f'\\nâœ… æœ¬æ ¼çš„gerrit-retentionå¼·åŒ–å­¦ç¿’ã‚·ã‚¹ãƒ†ãƒ å®Ÿè¡Œå®Œäº†!')
    print(f'æ·±ã„å­¦ç¿’ã§{completed_episodes:,}ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰å®Œäº†ã—ã¾ã—ãŸï¼')
    
    return 0


if __name__ == "__main__":
    sys.exit(main())