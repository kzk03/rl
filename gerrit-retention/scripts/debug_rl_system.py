#!/usr/bin/env python3
"""
å¼·åŒ–å­¦ç¿’ã‚·ã‚¹ãƒ†ãƒ ã®ãƒ‡ãƒãƒƒã‚°
ãƒ¢ãƒ‡ãƒ«ã®å‹•ä½œã‚’è©³ç´°ã«åˆ†æ
"""

import json
import sys
from pathlib import Path

import numpy as np
import torch

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ‘ã‚¹ã‚’è¿½åŠ 
sys.path.insert(0, str(Path(__file__).parent.parent / 'src'))
sys.path.append('scripts')

from advanced_rl_system import AdvancedActorCritic
from gerrit_retention.utils.logger import get_logger
from rl_task_optimizer import RLTaskOptimizer

logger = get_logger(__name__)

def debug_model_predictions():
    """ãƒ¢ãƒ‡ãƒ«äºˆæ¸¬ã®ãƒ‡ãƒãƒƒã‚°"""
    logger.info("=== å¼·åŒ–å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ãƒ‡ãƒãƒƒã‚° ===")
    
    # ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿
    with open('data/processed/unified/all_developers.json', 'r') as f:
        developers_data = json.load(f)
    
    with open('data/processed/unified/all_reviews.json', 'r') as f:
        reviews_data = json.load(f)
    
    # äººé–“ã®é–‹ç™ºè€…ã‚’æŠ½å‡º
    human_developers = []
    for dev in developers_data:
        name_lower = dev['name'].lower()
        email_lower = dev['developer_id'].lower()
        
        is_bot = any(keyword in name_lower for keyword in ['bot', 'robot', 'lint', 'presubmit', 'treehugger'])
        is_bot = is_bot or any(keyword in email_lower for keyword in ['bot', 'robot', 'system.gserviceaccount', 'presubmit'])
        
        if not is_bot and (dev['changes_authored'] > 0 or dev['changes_reviewed'] > 5):
            human_developers.append(dev)
    
    # ä¸Šä½é–‹ç™ºè€…ã‚’é¸æŠ
    top_developers = sorted(
        human_developers, 
        key=lambda x: x['changes_authored'] + x['changes_reviewed'], 
        reverse=True
    )[:3]
    
    # ã‚¿ã‚¹ã‚¯ã‚’é¸æŠ
    tasks = reviews_data[:3]
    
    print(f"\\nğŸ” ãƒ‡ãƒãƒƒã‚°å¯¾è±¡:")
    print(f"   é–‹ç™ºè€…: {len(top_developers)}å")
    print(f"   ã‚¿ã‚¹ã‚¯: {len(tasks)}ä»¶")
    
    # æœ€é©åŒ–å™¨ã®åˆæœŸåŒ–
    optimizer = RLTaskOptimizer()
    
    print(f"\\nğŸ“Š è©³ç´°ãªäºˆæ¸¬åˆ†æ:")
    
    for i, task in enumerate(tasks):
        print(f"\\n--- ã‚¿ã‚¹ã‚¯ {i+1}: {task.get('change_id', 'unknown')} ---")
        print(f"    ã‚µã‚¤ã‚º: {task.get('lines_added', 0)}è¡Œè¿½åŠ , {task.get('files_changed', 0)}ãƒ•ã‚¡ã‚¤ãƒ«å¤‰æ›´")
        
        for j, developer in enumerate(top_developers):
            print(f"\\n  é–‹ç™ºè€… {j+1}: {developer['name']}")
            print(f"    æ´»å‹•: ä½œæˆ{developer['changes_authored']}ä»¶, ãƒ¬ãƒ“ãƒ¥ãƒ¼{developer['changes_reviewed']}ä»¶")
            
            # è¦³æ¸¬ãƒ™ã‚¯ãƒˆãƒ«ã®æ§‹ç¯‰
            obs = optimizer._build_observation(developer, task)
            print(f"    è¦³æ¸¬ãƒ™ã‚¯ãƒˆãƒ« (æœ€åˆã®10æ¬¡å…ƒ): {obs[:10]}")
            
            # ãƒ¢ãƒ‡ãƒ«äºˆæ¸¬
            obs_tensor = torch.FloatTensor(obs).unsqueeze(0)
            
            with torch.no_grad():
                action_logits, value = optimizer.actor_critic(obs_tensor)
                probs = torch.softmax(action_logits, dim=-1)
                
                print(f"    è¡Œå‹•ãƒ­ã‚¸ãƒƒãƒˆ: {action_logits[0].numpy()}")
                print(f"    è¡Œå‹•ç¢ºç‡: assign={probs[0,0]:.3f}, reject={probs[0,1]:.3f}, defer={probs[0,2]:.3f}")
                print(f"    çŠ¶æ…‹ä¾¡å€¤: {value.item():.3f}")
                
                # æœ€é©è¡Œå‹•
                best_action_idx = torch.argmax(probs, dim=-1).item()
                action_names = ['assign', 'reject', 'defer']
                best_action = action_names[best_action_idx]
                confidence = probs[0, best_action_idx].item()
                
                print(f"    â†’ æ¨å¥¨: {best_action} (ä¿¡é ¼åº¦: {confidence:.1%})")
    
    # çµ±è¨ˆåˆ†æ
    print(f"\\nğŸ“ˆ çµ±è¨ˆåˆ†æ:")
    
    all_predictions = []
    for task in tasks:
        for developer in top_developers:
            obs = optimizer._build_observation(developer, task)
            obs_tensor = torch.FloatTensor(obs).unsqueeze(0)
            
            with torch.no_grad():
                action_logits, value = optimizer.actor_critic(obs_tensor)
                probs = torch.softmax(action_logits, dim=-1)
                best_action_idx = torch.argmax(probs, dim=-1).item()
                
                all_predictions.append({
                    'action': best_action_idx,
                    'probs': probs[0].numpy(),
                    'value': value.item()
                })
    
    # è¡Œå‹•åˆ†å¸ƒ
    action_counts = [0, 0, 0]
    for pred in all_predictions:
        action_counts[pred['action']] += 1
    
    total_predictions = len(all_predictions)
    print(f"   ç·äºˆæ¸¬æ•°: {total_predictions}")
    print(f"   å‰²ã‚Šå½“ã¦ (assign): {action_counts[0]}ä»¶ ({action_counts[0]/total_predictions*100:.1f}%)")
    print(f"   æ‹’å¦ (reject): {action_counts[1]}ä»¶ ({action_counts[1]/total_predictions*100:.1f}%)")
    print(f"   å»¶æœŸ (defer): {action_counts[2]}ä»¶ ({action_counts[2]/total_predictions*100:.1f}%)")
    
    # å¹³å‡ç¢ºç‡
    avg_probs = np.mean([pred['probs'] for pred in all_predictions], axis=0)
    print(f"   å¹³å‡ç¢ºç‡: assign={avg_probs[0]:.3f}, reject={avg_probs[1]:.3f}, defer={avg_probs[2]:.3f}")
    
    # å¹³å‡ä¾¡å€¤
    avg_value = np.mean([pred['value'] for pred in all_predictions])
    print(f"   å¹³å‡çŠ¶æ…‹ä¾¡å€¤: {avg_value:.3f}")
    
    # ãƒ¢ãƒ‡ãƒ«ã®é‡ã¿ã‚’ç¢ºèª
    print(f"\\nğŸ”§ ãƒ¢ãƒ‡ãƒ«æ§‹é€ åˆ†æ:")
    total_params = sum(p.numel() for p in optimizer.actor_critic.parameters())
    trainable_params = sum(p.numel() for p in optimizer.actor_critic.parameters() if p.requires_grad)
    print(f"   ç·ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: {total_params:,}")
    print(f"   è¨“ç·´å¯èƒ½ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: {trainable_params:,}")
    
    # æœ€çµ‚å±¤ã®é‡ã¿ã‚’ç¢ºèª
    actor_final_layer = optimizer.actor_critic.actor_head[-1]
    print(f"   Actoræœ€çµ‚å±¤ã®é‡ã¿ç¯„å›²: [{actor_final_layer.weight.min():.3f}, {actor_final_layer.weight.max():.3f}]")
    print(f"   Actoræœ€çµ‚å±¤ã®ãƒã‚¤ã‚¢ã‚¹: {actor_final_layer.bias.data}")
    
    logger.info("ãƒ‡ãƒãƒƒã‚°å®Œäº†")


def create_improved_model():
    """æ”¹è‰¯ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã®ä½œæˆ"""
    logger.info("=== æ”¹è‰¯ãƒ¢ãƒ‡ãƒ«ä½œæˆ ===")
    
    # ã‚ˆã‚Šç©æ¥µçš„ãªå‰²ã‚Šå½“ã¦ã‚’è¡Œã†ãƒ¢ãƒ‡ãƒ«ã‚’ä½œæˆ
    model = AdvancedActorCritic(obs_dim=20, action_dim=3)
    
    # Actoræœ€çµ‚å±¤ã®ãƒã‚¤ã‚¢ã‚¹ã‚’èª¿æ•´ï¼ˆassignã‚’å„ªé‡ï¼‰
    with torch.no_grad():
        model.actor_head[-1].bias[0] = 1.0   # assign
        model.actor_head[-1].bias[1] = -0.5  # reject
        model.actor_head[-1].bias[2] = 0.0   # defer
    
    # ãƒ¢ãƒ‡ãƒ«ã®ä¿å­˜
    torch.save({
        'actor_critic_state_dict': model.state_dict(),
        'optimizer_state_dict': None,
        'training_stats': {}
    }, 'models/improved_ppo_agent.pth')
    
    logger.info("æ”¹è‰¯ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜: models/improved_ppo_agent.pth")
    
    # æ”¹è‰¯ãƒ¢ãƒ‡ãƒ«ã§ãƒ†ã‚¹ãƒˆ
    print(f"\\nğŸš€ æ”¹è‰¯ãƒ¢ãƒ‡ãƒ«ã®ãƒ†ã‚¹ãƒˆ:")
    
    optimizer = RLTaskOptimizer(model_path='models/improved_ppo_agent.pth')
    
    # ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã§ãƒ†ã‚¹ãƒˆ
    sample_developer = {
        'developer_id': 'test@example.com',
        'name': 'Test Developer',
        'changes_authored': 50,
        'changes_reviewed': 100,
        'projects': ['project1', 'project2']
    }
    
    sample_task = {
        'change_id': 'test_change_001',
        'lines_added': 100,
        'files_changed': 3,
        'status': 'NEW',
        'score': 1
    }
    
    recommendation = optimizer.get_recommendation(sample_developer, sample_task)
    
    print(f"   é–‹ç™ºè€…: {recommendation.developer_name}")
    print(f"   ã‚¿ã‚¹ã‚¯: {recommendation.task_id}")
    print(f"   æ¨å¥¨: {recommendation.action} (ä¿¡é ¼åº¦: {recommendation.confidence:.1%})")
    print(f"   ç¢ºç‡åˆ†å¸ƒ: {recommendation.action_probabilities}")
    print(f"   ç†ç”±: {recommendation.reasoning}")


if __name__ == "__main__":
    debug_model_predictions()
    print("\\n" + "="*60)
    create_improved_model()