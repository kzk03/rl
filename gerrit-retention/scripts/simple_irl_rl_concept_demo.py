#!/usr/bin/env python3
"""
IRL/RLç¶™ç¶šäºˆæ¸¬ æ¦‚å¿µå®Ÿè¨¼ãƒ‡ãƒ¢

ãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹ã‚·ã‚¹ãƒ†ãƒ ã‚’IRL/RLã§ç½®ãæ›ãˆã‚‹æ¦‚å¿µã‚’
ã‚·ãƒ³ãƒ—ãƒ«ã«å®Ÿè¨¼ã™ã‚‹ãƒ‡ãƒ¢ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
"""

import json
from pathlib import Path
from typing import Any, Dict, List

import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim


class SimpleRetentionIRL:
    """ã‚·ãƒ³ãƒ—ãƒ«ãªç¶™ç¶šäºˆæ¸¬IRL"""
    
    def __init__(self):
        # ç‰¹å¾´é‡æ¬¡å…ƒ
        self.feature_dim = 8
        
        # å ±é…¬é‡ã¿ï¼ˆå­¦ç¿’å¯¾è±¡ï¼‰
        self.reward_weights = torch.randn(self.feature_dim, requires_grad=True)
        
        # ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ãƒ¼
        self.optimizer = optim.Adam([self.reward_weights], lr=0.01)
        
        print("ğŸ§  ã‚·ãƒ³ãƒ—ãƒ«IRLã‚·ã‚¹ãƒ†ãƒ ã‚’åˆæœŸåŒ–ã—ã¾ã—ãŸ")
    
    def extract_features(self, developer: Dict[str, Any]) -> np.ndarray:
        """é–‹ç™ºè€…ã‹ã‚‰ç‰¹å¾´é‡ã‚’æŠ½å‡º"""
        
        features = [
            developer.get('changes_authored', 0) / 100.0,      # ä½œæˆå¤‰æ›´æ•°
            developer.get('changes_reviewed', 0) / 100.0,      # ãƒ¬ãƒ“ãƒ¥ãƒ¼æ•°
            len(developer.get('projects', [])) / 5.0,          # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæ•°
            len(developer.get('activity_history', [])) / 20.0, # æ´»å‹•æ•°
            1.0 if developer.get('changes_authored', 0) > 50 else 0.0,  # é«˜æ´»å‹•ãƒ•ãƒ©ã‚°
            1.0 if len(developer.get('projects', [])) > 2 else 0.0,     # å¤šãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ•ãƒ©ã‚°
            developer.get('collaboration_score', 0.5),         # å”åŠ›ã‚¹ã‚³ã‚¢
            developer.get('code_quality_score', 0.5)           # ã‚³ãƒ¼ãƒ‰å“è³ªã‚¹ã‚³ã‚¢
        ]
        
        return np.array(features, dtype=np.float32)
    
    def train_irl(self, expert_data: List[Dict[str, Any]], epochs: int = 100):
        """IRLã§å ±é…¬é–¢æ•°ã‚’å­¦ç¿’"""
        
        print(f"ğŸ¯ IRLè¨“ç·´é–‹å§‹: {epochs}ã‚¨ãƒãƒƒã‚¯")
        
        # ç¶™ç¶šã—ãŸé–‹ç™ºè€…ï¼ˆã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆï¼‰ã¨é›¢è„±ã—ãŸé–‹ç™ºè€…ã‚’åˆ†é›¢
        continued_devs = [d for d in expert_data if d.get('continued', True)]
        left_devs = [d for d in expert_data if not d.get('continued', True)]
        
        print(f"   ç¶™ç¶šé–‹ç™ºè€…: {len(continued_devs)}äºº")
        print(f"   é›¢è„±é–‹ç™ºè€…: {len(left_devs)}äºº")
        
        losses = []
        
        for epoch in range(epochs):
            epoch_loss = 0.0
            batch_count = 0
            
            # ç¶™ç¶šé–‹ç™ºè€…ã®ç‰¹å¾´é‡
            for expert_dev in continued_devs:
                expert_features = torch.tensor(
                    self.extract_features(expert_dev['developer']), 
                    dtype=torch.float32
                )
                
                # é›¢è„±é–‹ç™ºè€…ã¨ã®æ¯”è¼ƒ
                for non_expert_dev in left_devs:
                    non_expert_features = torch.tensor(
                        self.extract_features(non_expert_dev['developer']), 
                        dtype=torch.float32
                    )
                    
                    # å ±é…¬è¨ˆç®—
                    expert_reward = torch.dot(self.reward_weights, expert_features)
                    non_expert_reward = torch.dot(self.reward_weights, non_expert_features)
                    
                    # IRLæå¤±ï¼ˆç¶™ç¶šé–‹ç™ºè€…ã®å ±é…¬ > é›¢è„±é–‹ç™ºè€…ã®å ±é…¬ï¼‰
                    loss = torch.max(
                        torch.tensor(0.0), 
                        1.0 - (expert_reward - non_expert_reward)
                    )
                    
                    # ãƒãƒƒã‚¯ãƒ—ãƒ­ãƒ‘ã‚²ãƒ¼ã‚·ãƒ§ãƒ³
                    self.optimizer.zero_grad()
                    loss.backward()
                    self.optimizer.step()
                    
                    epoch_loss += loss.item()
                    batch_count += 1
            
            avg_loss = epoch_loss / max(batch_count, 1)
            losses.append(avg_loss)
            
            if epoch % 20 == 0:
                print(f"   ã‚¨ãƒãƒƒã‚¯ {epoch}: æå¤± = {avg_loss:.4f}")
        
        print(f"ğŸ‰ IRLè¨“ç·´å®Œäº†: æœ€çµ‚æå¤± = {losses[-1]:.4f}")
        
        return {'losses': losses, 'learned_weights': self.reward_weights.detach().numpy()}
    
    def predict_continuation_probability(self, developer: Dict[str, Any]) -> float:
        """ç¶™ç¶šç¢ºç‡ã‚’äºˆæ¸¬"""
        
        features = torch.tensor(self.extract_features(developer), dtype=torch.float32)
        
        with torch.no_grad():
            reward = torch.dot(self.reward_weights, features)
            # ã‚·ã‚°ãƒ¢ã‚¤ãƒ‰é–¢æ•°ã§ç¢ºç‡ã«å¤‰æ›
            probability = torch.sigmoid(reward).item()
        
        return probability


class SimpleRetentionRL:
    """ã‚·ãƒ³ãƒ—ãƒ«ãªç¶™ç¶šäºˆæ¸¬RL"""
    
    def __init__(self):
        # ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯
        self.network = nn.Sequential(
            nn.Linear(8, 32),
            nn.ReLU(),
            nn.Linear(32, 16),
            nn.ReLU(),
            nn.Linear(16, 1),
            nn.Sigmoid()
        )
        
        # ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ãƒ¼
        self.optimizer = optim.Adam(self.network.parameters(), lr=0.01)
        
        print("ğŸ® ã‚·ãƒ³ãƒ—ãƒ«RLã‚·ã‚¹ãƒ†ãƒ ã‚’åˆæœŸåŒ–ã—ã¾ã—ãŸ")
    
    def extract_features(self, developer: Dict[str, Any]) -> np.ndarray:
        """é–‹ç™ºè€…ã‹ã‚‰ç‰¹å¾´é‡ã‚’æŠ½å‡º"""
        
        features = [
            developer.get('changes_authored', 0) / 100.0,      # ä½œæˆå¤‰æ›´æ•°
            developer.get('changes_reviewed', 0) / 100.0,      # ãƒ¬ãƒ“ãƒ¥ãƒ¼æ•°
            len(developer.get('projects', [])) / 5.0,          # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæ•°
            len(developer.get('activity_history', [])) / 20.0, # æ´»å‹•æ•°
            1.0 if developer.get('changes_authored', 0) > 50 else 0.0,  # é«˜æ´»å‹•ãƒ•ãƒ©ã‚°
            1.0 if len(developer.get('projects', [])) > 2 else 0.0,     # å¤šãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ•ãƒ©ã‚°
            developer.get('collaboration_score', 0.5),         # å”åŠ›ã‚¹ã‚³ã‚¢
            developer.get('code_quality_score', 0.5)           # ã‚³ãƒ¼ãƒ‰å“è³ªã‚¹ã‚³ã‚¢
        ]
        
        return np.array(features, dtype=np.float32)
    
    def train_rl(self, training_data: List[Dict[str, Any]], epochs: int = 100):
        """RLã§äºˆæ¸¬ãƒãƒªã‚·ãƒ¼ã‚’å­¦ç¿’"""
        
        print(f"ğŸ¯ RLè¨“ç·´é–‹å§‹: {epochs}ã‚¨ãƒãƒƒã‚¯")
        
        losses = []
        accuracies = []
        
        for epoch in range(epochs):
            epoch_loss = 0.0
            correct_predictions = 0
            
            for data_point in training_data:
                developer = data_point['developer']
                true_continued = data_point.get('continued', True)
                
                # ç‰¹å¾´é‡æŠ½å‡º
                features = torch.tensor(
                    self.extract_features(developer), 
                    dtype=torch.float32
                ).unsqueeze(0)
                
                # äºˆæ¸¬
                predicted_prob = self.network(features)
                
                # çœŸã®ãƒ©ãƒ™ãƒ«
                target = torch.tensor([[1.0 if true_continued else 0.0]], dtype=torch.float32)
                
                # æå¤±è¨ˆç®—ï¼ˆãƒã‚¤ãƒŠãƒªã‚¯ãƒ­ã‚¹ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ï¼‰
                loss = nn.BCELoss()(predicted_prob, target)
                
                # ãƒãƒƒã‚¯ãƒ—ãƒ­ãƒ‘ã‚²ãƒ¼ã‚·ãƒ§ãƒ³
                self.optimizer.zero_grad()
                loss.backward()
                self.optimizer.step()
                
                epoch_loss += loss.item()
                
                # ç²¾åº¦è¨ˆç®—
                predicted_label = predicted_prob.item() > 0.5
                if predicted_label == true_continued:
                    correct_predictions += 1
            
            avg_loss = epoch_loss / len(training_data)
            accuracy = correct_predictions / len(training_data)
            
            losses.append(avg_loss)
            accuracies.append(accuracy)
            
            if epoch % 20 == 0:
                print(f"   ã‚¨ãƒãƒƒã‚¯ {epoch}: æå¤± = {avg_loss:.4f}, ç²¾åº¦ = {accuracy:.1%}")
        
        print(f"ğŸ‰ RLè¨“ç·´å®Œäº†: æœ€çµ‚ç²¾åº¦ = {accuracies[-1]:.1%}")
        
        return {'losses': losses, 'accuracies': accuracies}
    
    def predict_continuation_probability(self, developer: Dict[str, Any]) -> float:
        """ç¶™ç¶šç¢ºç‡ã‚’äºˆæ¸¬"""
        
        features = torch.tensor(
            self.extract_features(developer), 
            dtype=torch.float32
        ).unsqueeze(0)
        
        with torch.no_grad():
            probability = self.network(features).item()
        
        return probability


def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    
    print("ğŸ¤– IRL/RLç¶™ç¶šäºˆæ¸¬ã‚·ã‚¹ãƒ†ãƒ  æ¦‚å¿µå®Ÿè¨¼ãƒ‡ãƒ¢")
    print("=" * 70)
    print("ãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹ã‹ã‚‰æ©Ÿæ¢°å­¦ç¿’ã¸ã®ç§»è¡Œã‚’å®Ÿè¨¼ã—ã¾ã™")
    print()
    
    # è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã‚’ä½œæˆï¼ˆç°¡ç•¥ç‰ˆï¼‰
    training_data = [
        # ç¶™ç¶šã—ãŸé–‹ç™ºè€…
        {
            'developer': {
                'developer_id': 'alice@example.com',
                'name': 'Alice Continued',
                'changes_authored': 85,
                'changes_reviewed': 120,
                'projects': ['project-a', 'project-b', 'project-c'],
                'activity_history': [{'type': 'commit'} for _ in range(15)],
                'collaboration_score': 0.8,
                'code_quality_score': 0.7
            },
            'continued': True
        },
        {
            'developer': {
                'developer_id': 'bob@example.com',
                'name': 'Bob Continued',
                'changes_authored': 156,
                'changes_reviewed': 89,
                'projects': ['project-a', 'project-c', 'project-d'],
                'activity_history': [{'type': 'commit'} for _ in range(18)],
                'collaboration_score': 0.9,
                'code_quality_score': 0.8
            },
            'continued': True
        },
        {
            'developer': {
                'developer_id': 'eve@example.com',
                'name': 'Eve Veteran',
                'changes_authored': 456,
                'changes_reviewed': 234,
                'projects': ['project-a', 'project-b', 'project-c', 'project-d', 'project-e'],
                'activity_history': [{'type': 'commit'} for _ in range(25)],
                'collaboration_score': 0.95,
                'code_quality_score': 0.9
            },
            'continued': True
        },
        # é›¢è„±ã—ãŸé–‹ç™ºè€…
        {
            'developer': {
                'developer_id': 'charlie@example.com',
                'name': 'Charlie Left',
                'changes_authored': 12,
                'changes_reviewed': 8,
                'projects': ['project-b'],
                'activity_history': [{'type': 'commit'} for _ in range(5)],
                'collaboration_score': 0.3,
                'code_quality_score': 0.4
            },
            'continued': False
        },
        {
            'developer': {
                'developer_id': 'diana@example.com',
                'name': 'Diana Left',
                'changes_authored': 34,
                'changes_reviewed': 18,
                'projects': ['project-c'],
                'activity_history': [{'type': 'commit'} for _ in range(8)],
                'collaboration_score': 0.4,
                'code_quality_score': 0.3
            },
            'continued': False
        },
        {
            'developer': {
                'developer_id': 'frank@example.com',
                'name': 'Frank Left',
                'changes_authored': 23,
                'changes_reviewed': 15,
                'projects': ['project-a'],
                'activity_history': [{'type': 'commit'} for _ in range(6)],
                'collaboration_score': 0.2,
                'code_quality_score': 0.35
            },
            'continued': False
        }
    ]
    
    print(f"ğŸ“Š è¨“ç·´ãƒ‡ãƒ¼ã‚¿: {len(training_data)}ä»¶")
    print(f"   ç¶™ç¶š: {sum(1 for d in training_data if d['continued'])}ä»¶")
    print(f"   é›¢è„±: {sum(1 for d in training_data if not d['continued'])}ä»¶")
    print()
    
    # 1. IRL ã‚·ã‚¹ãƒ†ãƒ ã®ãƒ†ã‚¹ãƒˆ
    print("ğŸ§  é€†å¼·åŒ–å­¦ç¿’ï¼ˆIRLï¼‰ã‚·ã‚¹ãƒ†ãƒ ")
    print("-" * 50)
    
    irl_system = SimpleRetentionIRL()
    irl_results = irl_system.train_irl(training_data, epochs=100)
    
    print("\nğŸ” å­¦ç¿’ã•ã‚ŒãŸå ±é…¬é‡ã¿:")
    feature_names = [
        'changes_authored', 'changes_reviewed', 'project_count', 'activity_count',
        'high_activity_flag', 'multi_project_flag', 'collaboration_score', 'code_quality_score'
    ]
    
    learned_weights = irl_results['learned_weights']
    for i, (name, weight) in enumerate(zip(feature_names, learned_weights)):
        print(f"   {name:20s}: {weight:6.3f}")
    
    print()
    
    # 2. RL ã‚·ã‚¹ãƒ†ãƒ ã®ãƒ†ã‚¹ãƒˆ
    print("ğŸ® å¼·åŒ–å­¦ç¿’ï¼ˆRLï¼‰ã‚·ã‚¹ãƒ†ãƒ ")
    print("-" * 50)
    
    rl_system = SimpleRetentionRL()
    rl_results = rl_system.train_rl(training_data, epochs=100)
    
    print()
    
    # 3. äºˆæ¸¬æ¯”è¼ƒãƒ†ã‚¹ãƒˆ
    print("ğŸ“Š äºˆæ¸¬ã‚·ã‚¹ãƒ†ãƒ æ¯”è¼ƒãƒ†ã‚¹ãƒˆ")
    print("=" * 70)
    
    # ãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹äºˆæ¸¬ï¼ˆå¾“æ¥ã‚·ã‚¹ãƒ†ãƒ ï¼‰
    def rule_based_predict(developer: Dict[str, Any]) -> float:
        """ãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹äºˆæ¸¬"""
        changes = developer.get('changes_authored', 0)
        reviews = developer.get('changes_reviewed', 0)
        projects = len(developer.get('projects', []))
        
        # å˜ç´”ãªãƒ«ãƒ¼ãƒ«
        if changes > 100 and reviews > 50 and projects > 2:
            return 0.9  # é«˜ç¢ºç‡ã§ç¶™ç¶š
        elif changes > 50 and reviews > 30:
            return 0.7  # ä¸­ç¢ºç‡ã§ç¶™ç¶š
        elif changes > 20:
            return 0.5  # ä¸­ç¨‹åº¦
        else:
            return 0.3  # ä½ç¢ºç‡ã§ç¶™ç¶š
    
    print(f"{'é–‹ç™ºè€…å':<15} {'å®Ÿéš›':<6} {'ãƒ«ãƒ¼ãƒ«':<8} {'IRL':<8} {'RL':<8} {'ãƒ«ãƒ¼ãƒ«æ­£è§£':<10} {'IRLæ­£è§£':<8} {'RLæ­£è§£':<8}")
    print("-" * 80)
    
    rule_correct = 0
    irl_correct = 0
    rl_correct = 0
    
    for data_point in training_data:
        developer = data_point['developer']
        true_continued = data_point['continued']
        
        # å„ã‚·ã‚¹ãƒ†ãƒ ã§äºˆæ¸¬
        rule_prob = rule_based_predict(developer)
        irl_prob = irl_system.predict_continuation_probability(developer)
        rl_prob = rl_system.predict_continuation_probability(developer)
        
        # ãƒ©ãƒ™ãƒ«åŒ–ï¼ˆ50%é–¾å€¤ï¼‰
        rule_label = rule_prob > 0.5
        irl_label = irl_prob > 0.5
        rl_label = rl_prob > 0.5
        
        # æ­£è§£åˆ¤å®š
        rule_correct += (rule_label == true_continued)
        irl_correct += (irl_label == true_continued)
        rl_correct += (rl_label == true_continued)
        
        # çµæœè¡¨ç¤º
        true_str = "ç¶™ç¶š" if true_continued else "é›¢è„±"
        rule_correct_str = "âœ…" if rule_label == true_continued else "âŒ"
        irl_correct_str = "âœ…" if irl_label == true_continued else "âŒ"
        rl_correct_str = "âœ…" if rl_label == true_continued else "âŒ"
        
        print(f"{developer['name'][:14]:<15} {true_str:<6} {rule_prob:>6.1%} {irl_prob:>8.1%} {rl_prob:>8.1%} {rule_correct_str:>10} {irl_correct_str:>8} {rl_correct_str:>8}")
    
    print("-" * 80)
    total_samples = len(training_data)
    print(f"{'ç·åˆç²¾åº¦':<15} {'':<6} {rule_correct/total_samples:>6.1%} {irl_correct/total_samples:>8.1%} {rl_correct/total_samples:>8.1%}")
    print()
    
    # 4. ã‚·ã‚¹ãƒ†ãƒ æ¯”è¼ƒåˆ†æ
    print("ğŸ† ã‚·ã‚¹ãƒ†ãƒ æ¯”è¼ƒåˆ†æ")
    print("=" * 70)
    
    systems = [
        ('ãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹', rule_correct/total_samples),
        ('é€†å¼·åŒ–å­¦ç¿’ï¼ˆIRLï¼‰', irl_correct/total_samples),
        ('å¼·åŒ–å­¦ç¿’ï¼ˆRLï¼‰', rl_correct/total_samples)
    ]
    
    # æœ€é«˜æ€§èƒ½ã‚·ã‚¹ãƒ†ãƒ 
    best_system = max(systems, key=lambda x: x[1])
    
    print("**ã‚·ã‚¹ãƒ†ãƒ æ€§èƒ½ãƒ©ãƒ³ã‚­ãƒ³ã‚°:**")
    for i, (system_name, accuracy) in enumerate(sorted(systems, key=lambda x: x[1], reverse=True), 1):
        status = "ğŸ¥‡" if i == 1 else "ğŸ¥ˆ" if i == 2 else "ğŸ¥‰"
        print(f"  {i}. {status} {system_name}: {accuracy:.1%}")
    
    print()
    print("**å„ã‚·ã‚¹ãƒ†ãƒ ã®ç‰¹å¾´:**")
    print()
    print("ğŸ”§ **ãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹ã‚·ã‚¹ãƒ†ãƒ :**")
    print("   âœ… è§£é‡ˆã—ã‚„ã™ã„")
    print("   âœ… å®Ÿè£…ãŒç°¡å˜")
    print("   âŒ å›ºå®šçš„ãªãƒ«ãƒ¼ãƒ«")
    print("   âŒ è¤‡é›‘ãªãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æ‰ãˆã‚‰ã‚Œãªã„")
    print()
    
    print("ğŸ§  **é€†å¼·åŒ–å­¦ç¿’ï¼ˆIRLï¼‰ã‚·ã‚¹ãƒ†ãƒ :**")
    print("   âœ… ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã®è¡Œå‹•ã‹ã‚‰å­¦ç¿’")
    print("   âœ… è§£é‡ˆå¯èƒ½ãªå ±é…¬é–¢æ•°")
    print("   âœ… ç¶™ç¶šè¦å› ã‚’è‡ªå‹•ç™ºè¦‹")
    print("   âŒ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆãƒ‡ãƒ¼ã‚¿ãŒå¿…è¦")
    print()
    
    print("ğŸ® **å¼·åŒ–å­¦ç¿’ï¼ˆRLï¼‰ã‚·ã‚¹ãƒ†ãƒ :**")
    print("   âœ… äºˆæ¸¬ç²¾åº¦ã‚’ç›´æ¥æœ€é©åŒ–")
    print("   âœ… è¤‡é›‘ãªãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’å­¦ç¿’å¯èƒ½")
    print("   âœ… å¤§é‡ãƒ‡ãƒ¼ã‚¿ã§æ€§èƒ½å‘ä¸Š")
    print("   âŒ ãƒ–ãƒ©ãƒƒã‚¯ãƒœãƒƒã‚¯ã‚¹çš„")
    print()
    
    # 5. å®Ÿç”¨åŒ–ã¸ã®ææ¡ˆ
    print("ğŸ’¡ å®Ÿç”¨åŒ–ã¸ã®ææ¡ˆ")
    print("=" * 70)
    
    if best_system[0] == 'ãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹':
        print("ğŸ”§ **æ¨å¥¨**: ãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹ã‚·ã‚¹ãƒ†ãƒ ã®æ”¹è‰¯")
        print("   - ã‚ˆã‚Šå¤šãã®ç‰¹å¾´é‡ã‚’è¿½åŠ ")
        print("   - é‡ã¿ä»˜ã‘ã®æœ€é©åŒ–")
        print("   - æ©Ÿæ¢°å­¦ç¿’ã¨ã®çµ„ã¿åˆã‚ã›")
    elif 'IRL' in best_system[0]:
        print("ğŸ§  **æ¨å¥¨**: IRLã‚·ã‚¹ãƒ†ãƒ ã®æœ¬æ ¼å°å…¥")
        print("   - ã‚ˆã‚Šå¤šãã®ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆãƒ‡ãƒ¼ã‚¿ã®åé›†")
        print("   - ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ã®æ”¹å–„")
        print("   - å ±é…¬é–¢æ•°ã®è§£é‡ˆæ€§å‘ä¸Š")
    else:
        print("ğŸ® **æ¨å¥¨**: RLã‚·ã‚¹ãƒ†ãƒ ã®æœ¬æ ¼å°å…¥")
        print("   - ã‚ˆã‚Šå¤§è¦æ¨¡ãªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã®è¨“ç·´")
        print("   - ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ§‹é€ ã®æœ€é©åŒ–")
        print("   - èª¬æ˜å¯èƒ½æ€§ã®å‘ä¸Š")
    
    print()
    print("**å…±é€šã®æ”¹å–„ç‚¹:**")
    print("1. ç‰¹å¾´é‡ã®æ‹¡å¼µï¼ˆã‚³ãƒ¼ãƒ‰å“è³ªã€å”åŠ›é–¢ä¿‚ã€ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆé‡è¦åº¦ï¼‰")
    print("2. æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ã®æ´»ç”¨ï¼ˆæ´»å‹•ãƒˆãƒ¬ãƒ³ãƒ‰ã€å­£ç¯€æ€§ï¼‰")
    print("3. ã‚ˆã‚Šå¤§è¦æ¨¡ãªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã®æ¤œè¨¼")
    print("4. ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ å­¦ç¿’æ©Ÿèƒ½ã®è¿½åŠ ")
    print("5. èª¬æ˜å¯èƒ½æ€§ã®å‘ä¸Š")
    print()
    
    # çµæœä¿å­˜
    results = {
        'system_comparison': {
            'rule_based_accuracy': rule_correct / total_samples,
            'irl_accuracy': irl_correct / total_samples,
            'rl_accuracy': rl_correct / total_samples,
            'best_system': best_system[0],
            'best_accuracy': best_system[1]
        },
        'irl_results': irl_results,
        'rl_results': rl_results,
        'training_data_size': len(training_data),
        'demo_date': str(datetime.now())
    }
    
    output_path = "outputs/comprehensive_retention/irl_rl_concept_demo.json"
    Path(output_path).parent.mkdir(parents=True, exist_ok=True)
    
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(results, f, indent=2, ensure_ascii=False, default=str)
    
    print(f"ğŸ’¾ çµæœã‚’ä¿å­˜ã—ã¾ã—ãŸ: {output_path}")
    print()
    print("ğŸ‰ IRL/RLæ¦‚å¿µå®Ÿè¨¼ãƒ‡ãƒ¢å®Œäº†ï¼")
    print(f"   æœ€é«˜æ€§èƒ½: {best_system[0]} ({best_system[1]:.1%})")
    print("   ãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹ã‹ã‚‰æ©Ÿæ¢°å­¦ç¿’ã¸ã®ç§»è¡Œå¯èƒ½æ€§ãŒå®Ÿè¨¼ã•ã‚Œã¾ã—ãŸã€‚")


if __name__ == "__main__":
    main()