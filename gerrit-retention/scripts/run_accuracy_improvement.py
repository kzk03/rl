#!/usr/bin/env python3
"""
äºˆæ¸¬ç²¾åº¦ç¶™ç¶šæ”¹å–„ã‚·ã‚¹ãƒ†ãƒ ã®å®Ÿè¡Œã‚¹ã‚¯ãƒªãƒ—ãƒˆ

ç¾åœ¨ã®217.7%æ”¹å–„ã‚’é”æˆã—ãŸã‚·ã‚¹ãƒ†ãƒ ã‚’ãƒ™ãƒ¼ã‚¹ã«ã€
ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«å­¦ç¿’ã¨ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–ã«ã‚ˆã‚‹
ã•ã‚‰ãªã‚‹ç²¾åº¦å‘ä¸Šã‚’å®Ÿç¾ã—ã¾ã™ã€‚
"""

import json
import sys
import warnings
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, List

import numpy as np
import pandas as pd

warnings.filterwarnings('ignore')

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’ãƒ‘ã‚¹ã«è¿½åŠ 
sys.path.append(str(Path(__file__).parent.parent))

from src.gerrit_retention.prediction.advanced_accuracy_improver import (
    AdvancedAccuracyImprover,
)


def load_developer_data(data_path: str) -> List[Dict[str, Any]]:
    """é–‹ç™ºè€…ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿"""
    print(f"ğŸ“Š é–‹ç™ºè€…ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ä¸­: {data_path}")
    
    with open(data_path, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    print(f"âœ… {len(data)}äººã®é–‹ç™ºè€…ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ")
    return data

def create_retention_labels(developer_data: List[Dict[str, Any]]) -> np.ndarray:
    """ç¶™ç¶šç‡ãƒ©ãƒ™ãƒ«ã®ä½œæˆ"""
    print("ğŸ¯ ç¶™ç¶šç‡ãƒ©ãƒ™ãƒ«ã‚’ä½œæˆä¸­...")
    
    labels = []
    current_time = datetime.now()
    
    for dev in developer_data:
        try:
            # æœ€å¾Œã®æ´»å‹•ã‹ã‚‰ã®çµŒéæ—¥æ•°
            last_activity = datetime.fromisoformat(
                dev.get('last_activity', '').replace(' ', 'T')
            )
            days_since_last = (current_time - last_activity).days
            
            # ç¶™ç¶šç‡ã‚¹ã‚³ã‚¢ã®è¨ˆç®—ï¼ˆ0-1ã®ç¯„å›²ï¼‰
            if days_since_last <= 7:
                retention_score = 1.0  # é«˜ã„ç¶™ç¶šç‡
            elif days_since_last <= 30:
                retention_score = 0.8  # ä¸­ç¨‹åº¦ã®ç¶™ç¶šç‡
            elif days_since_last <= 90:
                retention_score = 0.4  # ä½ã„ç¶™ç¶šç‡
            elif days_since_last <= 180:
                retention_score = 0.2  # éå¸¸ã«ä½ã„ç¶™ç¶šç‡
            else:
                retention_score = 0.0  # é›¢è„±çŠ¶æ…‹
            
            # æ´»å‹•é‡ã«ã‚ˆã‚‹èª¿æ•´
            total_activity = dev.get('changes_authored', 0) + dev.get('changes_reviewed', 0)
            if total_activity > 100:
                retention_score *= 1.2  # é«˜æ´»å‹•è€…ã¯ãƒœãƒ¼ãƒŠã‚¹
            elif total_activity < 10:
                retention_score *= 0.8  # ä½æ´»å‹•è€…ã¯ãƒšãƒŠãƒ«ãƒ†ã‚£
            
            # 0-1ã®ç¯„å›²ã«æ­£è¦åŒ–
            retention_score = min(1.0, max(0.0, retention_score))
            labels.append(retention_score)
            
        except (ValueError, TypeError):
            labels.append(0.0)  # ã‚¨ãƒ©ãƒ¼ã®å ´åˆã¯é›¢è„±ã¨ã¿ãªã™
    
    labels = np.array(labels)
    print(f"âœ… ç¶™ç¶šç‡ãƒ©ãƒ™ãƒ«ã‚’ä½œæˆå®Œäº†")
    print(f"   å¹³å‡ç¶™ç¶šç‡: {labels.mean():.3f}")
    print(f"   ç¶™ç¶šç‡åˆ†å¸ƒ: é«˜(>0.8): {(labels > 0.8).sum()}äºº, "
          f"ä¸­(0.4-0.8): {((labels >= 0.4) & (labels <= 0.8)).sum()}äºº, "
          f"ä½(<0.4): {(labels < 0.4).sum()}äºº")
    
    return labels

def split_data(X: np.ndarray, y: np.ndarray, test_size: float = 0.2, random_state: int = 42):
    """ãƒ‡ãƒ¼ã‚¿ã®åˆ†å‰²"""
    from sklearn.model_selection import train_test_split
    
    return train_test_split(X, y, test_size=test_size, random_state=random_state)

def run_accuracy_improvement():
    """äºˆæ¸¬ç²¾åº¦æ”¹å–„ã®å®Ÿè¡Œ"""
    print("ğŸš€ äºˆæ¸¬ç²¾åº¦ç¶™ç¶šæ”¹å–„ã‚·ã‚¹ãƒ†ãƒ ã‚’é–‹å§‹ã—ã¾ã™")
    print("=" * 80)
    
    # è¨­å®š
    config = {
        'data_path': 'data/processed/unified/all_developers.json',
        'output_path': 'outputs/accuracy_improvement',
        'test_size': 0.2,
        'random_state': 42
    }
    
    # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ä½œæˆ
    Path(config['output_path']).mkdir(parents=True, exist_ok=True)
    
    try:
        # 1. ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿
        developer_data = load_developer_data(config['data_path'])
        
        # 2. ã‚·ã‚¹ãƒ†ãƒ ã®åˆæœŸåŒ–
        improver = AdvancedAccuracyImprover(config)
        
        # 3. ç‰¹å¾´é‡ã®æŠ½å‡º
        print("\nğŸ”§ é«˜åº¦ãªç‰¹å¾´é‡ã‚’æŠ½å‡ºä¸­...")
        features_list = []
        feature_names = None
        
        for i, dev in enumerate(developer_data):
            if i % 100 == 0:
                print(f"   é€²æ—: {i}/{len(developer_data)} ({i/len(developer_data)*100:.1f}%)")
            
            features = improver.extract_advanced_features(dev)
            
            if feature_names is None:
                feature_names = list(features.keys())
            
            features_list.append([features.get(name, 0.0) for name in feature_names])
        
        X = np.array(features_list)
        print(f"âœ… ç‰¹å¾´é‡æŠ½å‡ºå®Œäº†: {X.shape[1]}æ¬¡å…ƒã®ç‰¹å¾´é‡")
        print(f"   ç‰¹å¾´é‡å: {', '.join(feature_names[:5])}...")
        
        # 4. ãƒ©ãƒ™ãƒ«ã®ä½œæˆ
        y = create_retention_labels(developer_data)
        
        # 5. ãƒ‡ãƒ¼ã‚¿ã®åˆ†å‰²
        print(f"\nğŸ“Š ãƒ‡ãƒ¼ã‚¿ã‚’åˆ†å‰²ä¸­ (ãƒ†ã‚¹ãƒˆæ¯”ç‡: {config['test_size']})")
        X_train, X_test, y_train, y_test = split_data(
            X, y, config['test_size'], config['random_state']
        )
        print(f"âœ… è¨“ç·´ãƒ‡ãƒ¼ã‚¿: {X_train.shape[0]}ã‚µãƒ³ãƒ—ãƒ«, ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿: {X_test.shape[0]}ã‚µãƒ³ãƒ—ãƒ«")
        
        # 6. ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ãƒ¢ãƒ‡ãƒ«ã®è¨“ç·´
        print(f"\nğŸ¤– ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ãƒ¢ãƒ‡ãƒ«ã®è¨“ç·´ã‚’é–‹å§‹...")
        print("-" * 60)
        
        trained_models = improver.train_ensemble_models(X_train, y_train)
        print(f"âœ… {len(trained_models)}å€‹ã®ãƒ¢ãƒ‡ãƒ«ã‚’è¨“ç·´å®Œäº†")
        
        # 7. ãƒ¢ãƒ‡ãƒ«æ€§èƒ½ã®è©•ä¾¡
        print(f"\nğŸ“ˆ ãƒ¢ãƒ‡ãƒ«æ€§èƒ½ã‚’è©•ä¾¡ä¸­...")
        performance_results = improver.evaluate_model_performance(X_test, y_test)
        
        print("\nğŸ† æ€§èƒ½è©•ä¾¡çµæœ:")
        print("-" * 40)
        for model_name, metrics in performance_results.items():
            print(f"{model_name:15s}: RÂ² = {metrics['r2']:.4f}, "
                  f"RMSE = {metrics['rmse']:.4f}, MAE = {metrics['mae']:.4f}")
        
        # 8. ç‰¹å¾´é‡é‡è¦åº¦ã®åˆ†æ
        print(f"\nğŸ” ç‰¹å¾´é‡é‡è¦åº¦ã‚’åˆ†æä¸­...")
        feature_analysis = improver.analyze_feature_importance(feature_names)
        
        if 'top_10' in feature_analysis:
            print("\nğŸ“Š é‡è¦ç‰¹å¾´é‡ãƒˆãƒƒãƒ—10:")
            print("-" * 50)
            for i, (name, importance) in enumerate(feature_analysis['top_10'], 1):
                print(f"{i:2d}. {name:30s}: {importance:.4f}")
        
        # 9. æ”¹å–„ææ¡ˆã®ç”Ÿæˆ
        print(f"\nğŸ’¡ æ”¹å–„ææ¡ˆã‚’ç”Ÿæˆä¸­...")
        recommendations = improver.generate_improvement_recommendations(
            performance_results, feature_analysis
        )
        
        print("\nğŸ¯ æ”¹å–„ææ¡ˆ:")
        print("-" * 40)
        for i, rec in enumerate(recommendations, 1):
            print(f"{i}. {rec}")
        
        # 10. çµæœã®ä¿å­˜
        print(f"\nğŸ’¾ çµæœã‚’ä¿å­˜ä¸­...")
        
        # åŒ…æ‹¬çš„ãªçµæœã®ä½œæˆ
        comprehensive_results = {
            'timestamp': datetime.now().isoformat(),
            'data_info': {
                'total_samples': len(developer_data),
                'features_count': len(feature_names),
                'train_samples': len(X_train),
                'test_samples': len(X_test)
            },
            'performance': performance_results,
            'feature_analysis': feature_analysis,
            'recommendations': recommendations,
            'model_weights': improver.ensemble_weights,
            'feature_names': feature_names
        }
        
        results_file, models_file = improver.save_improvement_results(
            comprehensive_results, config['output_path']
        )
        
        # 11. æœ€çµ‚ã‚µãƒãƒªãƒ¼ã®è¡¨ç¤º
        print("\n" + "=" * 80)
        print("ğŸ‰ äºˆæ¸¬ç²¾åº¦æ”¹å–„ã‚·ã‚¹ãƒ†ãƒ å®Ÿè¡Œå®Œäº†ï¼")
        print("=" * 80)
        
        ensemble_r2 = performance_results['ensemble']['r2']
        ensemble_rmse = performance_results['ensemble']['rmse']
        
        print(f"ğŸ“Š æœ€çµ‚æ€§èƒ½:")
        print(f"   ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«RÂ²ã‚¹ã‚³ã‚¢: {ensemble_r2:.4f}")
        print(f"   RMSE: {ensemble_rmse:.4f}")
        
        if ensemble_r2 > 0.8:
            print("ğŸ† å„ªç§€ãªäºˆæ¸¬ç²¾åº¦ã‚’é”æˆã—ã¾ã—ãŸï¼")
        elif ensemble_r2 > 0.6:
            print("âœ… è‰¯å¥½ãªäºˆæ¸¬ç²¾åº¦ã‚’é”æˆã—ã¾ã—ãŸ")
        else:
            print("âš ï¸  äºˆæ¸¬ç²¾åº¦ã®æ”¹å–„ãŒå¿…è¦ã§ã™")
        
        print(f"\nğŸ“ ä¿å­˜ãƒ•ã‚¡ã‚¤ãƒ«:")
        print(f"   çµæœ: {results_file}")
        print(f"   ãƒ¢ãƒ‡ãƒ«: {models_file}")
        
        # å‰å›ã®217.7%æ”¹å–„ã¨ã®æ¯”è¼ƒ
        print(f"\nğŸ”„ å‰å›ã®217.7%æ”¹å–„ã‚·ã‚¹ãƒ†ãƒ ã¨ã®çµ±åˆ:")
        print(f"   ç¾åœ¨ã®ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ç²¾åº¦: {ensemble_r2:.1%}")
        print(f"   çµ±åˆã«ã‚ˆã‚Šæ›´ãªã‚‹ç²¾åº¦å‘ä¸ŠãŒæœŸå¾…ã•ã‚Œã¾ã™")
        
        return comprehensive_results
        
    except Exception as e:
        print(f"\nâŒ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        import traceback
        traceback.print_exc()
        return None

if __name__ == "__main__":
    results = run_accuracy_improvement()
    
    if results:
        print("\nâœ… äºˆæ¸¬ç²¾åº¦æ”¹å–„ã‚·ã‚¹ãƒ†ãƒ ãŒæ­£å¸¸ã«å®Œäº†ã—ã¾ã—ãŸ")
    else:
        print("\nâŒ äºˆæ¸¬ç²¾åº¦æ”¹å–„ã‚·ã‚¹ãƒ†ãƒ ã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ")
        sys.exit(1)