#!/usr/bin/env python3
"""å„é–‹ç™ºè€…ã®ç¶™ç¶šç¢ºçŽ‡äºˆæ¸¬ã¨å®Ÿéš›ãƒ©ãƒ™ãƒ«ã®æ•´åˆæ€§ã‚’åˆ†æžã™ã‚‹ã‚¹ã‚¯ãƒªãƒ—ãƒˆ

å‰æ:
  å…ˆã« `scripts/evaluate_workload_aware_probabilities.py` ã‚’å®Ÿè¡Œã—
  `outputs/retention_probability/predictions.json` ãŒç”Ÿæˆã•ã‚Œã¦ã„ã‚‹ã“ã¨ã€‚

æ©Ÿèƒ½:
 1. predictions.json ã‚’èª­ã¿è¾¼ã¿ (å„ developer ã® label_retained / prob_retained)
 2. åˆ†é¡žé–¾å€¤ (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ0.5) ã§äºˆæ¸¬ãƒ©ãƒ™ãƒ«ä½œæˆ
 3. æ­£è§£/ä¸æ­£è§£ã€éŽä¿¡/éŽå° (ç¢ºçŽ‡ä¿¡å¿µã«å¯¾ã™ã‚‹èª¤å·®) ã‚’è¨ˆç®—
 4. ã‚­ãƒ£ãƒªãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ (10åˆ†å‰²) & ECE (Expected Calibration Error) ã‚’ç®—å‡º
 5. ä¸Šä½: é«˜ç¢ºçŽ‡å¤–ã‚Œ (False Positive), ä½Žç¢ºçŽ‡å¤–ã‚Œ (False Negative) ã‚’æŠ½å‡º
 6. alignment ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã¸ summary, calibration, misclassified JSON ä¿å­˜

é™ç•Œ:
  - ç¾è¡Œãƒ©ãƒ™ãƒ«ã¯ã€Œç¾åœ¨æ™‚ç‚¹ã§ä¸€å®šæ—¥é–¾å€¤å†…ã«æœ€å¾Œã®æ´»å‹•ãŒã‚ã‚‹ã‹ã€ã®ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆæŒ‡æ¨™ã§ã‚ã‚Šã€
    æœªæ¥ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦è¦³æ¸¬ã«ã‚ˆã‚‹çœŸã®å°†æ¥ç¶™ç¶š (future engagement) ã‚’ç›´æŽ¥æ¤œè¨¼ã—ã¦ã„ã¾ã›ã‚“ã€‚
  - æœªæ¥ãƒ™ãƒ¼ã‚¹è©•ä¾¡ãŒå¿…è¦ãªå ´åˆã¯éŽåŽ»ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆæ™‚ç‚¹ã‚’å›ºå®šã—ç›´å¾Œ Î” æ—¥ã®æ´»å‹•æœ‰ç„¡ã‚’å†æ§‹ç¯‰ã™ã‚‹åˆ¥ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ãŒå¿…è¦ã§ã™ã€‚

ä½¿ç”¨ä¾‹:
  uv run scripts/analyze_retention_prediction_alignment.py --pred-file outputs/retention_probability/predictions.json --threshold 0.5
"""
from __future__ import annotations

import argparse
import json
import math
from pathlib import Path
from typing import Any, Dict, List

import numpy as np
from sklearn.metrics import (
    accuracy_score,
    brier_score_loss,
    f1_score,
    precision_score,
    recall_score,
    roc_auc_score,
)


def parse_args():
    ap = argparse.ArgumentParser(description="Workload-aware retention prediction alignment analysis")
    ap.add_argument('--pred-file', default='outputs/retention_probability/predictions.json')
    ap.add_argument('--prob-threshold', type=float, default=0.5, help='åˆ†é¡žç”¨é–¾å€¤')
    ap.add_argument('--calib-bins', type=int, default=10)
    ap.add_argument('--output-dir', default='outputs/retention_probability/alignment')
    return ap.parse_args()


def load_predictions(path: Path) -> List[Dict[str, Any]]:
    if not path.exists():
        raise FileNotFoundError(f"predictions file not found: {path}")
    return json.loads(path.read_text())


def calibration_bins(y: List[int], probs: List[float], n_bins: int):
    y_arr = np.asarray(y)
    p_arr = np.asarray(probs)
    edges = np.linspace(0,1,n_bins+1)
    bins = []
    ece = 0.0
    for i in range(n_bins):
        lo, hi = edges[i], edges[i+1]
        mask = (p_arr >= lo) & (p_arr < hi if i < n_bins-1 else p_arr <= hi)
        if mask.sum() == 0:
            bins.append({'bin': i,'lo': float(lo),'hi': float(hi),'count':0,'pred_mean':None,'true_rate':None,'gap':None})
            continue
        pred_mean = float(p_arr[mask].mean())
        true_rate = float(y_arr[mask].mean())
        gap = pred_mean - true_rate
        w = mask.mean()  # bin weight
        ece += abs(gap)*w
        bins.append({'bin': i,'lo': float(lo),'hi': float(hi),'count': int(mask.sum()),'pred_mean': pred_mean,'true_rate': true_rate,'gap': gap})
    return bins, float(ece)


def analyze(rows: List[Dict[str, Any]], threshold: float, n_bins: int):
    y = [int(r['label_retained']) for r in rows]
    probs = [float(r['prob_retained']) for r in rows]
    preds = [1 if p >= threshold else 0 for p in probs]
    metrics = {
        'count': len(rows),
        'threshold': threshold,
        'positive_rate': float(np.mean(y)),
        'accuracy': accuracy_score(y, preds),
        'precision': precision_score(y, preds, zero_division=0),
        'recall': recall_score(y, preds, zero_division=0),
        'f1': f1_score(y, preds, zero_division=0)
    }
    if len(set(y)) > 1:
        try: metrics['auc'] = roc_auc_score(y, probs)
        except: metrics['auc'] = None
        try: metrics['brier'] = brier_score_loss(y, probs)
        except: metrics['brier'] = None
    else:
        metrics['auc'] = None
        metrics['brier'] = None

    # misclassified analysis
    detailed = []
    for dev, label, prob, pred in zip(rows, y, probs, preds):
        correct = (label == pred)
        margin = abs(prob - threshold)
        err_mag = abs(prob - (1 if label==1 else 0))  # calibrationèª¤å·®
        detailed.append({
            'developer_id': dev.get('developer_id'),
            'label': label,
            'prob': prob,
            'pred': pred,
            'correct': correct,
            'margin_to_threshold': margin,
            'prob_error_vs_true': err_mag,
            'last_activity': dev.get('last_activity'),
            'changes_authored': dev.get('changes_authored'),
            'changes_reviewed': dev.get('changes_reviewed'),
        })

    false_pos = [d for d in detailed if d['pred']==1 and d['label']==0]
    false_neg = [d for d in detailed if d['pred']==0 and d['label']==1]
    # ã‚½ãƒ¼ãƒˆ: è‡ªä¿¡ã®é«˜ã„å¤–ã‚Œ (é–¾å€¤ã‹ã‚‰ã®è·é›¢ or èª¤å·®å¤§)
    false_pos.sort(key=lambda x: (-x['margin_to_threshold'], x['prob_error_vs_true']))
    false_neg.sort(key=lambda x: (-x['margin_to_threshold'], x['prob_error_vs_true']))
    top_false_pos = false_pos[:30]
    top_false_neg = false_neg[:30]

    calib, ece = calibration_bins(y, probs, n_bins)
    metrics['ece'] = ece

    return metrics, calib, top_false_pos, top_false_neg, detailed


def main():
    args = parse_args()
    rows = load_predictions(Path(args.pred_file))
    metrics, calib, fp, fn, detailed = analyze(rows, args.prob_threshold, args.calib_bins)

    out_dir = Path(args.output_dir)
    out_dir.mkdir(parents=True, exist_ok=True)
    (out_dir/'metrics_alignment.json').write_text(json.dumps(metrics, indent=2), encoding='utf-8')
    (out_dir/'calibration.json').write_text(json.dumps(calib, indent=2), encoding='utf-8')
    (out_dir/'top_false_positive.json').write_text(json.dumps(fp, indent=2), encoding='utf-8')
    (out_dir/'top_false_negative.json').write_text(json.dumps(fn, indent=2), encoding='utf-8')
    # å®Œå…¨è©³ç´°ã¯ã‚µã‚¤ã‚ºãŒå¤§ãã„å ´åˆãŒã‚ã‚‹ã®ã§å¿…è¦ãªã‚‰
    (out_dir/'detailed_predictions.json').write_text(json.dumps(detailed, indent=2), encoding='utf-8')

    print('âœ… Alignment Summary:')
    for k,v in metrics.items():
        print(f'  {k}: {v}')
    print(f'ðŸ’¾ Saved -> {out_dir}')
    if fp:
        print(f'  Top false positives: {len(fp)} (showing first 3)')
        for r in fp[:3]:
            print('   FP', r['developer_id'], 'prob', round(r['prob'],3), 'label', r['label'])
    if fn:
        print(f'  Top false negatives: {len(fn)} (showing first 3)')
        for r in fn[:3]:
            print('   FN', r['developer_id'], 'prob', round(r['prob'],3), 'label', r['label'])
    return 0

if __name__ == '__main__':
    raise SystemExit(main())
