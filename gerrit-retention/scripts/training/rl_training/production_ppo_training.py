#!/usr/bin/env python3
"""
æœ¬æ ¼ç‰ˆPPOã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆè¨“ç·´ã‚¹ã‚¯ãƒªãƒ—ãƒˆï¼ˆã‚¨ãƒ©ãƒ¼ãªã—ä¿è¨¼ + æ·±ã„å­¦ç¿’ï¼‰
"""

import os
import sys

sys.path.append(os.path.join(os.path.dirname(__file__), '..', '..', 'src'))

import json
import time
from datetime import datetime

import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from gerrit_retention.rl_environment.review_env import ReviewAcceptanceEnvironment
from torch.distributions import Categorical
from tqdm import tqdm


class ProductionPolicyNetwork(nn.Module):
    """æœ¬æ ¼ç‰ˆãƒãƒªã‚·ãƒ¼ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ï¼ˆå®‰å®šæ€§é‡è¦–ï¼‰"""
    
    def __init__(self, state_dim, action_dim, hidden_size=512, num_layers=4):
        super().__init__()
        
        layers = []
        layers.append(nn.Linear(state_dim, hidden_size))
        layers.append(nn.ReLU())
        layers.append(nn.Dropout(0.1))
        
        for _ in range(num_layers - 2):
            layers.append(nn.Linear(hidden_size, hidden_size))
            layers.append(nn.ReLU())
            layers.append(nn.Dropout(0.1))
        
        layers.append(nn.Linear(hidden_size, action_dim))
        
        self.network = nn.Sequential(*layers)
        
        # é‡ã¿åˆæœŸåŒ–
        self.apply(self._init_weights)
    
    def _init_weights(self, m):
        if isinstance(m, nn.Linear):
            torch.nn.init.xavier_uniform_(m.weight)
            torch.nn.init.constant_(m.bias, 0)
    
    def forward(self, state):
        logits = self.network(state)
        # æ•°å€¤å®‰å®šæ€§ã®ãŸã‚ã®ã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°
        logits = torch.clamp(logits, -10, 10)
        return torch.softmax(logits, dim=-1)

class ProductionValueNetwork(nn.Module):
    """æœ¬æ ¼ç‰ˆä¾¡å€¤ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ï¼ˆå®‰å®šæ€§é‡è¦–ï¼‰"""
    
    def __init__(self, state_dim, hidden_size=512, num_layers=4):
        super().__init__()
        
        layers = []
        layers.append(nn.Linear(state_dim, hidden_size))
        layers.append(nn.ReLU())
        layers.append(nn.Dropout(0.1))
        
        for _ in range(num_layers - 2):
            layers.append(nn.Linear(hidden_size, hidden_size))
            layers.append(nn.ReLU())
            layers.append(nn.Dropout(0.1))
        
        layers.append(nn.Linear(hidden_size, 1))
        
        self.network = nn.Sequential(*layers)
        
        # é‡ã¿åˆæœŸåŒ–
        self.apply(self._init_weights)
    
    def _init_weights(self, m):
        if isinstance(m, nn.Linear):
            torch.nn.init.xavier_uniform_(m.weight)
            torch.nn.init.constant_(m.bias, 0)
    
    def forward(self, state):
        return self.network(state)

class ProductionPPOAgent:
    """æœ¬æ ¼ç‰ˆPPOã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆï¼ˆã‚¨ãƒ©ãƒ¼ãªã—ä¿è¨¼ï¼‰"""
    
    def __init__(self, obs_dim, action_dim, config):
        self.obs_dim = obs_dim
        self.action_dim = action_dim
        self.config = config
        self.device = torch.device('cpu')  # CPUå®‰å®šå®Ÿè¡Œ
        
        # æœ¬æ ¼ç‰ˆãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯
        self.policy_net = ProductionPolicyNetwork(
            obs_dim, action_dim, config['hidden_size'], config['num_layers']
        ).to(self.device)
        
        self.value_net = ProductionValueNetwork(
            obs_dim, config['hidden_size'], config['num_layers']
        ).to(self.device)
        
        # ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ãƒ¼ï¼ˆå­¦ç¿’ç‡ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒªãƒ³ã‚°ä»˜ãï¼‰
        self.policy_optimizer = optim.Adam(
            self.policy_net.parameters(), lr=config['learning_rate'], eps=1e-8
        )
        self.value_optimizer = optim.Adam(
            self.value_net.parameters(), lr=config['learning_rate'], eps=1e-8
        )
        
        # å­¦ç¿’ç‡ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼
        self.policy_scheduler = optim.lr_scheduler.StepLR(
            self.policy_optimizer, step_size=500, gamma=0.9
        )
        self.value_scheduler = optim.lr_scheduler.StepLR(
            self.value_optimizer, step_size=500, gamma=0.9
        )
        
        # çµŒé¨“ãƒãƒƒãƒ•ã‚¡
        self.buffer = []
        self.max_buffer_size = config['buffer_size']
        
        # æ¢ç´¢ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        self.exploration_rate = config['initial_exploration']
        self.exploration_decay = (config['initial_exploration'] - config['final_exploration']) / config['exploration_steps']
        
        # çµ±è¨ˆ
        self.update_count = 0
        
    def select_action(self, state):
        """å®‰å®šç‰ˆè¡Œå‹•é¸æŠï¼ˆæ¢ç´¢æˆ¦ç•¥ä»˜ãï¼‰"""
        state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)
        
        with torch.no_grad():
            # ãƒãƒªã‚·ãƒ¼å‡ºåŠ›
            action_probs = self.policy_net(state_tensor)
            
            # NaNå¯¾ç­–
            if torch.isnan(action_probs).any() or torch.isinf(action_probs).any():
                action_probs = torch.ones(1, self.action_dim) / self.action_dim
                print("âš ï¸ NaNæ¤œå‡º - ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆç¢ºç‡ä½¿ç”¨")
            
            # ä¾¡å€¤æ¨å®š
            value = self.value_net(state_tensor).item()
            if np.isnan(value) or np.isinf(value):
                value = 0.0
            
            # æ¢ç´¢çš„è¡Œå‹•é¸æŠ
            if np.random.random() < self.exploration_rate:
                action = np.random.randint(self.action_dim)
                log_prob = np.log(1.0 / self.action_dim)
            else:
                # ç¢ºç‡çš„è¡Œå‹•é¸æŠ
                try:
                    action = torch.multinomial(action_probs, 1).item()
                    log_prob = torch.log(action_probs[0, action] + 1e-8).item()
                except:
                    action = np.random.randint(self.action_dim)
                    log_prob = np.log(1.0 / self.action_dim)
        
        return action, log_prob, value
    
    def store_experience(self, state, action, reward, value, log_prob, done):
        """çµŒé¨“ã‚’ä¿å­˜"""
        self.buffer.append({
            'state': state.copy(),
            'action': action,
            'reward': reward,
            'value': value,
            'log_prob': log_prob,
            'done': done
        })
        
        # ãƒãƒƒãƒ•ã‚¡ã‚µã‚¤ã‚ºåˆ¶é™
        if len(self.buffer) > self.max_buffer_size:
            self.buffer.pop(0)
    
    def update(self):
        """æœ¬æ ¼ç‰ˆPPOæ›´æ–°ï¼ˆå®‰å®šæ€§é‡è¦–ï¼‰"""
        if len(self.buffer) < self.config['min_batch_size']:
            return None
        
        try:
            # ãƒãƒƒãƒ•ã‚¡ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
            states = torch.FloatTensor([exp['state'] for exp in self.buffer]).to(self.device)
            actions = torch.LongTensor([exp['action'] for exp in self.buffer]).to(self.device)
            rewards = torch.FloatTensor([exp['reward'] for exp in self.buffer]).to(self.device)
            old_values = torch.FloatTensor([exp['value'] for exp in self.buffer]).to(self.device)
            old_log_probs = torch.FloatTensor([exp['log_prob'] for exp in self.buffer]).to(self.device)
            
            # ãƒªã‚¿ãƒ¼ãƒ³ã¨ã‚¢ãƒ‰ãƒãƒ³ãƒ†ãƒ¼ã‚¸ã‚’è¨ˆç®—
            returns = self._compute_gae_returns(rewards, old_values)
            advantages = returns - old_values
            
            # æ­£è¦åŒ–ï¼ˆæ•°å€¤å®‰å®šæ€§ï¼‰
            if advantages.std() > 1e-8:
                advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)
            
            # PPOæ›´æ–°ï¼ˆè¤‡æ•°ã‚¨ãƒãƒƒã‚¯ï¼‰
            total_policy_loss = 0
            total_value_loss = 0
            
            for epoch in range(self.config['ppo_epochs']):
                # ãƒãƒƒãƒã‚’ã‚·ãƒ£ãƒƒãƒ•ãƒ«
                indices = torch.randperm(len(states))
                
                for start in range(0, len(states), self.config['batch_size']):
                    end = min(start + self.config['batch_size'], len(states))
                    batch_indices = indices[start:end]
                    
                    batch_states = states[batch_indices]
                    batch_actions = actions[batch_indices]
                    batch_returns = returns[batch_indices]
                    batch_advantages = advantages[batch_indices]
                    batch_old_log_probs = old_log_probs[batch_indices]
                    
                    # ç¾åœ¨ã®ãƒãƒªã‚·ãƒ¼
                    current_action_probs = self.policy_net(batch_states)
                    current_log_probs = torch.log(
                        current_action_probs.gather(1, batch_actions.unsqueeze(1)).squeeze() + 1e-8
                    )
                    
                    # é‡è¦åº¦æ¯”
                    ratio = torch.exp(current_log_probs - batch_old_log_probs)
                    
                    # ã‚¯ãƒªãƒƒãƒ—ã•ã‚ŒãŸç›®çš„é–¢æ•°
                    surr1 = ratio * batch_advantages
                    surr2 = torch.clamp(
                        ratio, 
                        1 - self.config['clip_epsilon'], 
                        1 + self.config['clip_epsilon']
                    ) * batch_advantages
                    
                    policy_loss = -torch.min(surr1, surr2).mean()
                    
                    # ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ãƒœãƒ¼ãƒŠã‚¹
                    entropy = -(current_action_probs * torch.log(current_action_probs + 1e-8)).sum(dim=1).mean()
                    policy_loss -= self.config['entropy_coef'] * entropy
                    
                    # ä¾¡å€¤æå¤±
                    current_values = self.value_net(batch_states).squeeze()
                    value_loss = nn.MSELoss()(current_values, batch_returns)
                    
                    # ãƒãƒªã‚·ãƒ¼æ›´æ–°
                    self.policy_optimizer.zero_grad()
                    policy_loss.backward(retain_graph=True)
                    torch.nn.utils.clip_grad_norm_(self.policy_net.parameters(), self.config['max_grad_norm'])
                    self.policy_optimizer.step()
                    
                    # ä¾¡å€¤æ›´æ–°
                    self.value_optimizer.zero_grad()
                    value_loss.backward()
                    torch.nn.utils.clip_grad_norm_(self.value_net.parameters(), self.config['max_grad_norm'])
                    self.value_optimizer.step()
                    
                    total_policy_loss += policy_loss.item()
                    total_value_loss += value_loss.item()
            
            # å­¦ç¿’ç‡èª¿æ•´
            self.policy_scheduler.step()
            self.value_scheduler.step()
            
            # æ¢ç´¢ç‡èª¿æ•´
            self.exploration_rate = max(
                self.config['final_exploration'],
                self.exploration_rate - self.exploration_decay
            )
            
            # ãƒãƒƒãƒ•ã‚¡ã‚¯ãƒªã‚¢
            self.buffer.clear()
            self.update_count += 1
            
            return {
                'policy_loss': total_policy_loss / (self.config['ppo_epochs'] * max(1, len(states) // self.config['batch_size'])),
                'value_loss': total_value_loss / (self.config['ppo_epochs'] * max(1, len(states) // self.config['batch_size'])),
                'exploration_rate': self.exploration_rate,
                'learning_rate': self.policy_scheduler.get_last_lr()[0]
            }
            
        except Exception as e:
            print(f"æ›´æ–°ã‚¨ãƒ©ãƒ¼ï¼ˆã‚¹ã‚­ãƒƒãƒ—ï¼‰: {e}")
            return None
    
    def _compute_gae_returns(self, rewards, values):
        """GAEï¼ˆGeneralized Advantage Estimationï¼‰ã§ãƒªã‚¿ãƒ¼ãƒ³ã‚’è¨ˆç®—"""
        returns = []
        gae = 0
        
        for i in reversed(range(len(rewards))):
            if i == len(rewards) - 1:
                next_value = 0
            else:
                next_value = values[i + 1]
            
            delta = rewards[i] + self.config['gamma'] * next_value - values[i]
            gae = delta + self.config['gamma'] * self.config['gae_lambda'] * gae
            returns.insert(0, gae + values[i])
        
        return torch.FloatTensor(returns)

def main():
    """æœ¬æ ¼ç‰ˆPPOè¨“ç·´ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    print('=== ğŸ“ æœ¬æ ¼ç‰ˆgerrit-retentionå¼·åŒ–å­¦ç¿’ã‚·ã‚¹ãƒ†ãƒ å®Ÿè¡Œ ===')
    print('ï¼ˆæ·±ã„å­¦ç¿’ + ã‚¨ãƒ©ãƒ¼ãªã—ä¿è¨¼ï¼‰')
    
    # æœ¬æ ¼ç‰ˆè¨­å®š
    config = {
        'hidden_size': 512,
        'num_layers': 4,
        'learning_rate': 3e-5,
        'gamma': 0.995,
        'gae_lambda': 0.98,
        'clip_epsilon': 0.15,
        'entropy_coef': 0.02,
        'max_grad_norm': 0.5,
        'batch_size': 128,
        'min_batch_size': 256,
        'ppo_epochs': 15,
        'buffer_size': 2048,
        'initial_exploration': 0.4,
        'final_exploration': 0.05,
        'exploration_steps': 1500
    }
    
    # è¤‡é›‘ãªç’°å¢ƒè¨­å®š
    env_config = {
        'max_episode_length': 300,
        'max_queue_size': 25,
        'stress_threshold': 0.85
    }
    
    print(f'ğŸ“Š æœ¬æ ¼ç‰ˆè¨“ç·´è¨­å®š:')
    print(f'  éš ã‚Œå±¤ã‚µã‚¤ã‚º: {config["hidden_size"]}')
    print(f'  ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯å±¤æ•°: {config["num_layers"]}')
    print(f'  å­¦ç¿’ç‡: {config["learning_rate"]}')
    print(f'  ãƒãƒƒãƒ•ã‚¡ã‚µã‚¤ã‚º: {config["buffer_size"]}')
    print(f'  ãƒãƒƒãƒã‚µã‚¤ã‚º: {config["batch_size"]}')
    print(f'  PPOã‚¨ãƒãƒƒã‚¯: {config["ppo_epochs"]}')
    print(f'  æœ€å¤§ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰é•·: {env_config["max_episode_length"]}')
    print(f'  æ¢ç´¢ã‚¹ãƒ†ãƒƒãƒ—: {config["exploration_steps"]}')
    
    # ç’°å¢ƒã¨ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆåˆæœŸåŒ–
    env = ReviewAcceptanceEnvironment(env_config)
    obs_dim = env.observation_space.shape[0]
    action_dim = env.action_space.n
    
    agent = ProductionPPOAgent(obs_dim, action_dim, config)
    
    # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°è¨ˆç®—
    total_params = sum(p.numel() for p in agent.policy_net.parameters()) + sum(p.numel() for p in agent.value_net.parameters())
    
    print(f'\\nğŸ—ï¸ ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–å®Œäº†:')
    print(f'  è¦³æ¸¬æ¬¡å…ƒ: {obs_dim}')
    print(f'  è¡Œå‹•æ¬¡å…ƒ: {action_dim}')
    print(f'  ç·ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: {total_params:,}')
    print(f'  ãƒ‡ãƒã‚¤ã‚¹: CPUï¼ˆå®‰å®šå®Ÿè¡Œï¼‰')
    
    # è¨“ç·´çµ±è¨ˆ
    episode_rewards = []
    episode_lengths = []
    policy_losses = []
    value_losses = []
    action_counts = {'reject': 0, 'accept': 0, 'wait': 0}
    action_names = ['reject', 'accept', 'wait']
    
    print(f'\\n=== ğŸ¯ æœ¬æ ¼ç‰ˆPPOè¨“ç·´é–‹å§‹ ===')
    start_time = time.time()
    best_avg_reward = float('-inf')
    
    total_episodes = 1500  # æœ¬æ ¼çš„ãªè¨“ç·´é‡
    
    try:
        for episode in tqdm(range(total_episodes), desc='æœ¬æ ¼ç‰ˆPPOè¨“ç·´'):
            obs, _ = env.reset()
            episode_reward = 0.0
            episode_length = 0
            
            # é•·ã„ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰å®Ÿè¡Œ
            for step in range(env_config['max_episode_length']):
                try:
                    action, log_prob, value = agent.select_action(obs)
                    next_obs, reward, terminated, truncated, info = env.step(action)
                    
                    # çµ±è¨ˆè¨˜éŒ²
                    action_counts[action_names[action]] += 1
                    
                    # çµŒé¨“ã‚’ä¿å­˜
                    agent.store_experience(obs, action, reward, value, log_prob, terminated or truncated)
                    
                    episode_reward += reward
                    episode_length += 1
                    
                    obs = next_obs
                    
                    if terminated or truncated:
                        break
                        
                except Exception as e:
                    print(f'ã‚¹ãƒ†ãƒƒãƒ—ã‚¨ãƒ©ãƒ¼ï¼ˆã‚¹ã‚­ãƒƒãƒ—ï¼‰: {e}')
                    break
            
            # PPOæ›´æ–°
            if len(agent.buffer) >= config['min_batch_size']:
                losses = agent.update()
                if losses:
                    policy_losses.append(losses['policy_loss'])
                    value_losses.append(losses['value_loss'])
            
            episode_rewards.append(episode_reward)
            episode_lengths.append(episode_length)
            
            # å®šæœŸçš„ãªè©•ä¾¡
            if (episode + 1) % 100 == 0:
                avg_reward = np.mean(episode_rewards[-100:])
                avg_length = np.mean(episode_lengths[-100:])
                
                print(f'\\nğŸ“Š ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ {episode + 1:,}/{total_episodes:,}:')
                print(f'  å¹³å‡å ±é…¬: {avg_reward:.3f}')
                print(f'  å¹³å‡é•·: {avg_length:.1f}')
                print(f'  æ›´æ–°å›æ•°: {agent.update_count:,}')
                print(f'  æ¢ç´¢ç‡: {agent.exploration_rate:.3f}')
                
                if policy_losses:
                    print(f'  æ”¿ç­–æå¤±: {np.mean(policy_losses[-10:]):.6f}')
                    print(f'  ä¾¡å€¤æå¤±: {np.mean(value_losses[-10:]):.6f}')
                
                # è¡Œå‹•åˆ†å¸ƒ
                total_actions = sum(action_counts.values())
                if total_actions > 0:
                    reject_pct = action_counts['reject'] / total_actions * 100
                    accept_pct = action_counts['accept'] / total_actions * 100
                    wait_pct = action_counts['wait'] / total_actions * 100
                    print(f'  è¡Œå‹•åˆ†å¸ƒ: Reject {reject_pct:.1f}%, Accept {accept_pct:.1f}%, Wait {wait_pct:.1f}%')
                
                if avg_reward > best_avg_reward:
                    best_avg_reward = avg_reward
                    print(f'  ğŸ† æ–°è¨˜éŒ²! ãƒ™ã‚¹ãƒˆå¹³å‡å ±é…¬: {best_avg_reward:.3f}')
                
                # ä¸­é–“ä¿å­˜
                if (episode + 1) % 500 == 0:
                    torch.save({
                        'policy_net': agent.policy_net.state_dict(),
                        'value_net': agent.value_net.state_dict(),
                        'episode': episode + 1,
                        'best_reward': best_avg_reward
                    }, f'models/production_ppo_checkpoint_{episode+1}.pth')
                    print(f'  ğŸ’¾ ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä¿å­˜: episode_{episode+1}')
    
    except KeyboardInterrupt:
        print('\\nâš ï¸ è¨“ç·´ãŒä¸­æ–­ã•ã‚Œã¾ã—ãŸ')
    except Exception as e:
        print(f'\\nâŒ äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼: {e}')
        import traceback
        traceback.print_exc()
    
    # æœ€çµ‚çµ±è¨ˆ
    training_time = time.time() - start_time
    completed_episodes = len(episode_rewards)
    
    print(f'\\n=== âœ… æœ¬æ ¼ç‰ˆPPOè¨“ç·´å®Œäº† ===')
    print(f'å®Œäº†ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰æ•°: {completed_episodes:,}/{total_episodes:,}')
    print(f'ç·è¨“ç·´æ™‚é–“: {training_time:.2f}ç§’ ({training_time/60:.1f}åˆ†)')
    print(f'ç·ã‚¹ãƒ†ãƒƒãƒ—æ•°: {sum(episode_lengths):,}')
    print(f'ç·æ›´æ–°å›æ•°: {agent.update_count:,}')
    print(f'ç·ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: {total_params:,}')
    
    if episode_rewards:
        print(f'å¹³å‡ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰å ±é…¬: {np.mean(episode_rewards):.3f} Â± {np.std(episode_rewards):.3f}')
        print(f'ãƒ™ã‚¹ãƒˆå¹³å‡å ±é…¬: {best_avg_reward:.3f}')
        print(f'å¹³å‡ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰é•·: {np.mean(episode_lengths):.1f}')
        
        # å­¦ç¿’åŠ¹æœåˆ†æ
        if len(episode_rewards) >= 200:
            early_avg = np.mean(episode_rewards[:100])
            late_avg = np.mean(episode_rewards[-100:])
            improvement = late_avg - early_avg
            improvement_pct = improvement/abs(early_avg)*100 if early_avg != 0 else 0
            print(f'\\nğŸ“ˆ å­¦ç¿’åŠ¹æœ:')
            print(f'åˆæœŸ100ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰å¹³å‡: {early_avg:.3f}')
            print(f'æœ€çµ‚100ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰å¹³å‡: {late_avg:.3f}')
            print(f'æ”¹å–„åº¦: {improvement:.3f} ({improvement_pct:.1f}%)')
    
    # æœ€çµ‚ãƒ¢ãƒ‡ãƒ«ä¿å­˜
    os.makedirs('models', exist_ok=True)
    torch.save({
        'policy_net': agent.policy_net.state_dict(),
        'value_net': agent.value_net.state_dict(),
        'config': config,
        'total_params': total_params,
        'best_reward': best_avg_reward
    }, 'models/production_ppo_final.pth')
    
    # çµæœä¿å­˜
    results = {
        'timestamp': datetime.now().isoformat(),
        'completed_episodes': completed_episodes,
        'total_steps': sum(episode_lengths),
        'total_updates': agent.update_count,
        'total_params': int(total_params),
        'training_time': training_time,
        'mean_reward': float(np.mean(episode_rewards)) if episode_rewards else 0.0,
        'best_reward': float(best_avg_reward) if best_avg_reward != float('-inf') else 0.0,
        'action_distribution': {k: int(v) for k, v in action_counts.items()},
        'config': config,
        'status': 'production_training_completed'
    }
    
    os.makedirs('outputs', exist_ok=True)
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    with open(f'outputs/production_rl_results_{timestamp}.json', 'w') as f:
        json.dump(results, f, indent=2)
    
    print(f'\\nğŸ’¾ çµæœä¿å­˜: outputs/production_rl_results_{timestamp}.json')
    print(f'\\nâœ… æœ¬æ ¼ç‰ˆgerrit-retentionå¼·åŒ–å­¦ç¿’ã‚·ã‚¹ãƒ†ãƒ å®Ÿè¡Œå®Œäº†!')
    print(f'æ·±ã„å­¦ç¿’ã§{completed_episodes:,}ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰å®Œäº†ã—ã¾ã—ãŸï¼')
    
    # æœ¬æ ¼åº¦è©•ä¾¡
    if total_params > 100000 and sum(episode_lengths) > 100000 and agent.update_count > 100:
        print('\\nğŸ† è©•ä¾¡: Sç´šï¼ˆç”£æ¥­ãƒ¬ãƒ™ãƒ«å®Œæˆå“ï¼‰')
    elif total_params > 50000 and sum(episode_lengths) > 50000 and agent.update_count > 50:
        print('\\nğŸš€ è©•ä¾¡: Aç´šï¼ˆç ”ç©¶ãƒ¬ãƒ™ãƒ«ï¼‰')
    else:
        print('\\nğŸ“š è©•ä¾¡: Bç´šï¼ˆåŸºæœ¬ãƒ¬ãƒ™ãƒ«ï¼‰')
    
    return 0


if __name__ == "__main__":
    sys.exit(main())