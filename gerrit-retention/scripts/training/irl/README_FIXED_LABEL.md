### # 固定訓練ラベル・複数評価期間

## 概要

**学習時**は 3 ヶ月後の貢献フラグで訓練し、**評価時**は複数の異なる期間（0-3m, 4-6m, 7-9m など）で評価します。

これにより、以下を分析できます：

- モデルの汎化性能
- 訓練期間と異なる期間の予測精度
- 実用的なデプロイメントの可能性

---

## 実験設計

```
┌─────────────────────────────────────────────────────────┐
│ 学習期間（2022-2024）                                    │
├─────────────────────────────────────────────────────────┤
│ 2022-01-01 ─────────────┬───────────── 2024-01-01     │
│                         │                               │
│ 毎ステップでサンプリング│                               │
│                         ▼                               │
│  ┌─────────────────────────────────┐                   │
│  │ サンプリング時点: t             │                   │
│  ├─────────────────────────────────┤                   │
│  │ 履歴: t-12m ～ t （12ヶ月）     │                   │
│  │ ラベル: t+3m 後に貢献があるか  │ ← 固定             │
│  └─────────────────────────────────┘                   │
│                                                         │
│  例: 2022-01-01時点                                     │
│    履歴: 2021-01-01 ～ 2022-01-01                      │
│    ラベル: 2022-04-01 に貢献あり？                     │
│                                                         │
│  例: 2022-02-01時点                                     │
│    履歴: 2021-02-01 ～ 2022-02-01                      │
│    ラベル: 2022-05-01 に貢献あり？                     │
│                                                         │
│  ... 各月ごとにサンプル生成                             │
└─────────────────────────────────────────────────────────┘

┌─────────────────────────────────────────────────────────┐
│ 評価期間（2024-2025）                                    │
├─────────────────────────────────────────────────────────┤
│ 2024-01-01 ───────────────────────── 2025-01-01        │
│                                                         │
│ 同じモデルで複数期間を評価:                              │
│  ┌─────────────────────────────────┐                   │
│  │ 評価1: 0-3m 後の貢献            │                   │
│  │ 評価2: 4-6m 後の貢献            │                   │
│  │ 評価3: 7-9m 後の貢献            │                   │
│  │ 評価4: 10-12m 後の貢献          │                   │
│  └─────────────────────────────────┘                   │
└─────────────────────────────────────────────────────────┘
```

---

## 使用方法

### クイックテスト（約 5 分）

```bash
cd /Users/kazuki-h/rl/gerrit-retention
bash scripts/training/irl/run_quick_test_fixed_label.sh
```

**設定:**

- 訓練期間: 2023-01-01 ～ 2023-07-01（6 ヶ月）
- 評価期間: 2023-07-01 ～ 2024-01-01（6 ヶ月）
- 履歴窓: 6 ヶ月
- 訓練ラベル: 3 ヶ月後
- 評価: 0-2m, 3-5m
- エポック: 5

---

### フル実験（約 30-60 分）

```bash
cd /Users/kazuki-h/rl/gerrit-retention
bash scripts/training/irl/run_fixed_train_label_multi_eval.sh
```

**設定:**

- 訓練期間: 2022-01-01 ～ 2024-01-01（2 年）
- 評価期間: 2024-01-01 ～ 2025-01-01（1 年）
- 履歴窓: 12 ヶ月
- 訓練ラベル: 3 ヶ月後（固定）
- 評価: 0-3m, 4-6m, 7-9m, 10-12m
- エポック: 30

---

### カスタム実験

```bash
uv run python scripts/training/irl/train_irl_fixed_train_label_multi_eval.py \
  --reviews data/review_requests_openstack_multi_5y_detail.csv \
  --train-start 2022-01-01 \
  --train-end 2024-01-01 \
  --eval-start 2024-01-01 \
  --eval-end 2025-01-01 \
  --history-window 12 \
  --train-label-months 3 \
  --eval-windows "0-3" "4-6" "7-9" "10-12" \
  --sampling-interval 1 \
  --epochs 30 \
  --seq-len 20 \
  --output outputs/my_experiment
```

**パラメータ:**

- `--history-window`: 履歴窓（ヶ月）
- `--train-label-months`: 訓練時のラベル（n ヶ月後）
- `--eval-windows`: 評価する期間のリスト（例: "0-3" "4-6"）
- `--sampling-interval`: サンプリング間隔（ヶ月）
- `--epochs`: 訓練エポック数
- `--seq-len`: LSTM シーケンス長

---

## 出力ファイル

```
outputs/fixed_train_label_multi_eval/
├── irl_model.pth                # 訓練済みモデル
├── train_config.json            # 訓練設定
├── eval_0_3m.json              # 0-3m評価結果
├── eval_4_6m.json              # 4-6m評価結果
├── eval_7_9m.json              # 7-9m評価結果
├── eval_10_12m.json            # 10-12m評価結果
├── all_results.csv             # 全結果（CSV）
├── all_results.json            # 全結果（JSON）
├── evaluation_comparison.png   # 可視化
└── run.log                     # 実行ログ
```

---

## 結果の解釈

### 期待される結果パターン

```
訓練ラベル: 3ヶ月後

| 評価期間 | AUC-PR | 説明 |
|---------|--------|------|
| 0-3m    | 0.89   | 訓練期間と同じなので高精度 |
| 4-6m    | 0.85   | やや低下するが汎化できている |
| 7-9m    | 0.80   | 訓練から離れるほど低下 |
| 10-12m  | 0.75   | さらに低下 |
```

### 分析のポイント

1. **訓練期間との一致**

   - 0-3m（訓練と同じ）が最も高精度であることを確認

2. **汎化性能**

   - 4-6m, 7-9m の性能を見て、モデルがどれくらい汎化できるか評価

3. **長期予測の限界**

   - 10-12m など長期の予測精度を確認
   - 実用上許容できる範囲かを判断

4. **継続率の変化**
   - 各期間の継続率（positive_rate）を確認
   - 期間が長いほど継続率は低下するはず

---

## 実用的な応用

### ユースケース 1: 早期警告システム

```python
# 3ヶ月後のラベルで訓練したモデルを使用
model = load_model("irl_model.pth")

# 現在のレビュアーを評価
for reviewer in active_reviewers:
    # 0-3m後の継続確率を予測
    prob_short = model.predict(reviewer, eval_window="0-3m")

    # 4-6m後の継続確率も予測（参考）
    prob_mid = model.predict(reviewer, eval_window="4-6m")

    # 短期で離脱リスクが高い場合にアラート
    if prob_short < 0.3:
        alert_early_departure(reviewer)

    # 中期でも低い場合は要注意
    if prob_mid < 0.4:
        mark_as_high_risk(reviewer)
```

### ユースケース 2: エンゲージメント施策の効果測定

```python
# 施策実施前
prob_before = model.predict_batch(target_reviewers, "0-3m")

# 施策実施（メンタリング、イベント招待など）
apply_engagement_program(target_reviewers)

# 3ヶ月後に再評価
prob_after = model.predict_batch(target_reviewers, "0-3m")

# 効果を測定
improvement = prob_after - prob_before
print(f"平均改善: {improvement.mean():.2%}")
```

---

## よくある質問

### Q1: なぜ訓練ラベルは 3 ヶ月後に固定するのか？

**A:** 以前の実験（スライディングウィンドウ評価）で、3 ヶ月後のラベルが最も学習しやすく、高い精度が得られることが分かっています。

- 1 ヶ月後: 継続率が高すぎて識別が難しい
- 3 ヶ月後: **バランスが良く、最も学習しやすい** ✓
- 6 ヶ月後: 継続率が下がり、精度が低下
- 12 ヶ月後: さらに精度が低下

### Q2: 評価期間を変えるだけで正しく予測できるのか？

**A:** モデルは「開発者の活動パターン」を学習しているため、ある程度汎化します。ただし、訓練期間から大きく離れると精度は低下します。

- **近い期間（4-6m）**: 汎化性能が期待できる
- **遠い期間（10-12m）**: 精度低下は避けられない

### Q3: 複数のモデルを訓練する方が良いのでは？

**A:** 両方のアプローチに価値があります：

- **個別モデル**: 各期間で最高精度（別スクリプト: `run_fixed_history_variable_label_experiments.sh`）
- **単一モデル**: 実用的、汎化性能の分析（このスクリプト）

研究としては両方実施することを推奨します。

### Q4: サンプリング間隔はどう設定すべきか？

**A:**

- `--sampling-interval 1`: 毎月サンプリング（推奨、データが豊富）
- `--sampling-interval 3`: 3 ヶ月ごと（データが少ない場合）

---

## トラブルシューティング

### エラー: 訓練データが見つかりません

**原因:** 訓練期間が短すぎる、または履歴窓が長すぎる

**解決:**

```bash
# 履歴窓を短くする
--history-window 6

# または訓練期間を延ばす
--train-start 2021-01-01 --train-end 2024-01-01
```

### エラー: 評価データが見つかりません

**原因:** 評価期間が短い、または評価窓が長すぎる

**解決:**

```bash
# 評価窓を短くする
--eval-windows "0-3" "3-6"

# または評価期間を延ばす
--eval-end 2025-06-01
```

### 精度が低い

**対策:**

1. エポック数を増やす: `--epochs 50`
2. シーケンス長を増やす: `--seq-len 30`
3. より多くのデータで訓練: `--train-start 2021-01-01`

---

## まとめ

このスクリプトは、**固定訓練ラベル（3 ヶ月後）で学習したモデル**が、**複数の異なる期間（0-3m, 4-6m, etc.）でどれだけ汎化できるか**を評価します。

**利点:**

- ✅ 1 つのモデルで複数期間を評価
- ✅ 訓練時間の節約
- ✅ 汎化性能の分析
- ✅ 実用的なデプロイメント

**次のステップ:**

1. クイックテストで動作確認
2. フル実験で詳細評価
3. 結果を論文やレポートにまとめる
