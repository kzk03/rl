#!/usr/bin/env python3
"""
å°†æ¥ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦å‚ç…§å‹ ç¶™ç¶šäºˆæ¸¬è©•ä¾¡ã‚¹ã‚¯ãƒªãƒ—ãƒˆ (A + B å¯¾å¿œ)

ç›®çš„ (ãƒ¦ãƒ¼ã‚¶è¦æ±‚å¯¾å¿œ):
 A) ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆæ™‚ç‚¹ T ã®ç‰¹å¾´ã§ T+Î” æ—¥å†… (horizon) ã«æ´»å‹•ãŒã‚ã‚‹ã‹ã§ãƒ©ãƒ™ãƒ«ä»˜ã‘
 B) æ™‚ç³»åˆ—åˆ†å‰² (éå»ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆã§å­¦ç¿’ / ç›´è¿‘æœŸã§è©•ä¾¡) ã«ã‚ˆã‚Šãƒªãƒ¼ã‚¯ã‚’ä½æ¸›

ã‚¢ãƒ—ãƒ­ãƒ¼ãƒæ¦‚è¦:
 1. all_reviews.json (Gerrit changes) ã‹ã‚‰é–‹ç™ºè€…ã”ã¨ã®æ´»å‹•ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ç³»åˆ—ã‚’æŠ½å‡º
    - ç¾çŠ¶: change ã® owner.email ã‚’é–‹ç™ºè€…IDã¨ã¿ãªã— 'created' (å­˜åœ¨ã™ã‚Œã°) ã‚’æ´»å‹•ç™ºç”Ÿæ—¥ã¨ã—ã¦åˆ©ç”¨
 2. å„é–‹ç™ºè€…ã®æ´»å‹•ç³»åˆ—ã‹ã‚‰æœ€å¤§ N å€‹ ( --max-snapshots-per-dev ) ã®ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆæ—¥æ™‚ã‚’é¸æŠ (æœ«å°¾ã¯é™¤ã)
 3. å„ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆæ—¥æ™‚ snapshot_date ã«å¯¾ã— horizon_days å…ˆã¾ã§ã« >=1 æ´»å‹•ãŒã‚ã‚Œã° label=1 (ç¶™ç¶š), ãªã‘ã‚Œã° 0 (é›¢è„±)
 4. ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆæ™‚ç‚¹ã®ç‰¹å¾´é‡ã‚’ãƒªãƒ¼ã‚¯æœ€å°åŒ–ã®ãŸã‚è¿‘ä¼¼å†æ§‹æˆ:
      - all_developers.json ã®é›†è¨ˆå€¤ã‚’ã€ãã®æ™‚ç‚¹ã¾ã§ã®æ´»å‹•å‰²åˆã§ã‚¹ã‚±ãƒ¼ãƒ« (ç°¡æ˜“è¿‘ä¼¼)
      - review_scores ã¯å…ˆé ­ã‹ã‚‰å‰²åˆåˆ†ã‚’åˆ‡ã‚Šå‡ºã—
      - last_activity ã‚’ snapshot_date ã«ç½®æ›
    æ³¨æ„: å®Œå…¨ãªå±¥æ­´å†é›†è¨ˆãŒæœªæä¾›ãªãŸã‚è¿‘ä¼¼ (å°†æ¥æ”¹å–„ãƒã‚¤ãƒ³ãƒˆ)ã€‚
 5. å…¨ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆã‚’ snapshot_date æ˜‡é †ã«ä¸¦ã¹ã€å…ˆé ­ (1 - test_ratio) ã‚’è¨“ç·´ã€æ®‹ã‚Šã‚’ãƒ†ã‚¹ãƒˆ
 6. WorkloadAwarePredictor ã§å­¦ç¿’/æ¨è«–ã—ç²¾åº¦æŒ‡æ¨™ (accuracy / precision / recall / f1 / auc / brier / pos_rate) ã‚’ç®—å‡º
 7. å‡ºåŠ›: outputs/future_window_eval/{snapshots.csv, predictions.json, metrics.json}

åˆ¶ç´„ã¨ä»Šå¾Œã®æ”¹å–„ä½™åœ°:
 - é›†è¨ˆç‰¹å¾´ã‚’å‰²åˆã‚¹ã‚±ãƒ¼ãƒ«ã™ã‚‹è¿‘ä¼¼ã¯å°†æ¥ãƒ‡ãƒ¼ã‚¿ãƒªãƒ¼ã‚¯ã‚’å®Œå…¨ã«æ’é™¤ã—ãªã„å¯èƒ½æ€§ â†’ çœŸã®éå»é›†è¨ˆå†è¨ˆç®—å‡¦ç†ã¸æ‹¡å¼µäºˆå®š
 - review / insertion / deletion ã®æ­£ç¢ºãªæ™‚ç³»åˆ—å†…è¨³ãŒ changes ãƒ‡ãƒ¼ã‚¿ã«å«ã¾ã‚Œã‚‹å ´åˆã¯ç²¾ç·»åŒ–å¯èƒ½
 - ãƒ¡ãƒ¢ãƒªç¯€ç´„ã®ãŸã‚ all_reviews.json ã¯ JSON å…¨èª­ã¿ (ãƒ¡ãƒ¢ãƒªä¸è¶³æ™‚ã¯ç°¡æ˜“ã‚¹ãƒˆãƒªãƒ¼ãƒ å‡¦ç†ã«åˆ‡æ›¿)

ä½¿ç”¨ä¾‹:
  uv run python scripts/evaluate_future_window_retention.py \
      --developers data/processed/unified/all_developers.json \
      --changes data/processed/unified/all_reviews.json \
      --horizon-days 90 --max-snapshots-per-dev 4 --test-ratio 0.3

è»½é‡ãƒ†ã‚¹ãƒˆä¾‹:
  uv run python scripts/evaluate_future_window_retention.py --max-developers 20 --max-snapshots-per-dev 2 --horizon-days 30 --test-ratio 0.5
"""
from __future__ import annotations

import argparse
import json
import math
import sys
from dataclasses import dataclass
from datetime import datetime, timedelta, timezone
from pathlib import Path
from typing import Any, Dict, Iterable, List, Tuple

import numpy as np
from sklearn.metrics import (
    accuracy_score,
    brier_score_loss,
    f1_score,
    precision_score,
    recall_score,
    roc_auc_score,
)

# ãƒ‘ã‚¹è¿½åŠ 
ROOT = Path(__file__).resolve().parent.parent
if str(ROOT) not in sys.path:
    sys.path.append(str(ROOT))

from src.gerrit_retention.prediction.workload_aware_predictor import (  # noqa: E402
    WorkloadAwarePredictor,
)


@dataclass
class Snapshot:
    developer_id: str
    snapshot_date: datetime
    label: int
    developer_features: Dict[str, Any]


def parse_args():
    p = argparse.ArgumentParser(description="Future-window retention evaluation (time-series)")
    p.add_argument('--developers', default='data/processed/unified/all_developers.json')
    p.add_argument('--changes', default='data/processed/unified/all_reviews.json')
    p.add_argument('--project', default=None, help='If set, filter changes to this project only')
    p.add_argument('--horizon-days', type=int, default=90, help='Future window Î” (days)')
    p.add_argument('--max-snapshots-per-dev', type=int, default=5)
    p.add_argument('--min-activities', type=int, default=3, help='Minimum activity count for a developer to be considered')
    p.add_argument('--test-ratio', type=float, default=0.3, help='Proportion of latest snapshots for test (time ordered)')
    p.add_argument('--max-developers', type=int, default=0, help='Limit number of developers (0=all) for faster debug')
    p.add_argument('--random-seed', type=int, default=42)
    p.add_argument('--output-dir', default='outputs/future_window_eval')
    p.add_argument('--long-gap-days', type=int, default=0, help='If >0 and last activity gap >= this, add final snapshot forced negative')
    return p.parse_args()


def load_json(path: Path):
    with path.open('r', encoding='utf-8') as f:
        return json.load(f)


def parse_ts(ts: str) -> datetime | None:
    if not ts:
        return None
    try:
        ts = ts.replace('Z', '+00:00')
        # é•·ã™ãã‚‹ãƒŠãƒç§’éƒ¨ã‚’ãƒã‚¤ã‚¯ãƒ­ç§’ã«ä¸¸ã‚
        if '.' in ts:
            head, tail = ts.split('.', 1)
            if '+' in tail:
                frac, tz = tail.split('+', 1)
                if len(frac) > 6:
                    frac = frac[:6]
                ts = f"{head}.{frac}+{tz}"
            elif '-' in tail:
                frac, tz = tail.split('-', 1)
                if len(frac) > 6:
                    frac = frac[:6]
                ts = f"{head}.{frac}-{tz}"
            else:
                if len(tail) > 6:
                    ts = f"{head}.{tail[:6]}"
        dt = datetime.fromisoformat(ts)
        if dt.tzinfo is None:
            dt = dt.replace(tzinfo=timezone.utc)
        return dt.astimezone(timezone.utc)
    except Exception:
        return None


def build_activity_map(changes: List[Dict[str, Any]], max_developers: int = 0) -> Dict[str, List[datetime]]:
    """owner.email ã‚’ã‚­ãƒ¼ã« created æ—¥æ™‚ã‚’æ´»å‹•ã¨ã—ã¦é›†ç´„"""
    activity: Dict[str, List[datetime]] = {}
    for ch in changes:
        owner = ch.get('owner') or {}
        dev_id = owner.get('email') or owner.get('username')
        if not dev_id:
            continue
        created = ch.get('created') or ch.get('updated') or ch.get('submitted')
        dt = parse_ts(created)
        if not dt:
            continue
        activity.setdefault(dev_id, []).append(dt)
        # æ—©æœŸåˆ¶é™ (ã‚ªãƒ—ã‚·ãƒ§ãƒ³) - é©ç”¨ã¯å¾Œæ®µ (max_developers) ã§è¡Œã†ãŸã‚ã“ã“ã§ã¯ã—ãªã„
    # ã‚½ãƒ¼ãƒˆ
    for dev_id in activity:
        activity[dev_id].sort()
    if max_developers and len(activity) > max_developers:
        # å…ˆé ­ max_developers ã«åˆ¶é™ (alphabetical for determinism)
        selected_keys = sorted(activity.keys())[:max_developers]
        activity = {k: activity[k] for k in selected_keys}
    return activity


def choose_snapshot_indices(n: int, max_snapshots: int) -> List[int]:
    if n < 2:
        return []
    usable = n - 1  # æœ€å¾Œã¯æœªæ¥ãŒãªã„ã®ã§é™¤å¤–
    k = min(max_snapshots, usable)
    if k <= 0:
        return []
    if usable <= k:
        return list(range(usable))
    # å‡ç­‰ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
    positions = np.linspace(0, usable - 1, k)
    return sorted({int(round(p)) for p in positions})


def scale_developer_features(base_dev: Dict[str, Any], fraction: float, snapshot_date: datetime) -> Dict[str, Any]:
    """é›†è¨ˆç‰¹å¾´ã‚’ 'éå»å‰²åˆ' ã§ã‚¹ã‚±ãƒ¼ãƒ«ã™ã‚‹è¿‘ä¼¼ã€‚ fractionâˆˆ(0,1]"""
    fraction = max(0.0, min(1.0, fraction))
    dev = dict(base_dev)  # copy
    for key in ['changes_authored', 'changes_reviewed', 'total_insertions', 'total_deletions']:
        if key in dev and isinstance(dev[key], (int, float)):
            dev[key] = int(math.floor(dev[key] * fraction))
    # review_scores åˆ‡ã‚Šå‡ºã—
    rs = dev.get('review_scores')
    if isinstance(rs, list) and rs:
        cut = max(1, int(len(rs) * fraction))
        dev['review_scores'] = rs[:cut]
    # last_activity ã‚’ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆã«æ›¸ãæ›ãˆ
    dev['last_activity'] = snapshot_date.isoformat()
    return dev


def build_snapshots(
    developers: List[Dict[str, Any]],
    activity_map: Dict[str, List[datetime]],
    horizon_days: int,
    max_snapshots_per_dev: int,
    min_activities: int,
    long_gap_days: int = 0,
    dataset_end: datetime | None = None,
) -> List[Snapshot]:
    dev_index = {d.get('developer_id') or d.get('email'): d for d in developers}
    snapshots: List[Snapshot] = []
    horizon = timedelta(days=horizon_days)
    for dev_id, acts in activity_map.items():
        if len(acts) < min_activities:
            continue
        base_dev = dev_index.get(dev_id)
        if not base_dev:
            # é–‹ç™ºè€…ãƒã‚¹ã‚¿ãƒ¼ã«ç„¡ã„å ´åˆã‚¹ã‚­ãƒƒãƒ—
            continue
        idxs = choose_snapshot_indices(len(acts), max_snapshots_per_dev)
        total = len(acts)
        for idx in idxs:
            snap_date = acts[idx]
            future_end = snap_date + horizon
            # å°†æ¥æ´»å‹•åˆ¤å®š
            label = 0
            for future_act in acts[idx+1:]:
                if future_act <= snap_date:
                    continue
                if future_act <= future_end:
                    label = 1
                    break
                if future_act > future_end:
                    break
            fraction = (idx + 1) / total  # ãã®æ™‚ç‚¹ã¾ã§ã®æ´»å‹•å‰²åˆ (>=1 event)
            scaled_dev = scale_developer_features(base_dev, fraction, snap_date)
            snapshots.append(Snapshot(dev_id, snap_date, label, scaled_dev))
        # æœ«å°¾é•·æœŸã‚®ãƒ£ãƒƒãƒ—å¼·åˆ¶è² ä¾‹
        if long_gap_days and dataset_end:
            last_act = acts[-1]
            gap = (dataset_end - last_act).days
            if gap >= long_gap_days:
                fraction = 1.0
                scaled_dev = scale_developer_features(base_dev, fraction, last_act)
                snapshots.append(Snapshot(dev_id, last_act, 0, scaled_dev))
    return snapshots


def time_order_split(snapshots: List[Snapshot], test_ratio: float) -> Tuple[List[Snapshot], List[Snapshot]]:
    snapshots_sorted = sorted(snapshots, key=lambda s: s.snapshot_date)
    split = int(len(snapshots_sorted) * (1 - test_ratio))
    train = snapshots_sorted[:split]
    test = snapshots_sorted[split:]
    return train, test


def to_model_data(snaps: List[Snapshot]):
    return [s.developer_features for s in snaps], [s.label for s in snaps]


def compute_metrics(y_true, probs):
    preds = [1 if p >= 0.5 else 0 for p in probs]
    out = {
        'accuracy': accuracy_score(y_true, preds),
        'precision': precision_score(y_true, preds, zero_division=0),
        'recall': recall_score(y_true, preds, zero_division=0),
        'f1': f1_score(y_true, preds, zero_division=0),
        'positive_rate': float(np.mean(y_true))
    }
    if len(set(y_true)) > 1:
        try:
            out['auc'] = roc_auc_score(y_true, probs)
        except Exception:
            out['auc'] = None
        try:
            out['brier'] = brier_score_loss(y_true, probs)
        except Exception:
            out['brier'] = None
    else:
        out['auc'] = None
        out['brier'] = None
    return out


def main():  # noqa: C901 (è¤‡é›‘åº¦è¨±å®¹)
    args = parse_args()
    dev_path = Path(args.developers)
    ch_path = Path(args.changes)

    if not dev_path.exists():
        print(f"âŒ developers file not found: {dev_path}")
        return 1
    if not ch_path.exists():
        print(f"âŒ changes file not found: {ch_path}")
        return 1

    print(f"ğŸ“¥ Loading developers: {dev_path}")
    developers = load_json(dev_path)
    if args.max_developers:
        developers = developers[:args.max_developers]
    print(f"âœ… Developers loaded: {len(developers)}")

    print(f"ğŸ“¥ Loading changes (may take time): {ch_path}")
    try:
        changes_all = load_json(ch_path)
    except MemoryError:
        print("âš ï¸ MemoryError: file too large. Consider preprocessing to a smaller subset.")
        return 1
    # Optional project filter
    if args.project:
        changes = [c for c in changes_all if c.get('project') == args.project]
        print(f"âœ… Changes loaded: {len(changes_all)} (filtered by project='{args.project}' -> {len(changes)})")
        if not changes:
            print("âŒ No changes after project filter; aborting")
            return 1
    else:
        changes = changes_all
        print(f"âœ… Changes loaded: {len(changes)}")

    activity_map = build_activity_map(changes, max_developers=args.max_developers)
    print(f"ğŸ§ª Activity map built: developers with activity={len(activity_map)}")

    # dataset_end æ¨å®š (æ´»å‹•æœ€å¤§æ—¥æ™‚)
    all_last = []
    for acts in activity_map.values():
        if acts:
            all_last.append(acts[-1])
    dataset_end = max(all_last) if all_last else None

    snapshots = build_snapshots(
        developers, activity_map, args.horizon_days, args.max_snapshots_per_dev, args.min_activities,
        long_gap_days=args.long_gap_days, dataset_end=dataset_end
    )
    if not snapshots:
        print("âŒ No snapshots generated (check min-activities / data)")
        return 1
    print(f"ğŸ“¸ Generated snapshots: {len(snapshots)} (horizon={args.horizon_days}d)")

    train_snaps, test_snaps = time_order_split(snapshots, args.test_ratio)
    print(f"âœ‚ï¸ Time split: train={len(train_snaps)} test={len(test_snaps)} (ratio={args.test_ratio})")
    if len(train_snaps) < 5 or len(test_snaps) < 3:
        print("âš ï¸ Not enough snapshots for robust evaluation (need >=5 train & >=3 test). Try lowering --test-ratio, increasing --max-snapshots-per-dev, or removing --max-developers limit.")
    if not train_snaps or not test_snaps:
        print("âŒ Train/Test split invalid")
        return 1

    X_train, y_train = to_model_data(train_snaps)
    X_test, y_test = to_model_data(test_snaps)

    predictor = WorkloadAwarePredictor()
    # å˜ä¸€ã‚¯ãƒ©ã‚¹ã®å ´åˆã¯å­¦ç¿’ã›ãšå®šæ•°ç¢ºç‡
    unique_train = set(y_train)
    if len(unique_train) == 1:
        only_label = list(unique_train)[0]
        print(f"âš ï¸ Train set has single class={only_label}; using constant probability baseline")
        const_prob = 0.99 if only_label == 1 else 0.01
        probs = [const_prob] * len(X_test)
    else:
        predictor.fit(X_train, y_train)
        probs = predictor.predict_batch(X_test)
    metrics = compute_metrics(y_test, probs)
    # è¿½åŠ : ã‚¯ãƒ©ã‚¹åˆ†å¸ƒ
    def _class_counts(arr):
        return {int(k): int(v) for k, v in zip(*np.unique(arr, return_counts=True))}
    train_class_counts = _class_counts(y_train)
    test_class_counts = _class_counts(y_test)

    print("\nğŸ“Š Test Metrics")
    for k, v in metrics.items():
        print(f"  {k}: {v}")

    out_dir = Path(args.output_dir)
    out_dir.mkdir(parents=True, exist_ok=True)

    # ä¿å­˜: ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆä¸€è¦§ (CSV) ã¨äºˆæ¸¬
    import csv
    snap_csv = out_dir / 'snapshots.csv'
    with snap_csv.open('w', newline='', encoding='utf-8') as f:
        w = csv.writer(f)
        w.writerow(['developer_id','snapshot_date','label'])
        for s in snapshots:
            w.writerow([s.developer_id, s.snapshot_date.isoformat(), s.label])

    preds_json = []
    for s, p in zip(test_snaps, probs):
        preds_json.append({
            'developer_id': s.developer_id,
            'snapshot_date': s.snapshot_date.isoformat(),
            'label': s.label,
            'prob_retained': p
        })

    (out_dir / 'predictions.json').write_text(json.dumps(preds_json, ensure_ascii=False, indent=2), encoding='utf-8')
    meta = {
        'horizon_days': args.horizon_days,
        'max_snapshots_per_dev': args.max_snapshots_per_dev,
        'test_ratio': args.test_ratio,
        'total_snapshots': len(snapshots),
        'train_snapshots': len(train_snaps),
        'test_snapshots': len(test_snaps),
        'train_class_counts': train_class_counts,
        'test_class_counts': test_class_counts,
        'metrics': metrics,
        'approximation_warning': 'Feature scaling is an approximation; future leakage mitigated but not fully eliminated.'
    }
    (out_dir / 'metrics.json').write_text(json.dumps(meta, ensure_ascii=False, indent=2), encoding='utf-8')

    print(f"\nğŸ’¾ Saved snapshots: {snap_csv}")
    print(f"ğŸ’¾ Saved predictions: {out_dir / 'predictions.json'}")
    print(f"ğŸ’¾ Saved metrics: {out_dir / 'metrics.json'}")
    print("âœ… Done (future-window evaluation)")
    return 0


if __name__ == '__main__':
    raise SystemExit(main())
