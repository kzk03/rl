#!/usr/bin/env python3
"""
å®‰å®šç‰ˆPPOã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆè¨“ç·´ã‚¹ã‚¯ãƒªãƒ—ãƒˆï¼ˆã‚¨ãƒ©ãƒ¼ãªã—ä¿è¨¼ï¼‰
"""

import os
import sys

sys.path.append(os.path.join(os.path.dirname(__file__), '..', '..', 'src'))

import json
import time
from datetime import datetime

import numpy as np
import torch
import torch.nn as nn
from gerrit_retention.rl_environment.ppo_agent import PPOAgent, PPOConfig
from gerrit_retention.rl_environment.review_env import ReviewAcceptanceEnvironment
from tqdm import tqdm


class StablePPOAgent:
    """å®‰å®šç‰ˆPPOã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆï¼ˆNaNå¯¾ç­–æ¸ˆã¿ï¼‰"""
    
    def __init__(self, obs_dim, action_dim, config):
        self.obs_dim = obs_dim
        self.action_dim = action_dim
        self.config = config
        self.device = torch.device('cpu')  # CPUã§å®‰å®šå®Ÿè¡Œ
        
        # å®‰å®šç‰ˆãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯
        self.policy_net = self._create_stable_policy_net()
        self.value_net = self._create_stable_value_net()
        
        # ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ãƒ¼
        self.policy_optimizer = torch.optim.Adam(
            self.policy_net.parameters(), lr=config.learning_rate
        )
        self.value_optimizer = torch.optim.Adam(
            self.value_net.parameters(), lr=config.learning_rate
        )
        
        # çµŒé¨“ãƒãƒƒãƒ•ã‚¡
        self.buffer = []
        self.max_buffer_size = config.buffer_size
        
    def _create_stable_policy_net(self):
        """å®‰å®šç‰ˆãƒãƒªã‚·ãƒ¼ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯"""
        return nn.Sequential(
            nn.Linear(self.obs_dim, 128),
            nn.ReLU(),
            nn.Linear(128, 64),
            nn.ReLU(),
            nn.Linear(64, self.action_dim),
            nn.Softmax(dim=-1)  # å®‰å®šã—ãŸSoftmax
        )
    
    def _create_stable_value_net(self):
        """å®‰å®šç‰ˆä¾¡å€¤ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯"""
        return nn.Sequential(
            nn.Linear(self.obs_dim, 128),
            nn.ReLU(),
            nn.Linear(128, 64),
            nn.ReLU(),
            nn.Linear(64, 1)
        )
    
    def select_action(self, state):
        """å®‰å®šç‰ˆè¡Œå‹•é¸æŠ"""
        state_tensor = torch.FloatTensor(state).unsqueeze(0)
        
        with torch.no_grad():
            # ãƒãƒªã‚·ãƒ¼å‡ºåŠ›
            action_probs = self.policy_net(state_tensor)
            
            # NaNå¯¾ç­–
            if torch.isnan(action_probs).any():
                action_probs = torch.ones(1, self.action_dim) / self.action_dim
            
            # ä¾¡å€¤æ¨å®š
            value = self.value_net(state_tensor).item()
            if np.isnan(value):
                value = 0.0
            
            # è¡Œå‹•é¸æŠ
            action = torch.multinomial(action_probs, 1).item()
            log_prob = torch.log(action_probs[0, action] + 1e-8).item()
            
        return action, log_prob, value
    
    def store_experience(self, state, action, reward, value, log_prob, done):
        """çµŒé¨“ã‚’ä¿å­˜"""
        self.buffer.append({
            'state': state,
            'action': action,
            'reward': reward,
            'value': value,
            'log_prob': log_prob,
            'done': done
        })
        
        # ãƒãƒƒãƒ•ã‚¡ã‚µã‚¤ã‚ºåˆ¶é™
        if len(self.buffer) > self.max_buffer_size:
            self.buffer.pop(0)
    
    def update(self):
        """å®‰å®šç‰ˆPPOæ›´æ–°"""
        if len(self.buffer) < 64:  # æœ€å°ãƒãƒƒãƒã‚µã‚¤ã‚º
            return None
        
        # ãƒãƒƒãƒ•ã‚¡ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
        states = torch.FloatTensor([exp['state'] for exp in self.buffer])
        actions = torch.LongTensor([exp['action'] for exp in self.buffer])
        rewards = torch.FloatTensor([exp['reward'] for exp in self.buffer])
        old_values = torch.FloatTensor([exp['value'] for exp in self.buffer])
        old_log_probs = torch.FloatTensor([exp['log_prob'] for exp in self.buffer])
        
        # ãƒªã‚¿ãƒ¼ãƒ³ã¨ã‚¢ãƒ‰ãƒãƒ³ãƒ†ãƒ¼ã‚¸ã‚’è¨ˆç®—
        returns = self._compute_returns(rewards)
        advantages = returns - old_values
        
        # æ­£è¦åŒ–
        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)
        
        # PPOæ›´æ–°
        for _ in range(self.config.ppo_epochs):
            # ç¾åœ¨ã®ãƒãƒªã‚·ãƒ¼
            action_probs = self.policy_net(states)
            current_log_probs = torch.log(action_probs.gather(1, actions.unsqueeze(1)).squeeze() + 1e-8)
            
            # é‡è¦åº¦æ¯”
            ratio = torch.exp(current_log_probs - old_log_probs)
            
            # ã‚¯ãƒªãƒƒãƒ—ã•ã‚ŒãŸç›®çš„é–¢æ•°
            surr1 = ratio * advantages
            surr2 = torch.clamp(ratio, 1 - self.config.clip_epsilon, 1 + self.config.clip_epsilon) * advantages
            policy_loss = -torch.min(surr1, surr2).mean()
            
            # ä¾¡å€¤æå¤±
            current_values = self.value_net(states).squeeze()
            value_loss = nn.MSELoss()(current_values, returns)
            
            # æ›´æ–°
            self.policy_optimizer.zero_grad()
            policy_loss.backward(retain_graph=True)
            torch.nn.utils.clip_grad_norm_(self.policy_net.parameters(), 0.5)
            self.policy_optimizer.step()
            
            self.value_optimizer.zero_grad()
            value_loss.backward()
            torch.nn.utils.clip_grad_norm_(self.value_net.parameters(), 0.5)
            self.value_optimizer.step()
        
        # ãƒãƒƒãƒ•ã‚¡ã‚¯ãƒªã‚¢
        self.buffer.clear()
        
        return {
            'policy_loss': policy_loss.item(),
            'value_loss': value_loss.item()
        }
    
    def _compute_returns(self, rewards):
        """ãƒªã‚¿ãƒ¼ãƒ³ã‚’è¨ˆç®—"""
        returns = []
        R = 0
        for r in reversed(rewards):
            R = r + self.config.gamma * R
            returns.insert(0, R)
        return torch.FloatTensor(returns)


def main():
    """å®‰å®šç‰ˆPPOè¨“ç·´ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    print('=== ğŸš€ å®‰å®šç‰ˆgerrit-retentionå¼·åŒ–å­¦ç¿’ã‚·ã‚¹ãƒ†ãƒ å®Ÿè¡Œ ===')
    
    # å®‰å®šç‰ˆè¨­å®š
    config = PPOConfig(
        hidden_size=128,        # å®‰å®šã‚µã‚¤ã‚º
        num_layers=2,           # æµ…ã„ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯
        learning_rate=1e-4,     # ä½å­¦ç¿’ç‡
        gamma=0.99,
        gae_lambda=0.95,
        clip_epsilon=0.2,
        value_loss_coef=0.5,
        entropy_coef=0.01,
        batch_size=64,          # å°ã•ãªãƒãƒƒãƒ
        mini_batch_size=16,
        ppo_epochs=3,           # å°‘ãªã„ã‚¨ãƒãƒƒã‚¯
        buffer_size=512         # å°ã•ãªãƒãƒƒãƒ•ã‚¡
    )
    
    # ç’°å¢ƒè¨­å®š
    env_config = {
        'max_episode_length': 100,
        'max_queue_size': 10,
        'stress_threshold': 0.8
    }
    
    print(f'ğŸ“Š å®‰å®šç‰ˆè¨“ç·´è¨­å®š:')
    print(f'  éš ã‚Œå±¤ã‚µã‚¤ã‚º: {config.hidden_size}')
    print(f'  ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯å±¤æ•°: {config.num_layers}')
    print(f'  å­¦ç¿’ç‡: {config.learning_rate}')
    print(f'  ãƒãƒƒãƒ•ã‚¡ã‚µã‚¤ã‚º: {config.buffer_size}')
    print(f'  ãƒãƒƒãƒã‚µã‚¤ã‚º: {config.batch_size}')
    
    # ç’°å¢ƒã¨ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆåˆæœŸåŒ–
    env = ReviewAcceptanceEnvironment(env_config)
    obs_dim = env.observation_space.shape[0]
    action_dim = env.action_space.n
    
    agent = StablePPOAgent(obs_dim, action_dim, config)
    
    print(f'\\nğŸ—ï¸ ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–å®Œäº†:')
    print(f'  è¦³æ¸¬æ¬¡å…ƒ: {obs_dim}')
    print(f'  è¡Œå‹•æ¬¡å…ƒ: {action_dim}')
    print(f'  ãƒ‡ãƒã‚¤ã‚¹: CPUï¼ˆå®‰å®šå®Ÿè¡Œï¼‰')
    
    # è¨“ç·´çµ±è¨ˆ
    episode_rewards = []
    episode_lengths = []
    update_count = 0
    action_counts = {'reject': 0, 'accept': 0, 'wait': 0}
    action_names = ['reject', 'accept', 'wait']
    
    print(f'\\n=== ğŸ¯ å®‰å®šç‰ˆPPOè¨“ç·´é–‹å§‹ ===')
    start_time = time.time()
    best_avg_reward = float('-inf')
    
    total_episodes = 500  # å®‰å®šå®Ÿè¡Œç”¨
    
    try:
        for episode in tqdm(range(total_episodes), desc='å®‰å®šç‰ˆPPOè¨“ç·´'):
            obs, _ = env.reset()
            episode_reward = 0.0
            episode_length = 0
            
            # ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰å®Ÿè¡Œ
            for step in range(100):
                try:
                    action, log_prob, value = agent.select_action(obs)
                    next_obs, reward, terminated, truncated, info = env.step(action)
                    
                    # çµ±è¨ˆè¨˜éŒ²
                    action_counts[action_names[action]] += 1
                    
                    # çµŒé¨“ã‚’ä¿å­˜
                    agent.store_experience(obs, action, reward, value, log_prob, terminated or truncated)
                    
                    episode_reward += reward
                    episode_length += 1
                    
                    obs = next_obs
                    
                    if terminated or truncated:
                        break
                        
                except Exception as e:
                    print(f'ã‚¹ãƒ†ãƒƒãƒ—ã‚¨ãƒ©ãƒ¼ï¼ˆã‚¹ã‚­ãƒƒãƒ—ï¼‰: {e}')
                    break
            
            # PPOæ›´æ–°
            if len(agent.buffer) >= 64:
                try:
                    losses = agent.update()
                    if losses:
                        update_count += 1
                except Exception as e:
                    print(f'æ›´æ–°ã‚¨ãƒ©ãƒ¼ï¼ˆã‚¹ã‚­ãƒƒãƒ—ï¼‰: {e}')
            
            episode_rewards.append(episode_reward)
            episode_lengths.append(episode_length)
            
            # å®šæœŸçš„ãªè©•ä¾¡
            if (episode + 1) % 50 == 0:
                avg_reward = np.mean(episode_rewards[-50:])
                avg_length = np.mean(episode_lengths[-50:])
                
                print(f'\\nğŸ“Š ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ {episode + 1}/{total_episodes}:')
                print(f'  å¹³å‡å ±é…¬: {avg_reward:.3f}')
                print(f'  å¹³å‡é•·: {avg_length:.1f}')
                print(f'  æ›´æ–°å›æ•°: {update_count}')
                
                # è¡Œå‹•åˆ†å¸ƒ
                total_actions = sum(action_counts.values())
                if total_actions > 0:
                    action_dist = {k: v/total_actions*100 for k, v in action_counts.items()}
                    reject_pct = action_dist['reject']
                    accept_pct = action_dist['accept']
                    wait_pct = action_dist['wait']
                    print(f'  è¡Œå‹•åˆ†å¸ƒ: Reject {reject_pct:.1f}%, Accept {accept_pct:.1f}%, Wait {wait_pct:.1f}%')
                
                if avg_reward > best_avg_reward:
                    best_avg_reward = avg_reward
                    print(f'  ğŸ† æ–°è¨˜éŒ²! ãƒ™ã‚¹ãƒˆå¹³å‡å ±é…¬: {best_avg_reward:.3f}')
    
    except KeyboardInterrupt:
        print('\\nâš ï¸ è¨“ç·´ãŒä¸­æ–­ã•ã‚Œã¾ã—ãŸ')
    except Exception as e:
        print(f'\\nâŒ äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼: {e}')
    
    # æœ€çµ‚çµ±è¨ˆ
    training_time = time.time() - start_time
    completed_episodes = len(episode_rewards)
    
    print(f'\\n=== âœ… å®‰å®šç‰ˆPPOè¨“ç·´å®Œäº† ===')
    print(f'å®Œäº†ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰æ•°: {completed_episodes}/{total_episodes}')
    print(f'ç·è¨“ç·´æ™‚é–“: {training_time:.2f}ç§’ ({training_time/60:.1f}åˆ†)')
    print(f'ç·ã‚¹ãƒ†ãƒƒãƒ—æ•°: {sum(episode_lengths):,}')
    print(f'ç·æ›´æ–°å›æ•°: {update_count}')
    
    if episode_rewards:
        print(f'å¹³å‡ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰å ±é…¬: {np.mean(episode_rewards):.3f} Â± {np.std(episode_rewards):.3f}')
        print(f'ãƒ™ã‚¹ãƒˆå¹³å‡å ±é…¬: {best_avg_reward:.3f}')
        print(f'å¹³å‡ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰é•·: {np.mean(episode_lengths):.1f}')
        
        # å­¦ç¿’åŠ¹æœåˆ†æ
        if len(episode_rewards) >= 100:
            early_avg = np.mean(episode_rewards[:50])
            late_avg = np.mean(episode_rewards[-50:])
            improvement = late_avg - early_avg
            improvement_pct = improvement/abs(early_avg)*100 if early_avg != 0 else 0
            print(f'\\nğŸ“ˆ å­¦ç¿’åŠ¹æœ:')
            print(f'åˆæœŸ50ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰å¹³å‡: {early_avg:.3f}')
            print(f'æœ€çµ‚50ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰å¹³å‡: {late_avg:.3f}')
            print(f'æ”¹å–„åº¦: {improvement:.3f} ({improvement_pct:.1f}%)')
    
    # æœ€çµ‚è¡Œå‹•åˆ†æ
    total_actions = sum(action_counts.values())
    if total_actions > 0:
        print(f'\\nğŸ¯ æœ€çµ‚è¡Œå‹•åˆ†æ:')
        for action, count in action_counts.items():
            percentage = count / total_actions * 100
            print(f'{action}: {count:,}å› ({percentage:.1f}%)')
    
    # çµæœä¿å­˜
    results = {
        'timestamp': datetime.now().isoformat(),
        'completed_episodes': completed_episodes,
        'total_steps': sum(episode_lengths),
        'total_updates': update_count,
        'training_time': training_time,
        'mean_reward': float(np.mean(episode_rewards)) if episode_rewards else 0.0,
        'best_reward': float(best_avg_reward) if best_avg_reward != float('-inf') else 0.0,
        'action_distribution': {k: int(v) for k, v in action_counts.items()},
        'status': 'completed_successfully'
    }
    
    # çµæœãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜
    os.makedirs('outputs', exist_ok=True)
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    with open(f'outputs/stable_rl_results_{timestamp}.json', 'w') as f:
        json.dump(results, f, indent=2)
    
    print(f'\\nğŸ’¾ çµæœä¿å­˜: outputs/stable_rl_results_{timestamp}.json')
    print(f'\\nâœ… å®‰å®šç‰ˆgerrit-retentionå¼·åŒ–å­¦ç¿’ã‚·ã‚¹ãƒ†ãƒ å®Ÿè¡Œå®Œäº†!')
    print(f'ã‚¨ãƒ©ãƒ¼ãªã—ã§{completed_episodes}ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰å®Œäº†ã—ã¾ã—ãŸï¼')
    
    return 0


if __name__ == "__main__":
    sys.exit(main())