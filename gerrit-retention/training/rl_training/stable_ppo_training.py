#!/usr/bin/env python3
"""
å®‰å®šç‰ˆPPOã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆè¨“ç·´ã‚¹ã‚¯ãƒªãƒ—ãƒˆï¼ˆã‚¨ãƒ©ãƒ¼ãªã—ä¿è¨¼ï¼‰
"""

import os
import sys

sys.path.append(os.path.join(os.path.dirname(__file__), '..', '..', 'src'))

import json
import time
from datetime import datetime

import numpy as np
import torch
import torch.nn as nn
from tqdm import tqdm

from gerrit_retention.rl_environment.irl_reward_wrapper import IRLRewardWrapper
from gerrit_retention.rl_environment.ppo_agent import PPOAgent, PPOConfig
from gerrit_retention.rl_environment.review_env import ReviewAcceptanceEnvironment
from gerrit_retention.rl_environment.time_split_data_wrapper import TimeSplitDataWrapper
from gerrit_retention.rl_prediction.retention_irl_system import RetentionIRLSystem


class StablePPOAgent:
    """å®‰å®šç‰ˆPPOã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆï¼ˆNaNå¯¾ç­–æ¸ˆã¿ï¼‰"""
    
    def __init__(self, obs_dim, action_dim, config):
        self.obs_dim = obs_dim
        self.action_dim = action_dim
        self.config = config
        self.device = torch.device('cpu')  # CPUã§å®‰å®šå®Ÿè¡Œ
        
        # å®‰å®šç‰ˆãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯
        self.policy_net = self._create_stable_policy_net()
        self.value_net = self._create_stable_value_net()
        
        # ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ãƒ¼
        self.policy_optimizer = torch.optim.Adam(
            self.policy_net.parameters(), lr=config.learning_rate
        )
        self.value_optimizer = torch.optim.Adam(
            self.value_net.parameters(), lr=config.learning_rate
        )
        
        # çµŒé¨“ãƒãƒƒãƒ•ã‚¡
        self.buffer = []
        self.max_buffer_size = config.buffer_size
        
    def _create_stable_policy_net(self):
        """å®‰å®šç‰ˆãƒãƒªã‚·ãƒ¼ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯"""
        return nn.Sequential(
            nn.Linear(self.obs_dim, 128),
            nn.ReLU(),
            nn.Linear(128, 64),
            nn.ReLU(),
            nn.Linear(64, self.action_dim),
            nn.Softmax(dim=-1)  # å®‰å®šã—ãŸSoftmax
        )
    
    def _create_stable_value_net(self):
        """å®‰å®šç‰ˆä¾¡å€¤ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯"""
        return nn.Sequential(
            nn.Linear(self.obs_dim, 128),
            nn.ReLU(),
            nn.Linear(128, 64),
            nn.ReLU(),
            nn.Linear(64, 1)
        )
    
    def select_action(self, state):
        """å®‰å®šç‰ˆè¡Œå‹•é¸æŠ"""
        state_tensor = torch.FloatTensor(state).unsqueeze(0)
        
        with torch.no_grad():
            # ãƒãƒªã‚·ãƒ¼å‡ºåŠ›
            action_probs = self.policy_net(state_tensor)
            
            # NaNå¯¾ç­–
            if torch.isnan(action_probs).any():
                action_probs = torch.ones(1, self.action_dim) / self.action_dim
            
            # ä¾¡å€¤æ¨å®š
            value = self.value_net(state_tensor).item()
            if np.isnan(value):
                value = 0.0
            
            # è¡Œå‹•é¸æŠ
            action = torch.multinomial(action_probs, 1).item()
            log_prob = torch.log(action_probs[0, action] + 1e-8).item()
            
        return action, log_prob, value
    
    def store_experience(self, state, action, reward, value, log_prob, done):
        """çµŒé¨“ã‚’ä¿å­˜"""
        self.buffer.append({
            'state': state,
            'action': action,
            'reward': reward,
            'value': value,
            'log_prob': log_prob,
            'done': done
        })
        
        # ãƒãƒƒãƒ•ã‚¡ã‚µã‚¤ã‚ºåˆ¶é™
        if len(self.buffer) > self.max_buffer_size:
            self.buffer.pop(0)
    
    def update(self):
        """å®‰å®šç‰ˆPPOæ›´æ–°"""
        if len(self.buffer) < 64:  # æœ€å°ãƒãƒƒãƒã‚µã‚¤ã‚º
            return None
        
        # ãƒãƒƒãƒ•ã‚¡ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
        states = torch.FloatTensor([exp['state'] for exp in self.buffer])
        actions = torch.LongTensor([exp['action'] for exp in self.buffer])
        rewards = torch.FloatTensor([exp['reward'] for exp in self.buffer])
        old_values = torch.FloatTensor([exp['value'] for exp in self.buffer])
        old_log_probs = torch.FloatTensor([exp['log_prob'] for exp in self.buffer])
        
        # ãƒªã‚¿ãƒ¼ãƒ³ã¨ã‚¢ãƒ‰ãƒãƒ³ãƒ†ãƒ¼ã‚¸ã‚’è¨ˆç®—
        returns = self._compute_returns(rewards)
        advantages = returns - old_values
        
        # æ­£è¦åŒ–
        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)
        
        # PPOæ›´æ–°
        for _ in range(self.config.ppo_epochs):
            # ç¾åœ¨ã®ãƒãƒªã‚·ãƒ¼
            action_probs = self.policy_net(states)
            current_log_probs = torch.log(action_probs.gather(1, actions.unsqueeze(1)).squeeze() + 1e-8)
            
            # é‡è¦åº¦æ¯”
            ratio = torch.exp(current_log_probs - old_log_probs)
            
            # ã‚¯ãƒªãƒƒãƒ—ã•ã‚ŒãŸç›®çš„é–¢æ•°
            surr1 = ratio * advantages
            surr2 = torch.clamp(ratio, 1 - self.config.clip_epsilon, 1 + self.config.clip_epsilon) * advantages
            policy_loss = -torch.min(surr1, surr2).mean()
            
            # ä¾¡å€¤æå¤±
            current_values = self.value_net(states).squeeze()
            value_loss = nn.MSELoss()(current_values, returns)
            
            # æ›´æ–°
            self.policy_optimizer.zero_grad()
            policy_loss.backward(retain_graph=True)
            torch.nn.utils.clip_grad_norm_(self.policy_net.parameters(), 0.5)
            self.policy_optimizer.step()
            
            self.value_optimizer.zero_grad()
            value_loss.backward()
            torch.nn.utils.clip_grad_norm_(self.value_net.parameters(), 0.5)
            self.value_optimizer.step()
        
        # ãƒãƒƒãƒ•ã‚¡ã‚¯ãƒªã‚¢
        self.buffer.clear()
        
        return {
            'policy_loss': policy_loss.item(),
            'value_loss': value_loss.item()
        }
    
    def _compute_returns(self, rewards):
        """ãƒªã‚¿ãƒ¼ãƒ³ã‚’è¨ˆç®—"""
        returns = []
        R = 0
        for r in reversed(rewards):
            R = r + self.config.gamma * R
            returns.insert(0, R)
        return torch.FloatTensor(returns)


def main():
    """å®‰å®šç‰ˆPPOè¨“ç·´ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    print('=== ğŸš€ å®‰å®šç‰ˆgerrit-retentionå¼·åŒ–å­¦ç¿’ã‚·ã‚¹ãƒ†ãƒ å®Ÿè¡Œ ===')
    
    # å®‰å®šç‰ˆè¨­å®š
    config = PPOConfig(
        hidden_size=128,        # å®‰å®šã‚µã‚¤ã‚º
        num_layers=2,           # æµ…ã„ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯
        learning_rate=1e-4,     # ä½å­¦ç¿’ç‡
        gamma=0.99,
        gae_lambda=0.95,
        clip_epsilon=0.2,
        value_loss_coef=0.5,
        entropy_coef=0.01,
        batch_size=64,          # å°ã•ãªãƒãƒƒãƒ
        mini_batch_size=16,
        ppo_epochs=3,           # å°‘ãªã„ã‚¨ãƒãƒƒã‚¯
        buffer_size=512         # å°ã•ãªãƒãƒƒãƒ•ã‚¡
    )
    
    # ç’°å¢ƒè¨­å®šï¼ˆIRL å ±é…¬ãƒ©ãƒƒãƒ‘ãƒ¼ã‚’ä»»æ„ã§æœ‰åŠ¹åŒ–ã§ãã‚‹ï¼‰
    env_config = {
        'max_episode_length': 100,
        'max_queue_size': 10,
        'stress_threshold': 0.8,
    # å®Ÿãƒ‡ãƒ¼ã‚¿ã®ã¿ã‚’ä½¿ã†ãŸã‚ã®ãƒ©ãƒ³ãƒ€ãƒ ç”ŸæˆOFF
    'use_random_initial_queue': False,
    'enable_random_new_reviews': False,
        # è¿½åŠ ã‚ªãƒ—ã‚·ãƒ§ãƒ³
        'use_irl_reward': False,            # True ã«ã™ã‚‹ã¨ IRL å ±é…¬ã‚’ä½¿ç”¨
        'irl_reward_mode': 'blend',         # 'replace' or 'blend'
        'irl_reward_alpha': 0.7,            # blend ä¿‚æ•°
        'irl_model_path': None,             # æ—¢å­˜ IRL ãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ã‚¹ï¼ˆä»»æ„ï¼‰
    # é–‹ç™ºè€…ãŒã€Œãƒ¬ãƒ“ãƒ¥ãƒ¼ã—ã¦ãã‚Œã‚‹ï¼ˆå—è«¾ï¼‰ã€ã“ã¨ã¸ã®ãƒœãƒ¼ãƒŠã‚¹
    'engagement_bonus_weight': 0.0,     # >0 ã«ã™ã‚‹ã¨å—è«¾æ™‚ã«ãƒœãƒ¼ãƒŠã‚¹åŠ ç®—
    'accept_action_id': 1,              # å—è«¾è¡Œå‹•IDï¼ˆç’°å¢ƒã®å®šç¾©ã«åˆã‚ã›ã‚‹ï¼‰
    }
    
    print(f'ğŸ“Š å®‰å®šç‰ˆè¨“ç·´è¨­å®š:')
    print(f'  éš ã‚Œå±¤ã‚µã‚¤ã‚º: {config.hidden_size}')
    print(f'  ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯å±¤æ•°: {config.num_layers}')
    print(f'  å­¦ç¿’ç‡: {config.learning_rate}')
    print(f'  ãƒãƒƒãƒ•ã‚¡ã‚µã‚¤ã‚º: {config.buffer_size}')
    print(f'  ãƒãƒƒãƒã‚µã‚¤ã‚º: {config.batch_size}')
    
    # ç’°å¢ƒã¨ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆåˆæœŸåŒ–
    env = ReviewAcceptanceEnvironment(env_config)
    # æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿åˆ†å‰²ï¼ˆä»»æ„ï¼‰: extended_test_data.json ã¨ cutoff ã‚’ä½¿ã£ã¦ä¾›çµ¦
    time_split_enabled = True
    cutoff_iso = os.environ.get('STABLE_RL_CUTOFF', '2023-04-01T00:00:00Z')
    data_path = os.environ.get('STABLE_RL_DATA', os.path.join('data', 'extended_test_data.json'))
    if time_split_enabled:
        env = TimeSplitDataWrapper(env, data_path=data_path, cutoff_iso=cutoff_iso, phase='train')

    # IRL å ±é…¬ãƒ©ãƒƒãƒ—ï¼ˆä»»æ„ï¼‰
    if env_config.get('use_irl_reward'):
        irl_cfg = {
            'state_dim': env.observation_space.shape[0],
            'action_dim': env.action_space.n,
            'hidden_dim': 128,
            'learning_rate': 1e-3,
        }
        irl = RetentionIRLSystem(irl_cfg)
        model_path = env_config.get('irl_model_path')
        if model_path and os.path.exists(model_path):
            try:
                irl.load_model(model_path)
                print(f"IRL ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ: {model_path}")
            except Exception as e:
                print(f"IRL ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸï¼ˆç¶™ç¶šï¼‰: {e}")
        env = IRLRewardWrapper(
            env,
            irl_system=irl,
            mode=str(env_config.get('irl_reward_mode', 'blend')),
            alpha=float(env_config.get('irl_reward_alpha', 0.7)),
            engagement_bonus_weight=float(env_config.get('engagement_bonus_weight', 0.0)),
            accept_action_id=int(env_config.get('accept_action_id', 1)),
        )
    obs_dim = env.observation_space.shape[0]
    action_dim = env.action_space.n
    
    agent = StablePPOAgent(obs_dim, action_dim, config)
    
    print(f'\\nğŸ—ï¸ ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–å®Œäº†:')
    print(f'  è¦³æ¸¬æ¬¡å…ƒ: {obs_dim}')
    print(f'  è¡Œå‹•æ¬¡å…ƒ: {action_dim}')
    print(f'  ãƒ‡ãƒã‚¤ã‚¹: CPUï¼ˆå®‰å®šå®Ÿè¡Œï¼‰')
    
    # è¨“ç·´çµ±è¨ˆ
    episode_rewards = []
    episode_lengths = []
    update_count = 0
    action_counts = {'reject': 0, 'accept': 0, 'wait': 0}
    action_names = ['reject', 'accept', 'wait']
    
    print(f'\\n=== ğŸ¯ å®‰å®šç‰ˆPPOè¨“ç·´é–‹å§‹ ===')
    start_time = time.time()
    best_avg_reward = float('-inf')
    
    # è¨“ç·´/è©•ä¾¡ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰æ•°ï¼ˆæ“¬ä¼¼ãƒ‡ãƒ¼ã‚¿åˆ†å‰²: ã‚·ãƒ¼ãƒ‰ã‚’å¤‰ãˆã¦åˆ¥åˆ†å¸ƒã‚’æ¨¡æ“¬ï¼‰
    train_episodes = 400
    eval_episodes = 100
    train_seed = 42
    eval_seed = 4242
    total_episodes = train_episodes
    
    try:
        for episode in tqdm(range(total_episodes), desc='å®‰å®šç‰ˆPPOè¨“ç·´'):
            # è¨“ç·´ç”¨ã‚·ãƒ¼ãƒ‰ï¼ˆã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã”ã¨ã«ãšã‚‰ã™ï¼‰
            obs, _ = env.reset(seed=train_seed + episode)
            episode_reward = 0.0
            episode_length = 0
            
            # ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰å®Ÿè¡Œ
            for step in range(100):
                try:
                    action, log_prob, value = agent.select_action(obs)
                    next_obs, reward, terminated, truncated, info = env.step(action)
                    
                    # çµ±è¨ˆè¨˜éŒ²
                    action_counts[action_names[action]] += 1
                    
                    # çµŒé¨“ã‚’ä¿å­˜
                    agent.store_experience(obs, action, reward, value, log_prob, terminated or truncated)
                    
                    episode_reward += reward
                    episode_length += 1
                    
                    obs = next_obs
                    
                    if terminated or truncated:
                        break
                        
                except Exception as e:
                    print(f'ã‚¹ãƒ†ãƒƒãƒ—ã‚¨ãƒ©ãƒ¼ï¼ˆã‚¹ã‚­ãƒƒãƒ—ï¼‰: {e}')
                    break
            
            # PPOæ›´æ–°
            if len(agent.buffer) >= 64:
                try:
                    losses = agent.update()
                    if losses:
                        update_count += 1
                except Exception as e:
                    print(f'æ›´æ–°ã‚¨ãƒ©ãƒ¼ï¼ˆã‚¹ã‚­ãƒƒãƒ—ï¼‰: {e}')
            
            episode_rewards.append(episode_reward)
            episode_lengths.append(episode_length)
            
            # å®šæœŸçš„ãªè©•ä¾¡
            if (episode + 1) % 50 == 0:
                avg_reward = np.mean(episode_rewards[-50:])
                avg_length = np.mean(episode_lengths[-50:])
                
                print(f'\\nğŸ“Š ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ {episode + 1}/{total_episodes}:')
                print(f'  å¹³å‡å ±é…¬: {avg_reward:.3f}')
                print(f'  å¹³å‡é•·: {avg_length:.1f}')
                print(f'  æ›´æ–°å›æ•°: {update_count}')
                
                # è¡Œå‹•åˆ†å¸ƒ
                total_actions = sum(action_counts.values())
                if total_actions > 0:
                    action_dist = {k: v/total_actions*100 for k, v in action_counts.items()}
                    reject_pct = action_dist['reject']
                    accept_pct = action_dist['accept']
                    wait_pct = action_dist['wait']
                    print(f'  è¡Œå‹•åˆ†å¸ƒ: Reject {reject_pct:.1f}%, Accept {accept_pct:.1f}%, Wait {wait_pct:.1f}%')
                
                if avg_reward > best_avg_reward:
                    best_avg_reward = avg_reward
                    print(f'  ğŸ† æ–°è¨˜éŒ²! ãƒ™ã‚¹ãƒˆå¹³å‡å ±é…¬: {best_avg_reward:.3f}')
    
    except KeyboardInterrupt:
        print('\\nâš ï¸ è¨“ç·´ãŒä¸­æ–­ã•ã‚Œã¾ã—ãŸ')
    except Exception as e:
        print(f'\\nâŒ äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼: {e}')
    
    # æœ€çµ‚çµ±è¨ˆ
    training_time = time.time() - start_time
    completed_episodes = len(episode_rewards)
    
    print(f'\\n=== âœ… å®‰å®šç‰ˆPPOè¨“ç·´å®Œäº† ===')
    print(f'å®Œäº†ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰æ•°: {completed_episodes}/{total_episodes}')
    print(f'ç·è¨“ç·´æ™‚é–“: {training_time:.2f}ç§’ ({training_time/60:.1f}åˆ†)')
    print(f'ç·ã‚¹ãƒ†ãƒƒãƒ—æ•°: {sum(episode_lengths):,}')
    print(f'ç·æ›´æ–°å›æ•°: {update_count}')
    
    if episode_rewards:
        print(f'å¹³å‡ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰å ±é…¬: {np.mean(episode_rewards):.3f} Â± {np.std(episode_rewards):.3f}')
        print(f'ãƒ™ã‚¹ãƒˆå¹³å‡å ±é…¬: {best_avg_reward:.3f}')
        print(f'å¹³å‡ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰é•·: {np.mean(episode_lengths):.1f}')
        
        # å­¦ç¿’åŠ¹æœåˆ†æ
        if len(episode_rewards) >= 100:
            early_avg = np.mean(episode_rewards[:50])
            late_avg = np.mean(episode_rewards[-50:])
            improvement = late_avg - early_avg
            improvement_pct = improvement/abs(early_avg)*100 if early_avg != 0 else 0
            print(f'\\nğŸ“ˆ å­¦ç¿’åŠ¹æœ:')
            print(f'åˆæœŸ50ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰å¹³å‡: {early_avg:.3f}')
            print(f'æœ€çµ‚50ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰å¹³å‡: {late_avg:.3f}')
            print(f'æ”¹å–„åº¦: {improvement:.3f} ({improvement_pct:.1f}%)')
    
    # æœ€çµ‚è¡Œå‹•åˆ†æ
    total_actions = sum(action_counts.values())
    if total_actions > 0:
        print(f'\\nğŸ¯ æœ€çµ‚è¡Œå‹•åˆ†æ:')
        for action, count in action_counts.items():
            percentage = count / total_actions * 100
            print(f'{action}: {count:,}å› ({percentage:.1f}%)')
    
    # =============================
    # è©•ä¾¡ãƒ•ã‚§ãƒ¼ã‚ºï¼ˆå­¦ç¿’åœæ­¢ãƒ»è²ªæ¬²æ–¹ç­–ï¼‰
    # =============================
    print('\n=== ğŸ§ª è©•ä¾¡ãƒ•ã‚§ãƒ¼ã‚ºé–‹å§‹ï¼ˆå­¦ç¿’åœæ­¢ãƒ»è²ªæ¬²æ–¹ç­–ï¼‰ ===')
    eval_rewards = []
    eval_lengths = []
    eval_action_counts = {'reject': 0, 'accept': 0, 'wait': 0}
    max_steps = env_config.get('max_episode_length', 100)

    def greedy_action(obs_arr):
        st = torch.FloatTensor(obs_arr).unsqueeze(0)
        with torch.no_grad():
            probs = agent.policy_net(st)
            if torch.isnan(probs).any():
                probs = torch.ones(1, action_dim) / action_dim
            act = torch.argmax(probs, dim=1).item()
        return int(act)

    # è©•ä¾¡æ™‚ã¯ eval ãƒ•ã‚§ãƒ¼ã‚ºï¼ˆcutoff ã‚ˆã‚Šæ–°ã—ã„ãƒ‡ãƒ¼ã‚¿ï¼‰ã«åˆ‡æ›¿
    if time_split_enabled:
        env = TimeSplitDataWrapper(ReviewAcceptanceEnvironment(env_config), data_path=data_path, cutoff_iso=cutoff_iso, phase='eval')
    for e in range(eval_episodes):
        obs, _ = env.reset(seed=eval_seed + e)
        ep_rew = 0.0
        ep_len = 0
        for step in range(max_steps):
            a = greedy_action(obs)
            next_obs, r, terminated, truncated, info = env.step(a)
            # çµ±è¨ˆ
            try:
                name = ['reject', 'accept', 'wait'][a]
            except Exception:
                name = 'unknown'
            if name in eval_action_counts:
                eval_action_counts[name] += 1
            ep_rew += r
            ep_len += 1
            obs = next_obs
            if terminated or truncated:
                break
        eval_rewards.append(ep_rew)
        eval_lengths.append(ep_len)

    if eval_rewards:
        eval_avg = float(np.mean(eval_rewards))
        eval_std = float(np.std(eval_rewards))
        eval_len = float(np.mean(eval_lengths))
        print('\n=== ğŸ“Š è©•ä¾¡çµæœï¼ˆæ›´æ–°ãªã—ï¼‰ ===')
        print(f'  è©•ä¾¡ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰æ•°: {eval_episodes}')
        print(f'  å¹³å‡å ±é…¬: {eval_avg:.3f} Â± {eval_std:.3f}')
        print(f'  å¹³å‡é•·: {eval_len:.1f}')
        if sum(eval_action_counts.values()) > 0:
            tot = sum(eval_action_counts.values())
            dist = {k: (v / tot * 100.0) for k, v in eval_action_counts.items()}
            print('  è¡Œå‹•åˆ†å¸ƒ(%)', {k: f'{v:.1f}' for k, v in dist.items()})

    # çµæœä¿å­˜
    results = {
        'timestamp': datetime.now().isoformat(),
        'completed_episodes': completed_episodes,
        'total_steps': sum(episode_lengths),
        'total_updates': update_count,
        'training_time': training_time,
        'mean_reward': float(np.mean(episode_rewards)) if episode_rewards else 0.0,
        'best_reward': float(best_avg_reward) if best_avg_reward != float('-inf') else 0.0,
        'action_distribution': {k: int(v) for k, v in action_counts.items()},
        'train': {
            'episodes': int(train_episodes),
            'seed': int(train_seed),
        },
        'eval': {
            'episodes': int(eval_episodes),
            'seed': int(eval_seed),
            'mean_reward': float(np.mean(eval_rewards)) if eval_rewards else 0.0,
            'std_reward': float(np.std(eval_rewards)) if eval_rewards else 0.0,
            'mean_length': float(np.mean(eval_lengths)) if eval_lengths else 0.0,
            'action_distribution': {k: int(v) for k, v in eval_action_counts.items()},
        },
        'status': 'completed_successfully'
    }
    
    # çµæœãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜
    os.makedirs('outputs', exist_ok=True)
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    with open(f'outputs/stable_rl_results_{timestamp}.json', 'w') as f:
        json.dump(results, f, indent=2)
    
    print(f'\\nğŸ’¾ çµæœä¿å­˜: outputs/stable_rl_results_{timestamp}.json')
    print(f'\\nâœ… å®‰å®šç‰ˆgerrit-retentionå¼·åŒ–å­¦ç¿’ã‚·ã‚¹ãƒ†ãƒ å®Ÿè¡Œå®Œäº†!')
    print(f'ã‚¨ãƒ©ãƒ¼ãªã—ã§{completed_episodes}ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰å®Œäº†ã—ã¾ã—ãŸï¼')
    
    return 0


if __name__ == "__main__":
    sys.exit(main())